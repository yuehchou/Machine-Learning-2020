{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p9FfatPz6MU3"
   },
   "source": [
    "# **Homework 1: Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7RiAkkjCc6l"
   },
   "source": [
    "# **Load 'train.csv'**\n",
    "train.csv 的資料為 12 個月中，每個月取 20 天，每天 24 小時的資料(每小時資料有 18 個 features)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "1AfNX-hB3kN8",
    "outputId": "6b9d36ea-d38a-4d74-8abe-61c32a038606"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('./../data/train.csv', encoding = 'big5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqUdj00pDTpo"
   },
   "source": [
    "# **Preprocessing** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIGP7XUYD_Yb"
   },
   "outputs": [],
   "source": [
    "data = data.iloc[:, 3:]\n",
    "data[data == 'NR'] = 0\n",
    "raw_data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7PCrVwX6jBF"
   },
   "source": [
    "# **Extract Features (1)**\n",
    "\n",
    "將原始 4320 * 18 的資料依照每個月分重組成 12 個 18 (features) * 480 (hours) 的資料。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBnrGYXu9dZQ"
   },
   "outputs": [],
   "source": [
    "month_data = {}\n",
    "for month in range(12):\n",
    "    sample = np.empty([18, 480])\n",
    "    for day in range(20):\n",
    "        sample[:, day * 24 : (day + 1) * 24] = raw_data[18 * (20 * month + day) : 18 * (20 * month + day + 1), :]\n",
    "    month_data[month] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.empty([18, 480*12])\n",
    "for i in range(18):\n",
    "    for m in range(12):\n",
    "        if m == 0:\n",
    "            temp = month_data[m][i,:]\n",
    "        else:\n",
    "            temp = np.concatenate((temp, month_data[m][i,:]), axis = -1)\n",
    "    items[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01712724,  0.25465706,  0.28311942,  0.29177826,  0.02997038,\n",
       "        0.44911349,  0.37556381,  0.35667002,  0.77642643,  1.        ,\n",
       "       -0.06265388, -0.26419607,  0.3708308 ,  0.3521594 ,  0.18613794,\n",
       "        0.15699025, -0.08470312, -0.04545785])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor = np.corrcoef(items)[9]\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_bool = np.copy(cor)\n",
    "# items_bool[items_bool == 1] = 0\n",
    "items_bool[items_bool > 0.3] = 1\n",
    "items_bool[items_bool <= 0.3] = 0\n",
    "items_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_month_data = {}\n",
    "for month in range(12):\n",
    "    j = 0\n",
    "    sample = np.empty([int(np.sum(items_bool)), 480])\n",
    "    for i in range(18):\n",
    "        if items_bool[i] == 1:\n",
    "            sample[j,:] = month_data[month][i,:]\n",
    "            j += 1\n",
    "    new_month_data[month] = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WhVmtFEQ9D6t"
   },
   "source": [
    "# **Extract Features (2)**\n",
    "\n",
    "每個月會有 480hrs，每 9 小時形成一個 data，每個月會有 471 個 data，故總資料數為 471 * 12 筆，而每筆 data 有 9 * 18 的 features (一小時 18 個 features * 9 小時)。\n",
    "\n",
    "對應的 target 則有 471 * 12 個(第 10 個小時的 PM2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "dcOrC4Fi-n3i",
    "outputId": "83541460-d78d-4214-f057-9c37c84593ff"
   },
   "outputs": [],
   "source": [
    "x = np.empty([12 * 471, int(np.sum(items_bool)) * 9], dtype = float)\n",
    "y = np.empty([12 * 471, 1], dtype = float)\n",
    "for month in range(12):\n",
    "    for day in range(20):\n",
    "        for hour in range(24):\n",
    "            if day == 19 and hour > 14:\n",
    "                continue\n",
    "            x[month * 471 + day * 24 + hour, :] = new_month_data[month][:,day * 24 + hour : day * 24 + hour + 9].reshape(1, -1) #vector dim:18*9 (9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9)\n",
    "            y[month * 471 + day * 24 + hour, 0] = month_data[month][9, day * 24 + hour + 9] #value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wOii0TX8IwE"
   },
   "source": [
    "# **Normalize (1)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "ceMqFoNI8ftQ",
    "outputId": "99744546-62e0-4b92-db55-e570939eb2b2"
   },
   "outputs": [],
   "source": [
    "mean_x = np.mean(x, axis = 0) #18 * 9 \n",
    "std_x = np.std(x, axis = 0) #18 * 9 \n",
    "for i in range(len(x)): #12 * 471\n",
    "    for j in range(len(x[0])): #18 * 9 \n",
    "        if std_x[j] != 0:\n",
    "            x[i][j] = (x[i][j] - mean_x[j]) / std_x[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzvXP5Jya64j"
   },
   "source": [
    "#**Split Training Data Into \"train_set\" and \"validation_set\"**\n",
    "這部分是針對作業中 report 的第二題、第三題做的簡單示範，以生成比較中用來訓練的 train_set 和不會被放入訓練、只是用來驗證的 validation_set。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "feF4XXOQb5SC",
    "outputId": "6fb8314b-7228-4c67-af30-c8496b27f184"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "x_train_set = x[: math.floor(len(x) * 0.8), :]\n",
    "y_train_set = y[: math.floor(len(y) * 0.8), :]\n",
    "x_validation = x[math.floor(len(x) * 0.8): , :]\n",
    "y_validation = y[math.floor(len(y) * 0.8): , :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-qAu0KR_ZRR"
   },
   "source": [
    "# **Training**\n",
    "\n",
    "因為常數項的存在，所以 dimension (dim) 需要多加一欄；eps 項是避免 adagrad 的分母為 0 而加的極小數值。\n",
    "\n",
    "每一個 dimension (dim) 會對應到各自的 gradient, weight (w)，透過一次次的 iteration (iter_time) 學習。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cCzDfxBFBFqp",
    "outputId": "2e2eef7e-a49a-48bd-db5e-36e554c20105",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.493453344161135 validation 100:3.3468758972785735\n",
      "training 200:5.674992539386751 validation 200:2.8520444021738234\n",
      "training 300:5.507788589140977 validation 300:2.744473739271659\n",
      "training 400:5.4244335699907085 validation 400:2.6809801578842714\n",
      "training 500:5.371104358853002 validation 500:2.6374906797457984\n",
      "training 600:5.333824623688139 validation 600:2.6067259987987867\n",
      "training 700:5.306369691910539 validation 700:2.5843520583358406\n",
      "training 800:5.2854338373260195 validation 800:2.5676825640398566\n",
      "training 900:5.269081594459484 validation 900:2.555018261016676\n",
      "training 1000:5.2560923448562695 validation 1000:2.5452479229049705\n",
      "training 1100:5.245648938371093 validation 1100:2.537619011885057\n",
      "training 1200:5.237177624965252 validation 1200:2.531605610146172\n",
      "training 1300:5.230260176619787 validation 1300:2.5268302104783453\n",
      "training 1400:5.224582738944069 validation 1400:2.523015714172004\n",
      "training 1500:5.219904445132373 validation 1500:2.519954899219605\n",
      "training 1600:5.2160372198240985 validation 1600:2.5174903585277044\n",
      "training 1700:5.212832227499524 validation 1700:2.5155009357729066\n",
      "training 1800:5.210170444679475 validation 1800:2.5138923246334874\n",
      "training 1900:5.207955897135501 validation 1900:2.5125904123090153\n",
      "training 2000:5.2061106830123 validation 2000:2.5115364772155218\n",
      "training 2100:5.204571231617751 validation 2100:2.5106836667579056\n",
      "training 2200:5.203285441228358 validation 2200:2.5099943757372194\n",
      "training 2300:5.202210457309368 validation 2300:2.5094382691940393\n",
      "training 2400:5.201310926947465 validation 2400:2.5089907734301233\n",
      "training 2500:5.200557613625915 validation 2500:2.508631911910042\n",
      "training 2600:5.199926288750149 validation 2600:2.5083453984824673\n",
      "training 2700:5.199396838433461 validation 2700:2.508117924873396\n",
      "training 2800:5.198952539531301 validation 2800:2.5079385964643475\n",
      "training 2900:5.198579469975357 validation 2900:2.5077984824029103\n",
      "training 3000:5.19826602651137 validation 3000:2.5076902546884536\n",
      "training 3100:5.198002528903842 validation 3100:2.507607897088298\n",
      "training 3200:5.197780894146738 validation 3200:2.5075464692809133\n",
      "training 3300:5.197594367626136 validation 3300:2.5075019149790387\n",
      "training 3400:5.197437300805056 validation 3400:2.5074709052924535\n",
      "training 3500:5.197304967043461 validation 3500:2.507450710481634\n",
      "training 3600:5.197193408771705 validation 3600:2.507439094694774\n",
      "training 3700:5.197099310507406 validation 3700:2.507434229388989\n",
      "training 3800:5.197019893220671 validation 3800:2.50743462199636\n",
      "training 3900:5.196952826367551 validation 3900:2.5074390570679728\n",
      "early stopping at iter 3991\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "w = np.zeros([dim, 1])\n",
    "\n",
    "train_x = np.concatenate((np.ones([x_train_set.shape[0], 1]), x_train_set), axis = 1).astype(float)\n",
    "validation_x = np.concatenate((np.ones([x_validation.shape[0], 1]), x_validation), axis = 1).astype(float)\n",
    "\n",
    "learning_rate = 1.5\n",
    "iter_time = 20000\n",
    "early_stopping_iter = 250\n",
    "adagrad = np.zeros([dim, 1])\n",
    "eps = 0.0000000001\n",
    "\n",
    "temp_loss = np.inf\n",
    "temp_iter = 0\n",
    "\n",
    "for t in range(iter_time):\n",
    "    loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "    validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "    if validation_loss < temp_loss:\n",
    "        temp_w = np.copy(w)\n",
    "        temp_loss = np.copy(validation_loss)\n",
    "        temp_iter = 0\n",
    "    else:\n",
    "        if temp_iter < early_stopping_iter:\n",
    "            temp_iter += 1\n",
    "        else:\n",
    "            print(\"early stopping at iter\", t)\n",
    "            w = temp_w\n",
    "            break\n",
    "    if(t%100==0):\n",
    "        print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "    gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "    adagrad += gradient ** 2\n",
    "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "# np.save('./../model/weight4.npy', w)\n",
    "# w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqNdWKsYBK28"
   },
   "source": [
    "# **Testing**\n",
    "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
    "\n",
    "載入 test data，並且以相似於訓練資料預先處理和特徵萃取的方式處理，使 test data 形成 240 個維度為 18 * 9 + 1 的資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "AALygqJFCWOA",
    "outputId": "1a840905-645a-400e-c92b-d97fce8a3fad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "testdata = pd.read_csv('./../data/test.csv', header = None, encoding = 'big5')\n",
    "test_data = testdata.iloc[:, 2:]\n",
    "test_data[test_data == 'NR'] = 0\n",
    "test_data = test_data.to_numpy()\n",
    "test_x = np.empty([240, int(np.sum(items_bool)*9)], dtype = float)\n",
    "k = 0\n",
    "for i in range(18):\n",
    "    if items_bool[i] == 1:\n",
    "        for j in range(240):\n",
    "                test_x[j, k*9 : (k+1)*9] = test_data[18 * j + i, :]\n",
    "        k += 1\n",
    "for i in range(len(test_x)):\n",
    "    for j in range(len(test_x[0])):\n",
    "        if std_x[j] != 0:\n",
    "            test_x[i][j] = (test_x[i][j] - mean_x[j]) / std_x[j]\n",
    "test_x = np.concatenate((np.ones([240, 1]), test_x), axis = 1).astype(float)\n",
    "# test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJQks9JEHR6W"
   },
   "source": [
    "# **Prediction**\n",
    "說明圖同上\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1165ETzZyE6HStqKvgR0gKrJwgFLK6-CW)\n",
    "\n",
    "有了 weight 和測試資料即可預測 target。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jNyB229jHsEQ",
    "outputId": "b2ef6cbb-e040-4b03-9c0f-25eb91665cd1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.82975691],\n",
       "       [ 17.76852395],\n",
       "       [ 23.40144313],\n",
       "       [  6.66709447],\n",
       "       [ 26.63020263],\n",
       "       [ 21.89300618],\n",
       "       [ 24.30005025],\n",
       "       [ 29.22948677],\n",
       "       [ 17.37058281],\n",
       "       [ 58.54494234],\n",
       "       [ 13.10911139],\n",
       "       [ 11.04098904],\n",
       "       [ 62.40551913],\n",
       "       [ 52.14296428],\n",
       "       [ 22.03875111],\n",
       "       [ 11.44868447],\n",
       "       [ 32.76744294],\n",
       "       [ 67.70890778],\n",
       "       [  1.83830453],\n",
       "       [ 17.04073663],\n",
       "       [ 41.76233972],\n",
       "       [ 71.80365825],\n",
       "       [  9.57207555],\n",
       "       [ 18.68183015],\n",
       "       [ 13.38175287],\n",
       "       [ 37.30683375],\n",
       "       [ 13.01843992],\n",
       "       [ 74.46157981],\n",
       "       [  7.46458492],\n",
       "       [ 56.06918693],\n",
       "       [ 23.78328306],\n",
       "       [  8.21451732],\n",
       "       [  3.12748865],\n",
       "       [ 19.05679879],\n",
       "       [ 28.70185589],\n",
       "       [ 37.62004885],\n",
       "       [ 43.07562382],\n",
       "       [ 30.85712044],\n",
       "       [ 42.22687575],\n",
       "       [ 34.93079092],\n",
       "       [  8.18479243],\n",
       "       [ 39.33054737],\n",
       "       [ 32.77027141],\n",
       "       [ 51.10166832],\n",
       "       [ 15.93323255],\n",
       "       [ 36.6443819 ],\n",
       "       [ 23.70436486],\n",
       "       [  9.92363337],\n",
       "       [ 25.11161672],\n",
       "       [ 32.52795969],\n",
       "       [ 19.36256367],\n",
       "       [  8.91481268],\n",
       "       [ 24.0515891 ],\n",
       "       [ 53.3508087 ],\n",
       "       [ 15.33979562],\n",
       "       [ 35.57084653],\n",
       "       [ 32.51335513],\n",
       "       [ 21.57591937],\n",
       "       [ 57.91983914],\n",
       "       [ 23.5315623 ],\n",
       "       [ 13.61618917],\n",
       "       [ 42.4806934 ],\n",
       "       [ 13.02996037],\n",
       "       [ 50.18388771],\n",
       "       [ 15.3080102 ],\n",
       "       [ 16.40889522],\n",
       "       [ 16.75834969],\n",
       "       [  0.73885444],\n",
       "       [ 42.65691229],\n",
       "       [ 30.49295878],\n",
       "       [ 21.75236156],\n",
       "       [ 40.4779902 ],\n",
       "       [ 60.88075758],\n",
       "       [  4.94953285],\n",
       "       [ 16.79345796],\n",
       "       [  3.56976577],\n",
       "       [ 38.70716755],\n",
       "       [ 13.87696311],\n",
       "       [ 23.29388328],\n",
       "       [ 21.11837858],\n",
       "       [ 23.69575824],\n",
       "       [ 36.75163206],\n",
       "       [ 20.93592151],\n",
       "       [ 92.36574759],\n",
       "       [ 36.70902489],\n",
       "       [ 27.45745283],\n",
       "       [ 21.02863094],\n",
       "       [ 32.61483352],\n",
       "       [ 22.99618609],\n",
       "       [ 20.20285652],\n",
       "       [ 28.91522526],\n",
       "       [ 41.20254214],\n",
       "       [  4.77209989],\n",
       "       [ 38.80112043],\n",
       "       [ 46.21555866],\n",
       "       [ 17.00957203],\n",
       "       [ 32.14109576],\n",
       "       [ 13.21645729],\n",
       "       [ 23.35900547],\n",
       "       [  4.29953988],\n",
       "       [ 18.69015435],\n",
       "       [ 27.58042825],\n",
       "       [ 16.57877697],\n",
       "       [ 16.60393481],\n",
       "       [ 23.21308867],\n",
       "       [ 38.47156905],\n",
       "       [ 31.23648511],\n",
       "       [  7.15342985],\n",
       "       [  6.59406441],\n",
       "       [ 78.61851648],\n",
       "       [ 46.0621339 ],\n",
       "       [ 16.26913229],\n",
       "       [ 27.92884109],\n",
       "       [ 15.82160595],\n",
       "       [ 13.09757578],\n",
       "       [ 25.39990208],\n",
       "       [ 26.93802895],\n",
       "       [  8.89240401],\n",
       "       [ 17.09154881],\n",
       "       [ 19.07368896],\n",
       "       [ 81.59940685],\n",
       "       [ 23.81602613],\n",
       "       [ 34.24355049],\n",
       "       [ 24.80609188],\n",
       "       [  7.10422695],\n",
       "       [ 39.87541675],\n",
       "       [  9.30856179],\n",
       "       [ 21.79801039],\n",
       "       [ 30.03377762],\n",
       "       [ 61.95076056],\n",
       "       [ 21.73464271],\n",
       "       [ 24.20066899],\n",
       "       [ 58.36100316],\n",
       "       [ 15.05633006],\n",
       "       [ 12.82280493],\n",
       "       [  2.1370706 ],\n",
       "       [ 12.84940301],\n",
       "       [ 58.24429694],\n",
       "       [ 19.58994663],\n",
       "       [  5.76400138],\n",
       "       [ 27.76611961],\n",
       "       [ 25.26468545],\n",
       "       [ 43.86210087],\n",
       "       [ 30.13894913],\n",
       "       [ 17.6399275 ],\n",
       "       [ 26.16239681],\n",
       "       [ 11.07532822],\n",
       "       [ 51.16009977],\n",
       "       [ 23.02375153],\n",
       "       [ 39.34444157],\n",
       "       [ 10.99831737],\n",
       "       [  7.60744154],\n",
       "       [ 25.14558323],\n",
       "       [  6.4247163 ],\n",
       "       [ 15.44048759],\n",
       "       [ 40.64017678],\n",
       "       [  8.33360372],\n",
       "       [ 37.47795489],\n",
       "       [ 11.28539174],\n",
       "       [ 19.0184408 ],\n",
       "       [ 42.08326982],\n",
       "       [ 18.63578594],\n",
       "       [ 14.14895009],\n",
       "       [  7.60349243],\n",
       "       [ 52.25313764],\n",
       "       [ 29.7164134 ],\n",
       "       [  2.97815467],\n",
       "       [ 16.11156764],\n",
       "       [ 64.38126941],\n",
       "       [ 12.65966878],\n",
       "       [ 63.68432841],\n",
       "       [ 38.91242779],\n",
       "       [ 26.57442307],\n",
       "       [ 19.24216121],\n",
       "       [ 62.74671308],\n",
       "       [ 24.82064315],\n",
       "       [ 19.91574991],\n",
       "       [ 38.7281285 ],\n",
       "       [ 12.05945206],\n",
       "       [ 30.71794232],\n",
       "       [ 17.5635956 ],\n",
       "       [ 12.45594797],\n",
       "       [ 54.48634412],\n",
       "       [ 46.45339519],\n",
       "       [ 16.12240392],\n",
       "       [ 34.73050067],\n",
       "       [ 26.55800052],\n",
       "       [ 71.82090923],\n",
       "       [  9.89915274],\n",
       "       [ 58.73858064],\n",
       "       [ 38.43080375],\n",
       "       [ 14.18699662],\n",
       "       [ 28.87599064],\n",
       "       [  1.61556334],\n",
       "       [ 18.79790604],\n",
       "       [  1.18451132],\n",
       "       [ 33.95556379],\n",
       "       [ 11.53244322],\n",
       "       [ 18.84610639],\n",
       "       [ 61.948075  ],\n",
       "       [ 24.52197104],\n",
       "       [ 25.91005587],\n",
       "       [ 65.74017474],\n",
       "       [ 11.87707597],\n",
       "       [  8.58583365],\n",
       "       [ 11.16000662],\n",
       "       [  7.48133254],\n",
       "       [  1.48523972],\n",
       "       [123.72318045],\n",
       "       [ 20.52767649],\n",
       "       [ 14.93368731],\n",
       "       [ 14.29088248],\n",
       "       [ 36.67536257],\n",
       "       [ 35.93300156],\n",
       "       [ 20.65229162],\n",
       "       [ 34.70699246],\n",
       "       [ 78.88191439],\n",
       "       [  0.86170553],\n",
       "       [ 12.63128366],\n",
       "       [ 32.56996831],\n",
       "       [ 15.56803943],\n",
       "       [ 12.33134628],\n",
       "       [115.87910068],\n",
       "       [ 12.40474095],\n",
       "       [ 16.85198434],\n",
       "       [ 64.5730416 ],\n",
       "       [ 16.08751842],\n",
       "       [ 17.93823343],\n",
       "       [ 10.15167654],\n",
       "       [  7.15128044],\n",
       "       [ 44.34183422],\n",
       "       [ 12.74382993],\n",
       "       [ 53.82796347],\n",
       "       [ 43.1291073 ],\n",
       "       [ 25.52712802],\n",
       "       [ 42.68635624],\n",
       "       [ 69.04188009],\n",
       "       [ 43.46281918],\n",
       "       [ 11.06719339],\n",
       "       [ 17.21337105]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w = np.load('./../model/weight.npy')\n",
    "ans_y = np.dot(test_x, w)\n",
    "ans_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKMKW7RzHwuO"
   },
   "source": [
    "# **Save Prediction to CSV File**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Dwfpqqy0H8en",
    "outputId": "38e75a01-b540-4d64-bbbd-3139456128e6"
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('./../results/submit.csv', mode='w', newline='') as submit_file:\n",
    "#     csv_writer = csv.writer(submit_file)\n",
    "#     header = ['id', 'value']\n",
    "#     print(header)\n",
    "#     csv_writer.writerow(header)\n",
    "#     for i in range(240):\n",
    "#         row = ['id_' + str(i), ans_y[i][0]]\n",
    "#         csv_writer.writerow(row)\n",
    "#         print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Homework1 report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem1\n",
    "\n",
    "### 使用四種不同的 learning rate 進行 training (其他參數需一致)，作圖並討論其收斂過程（橫軸為 iteration 次數，縱軸為 loss 的大小，四種 learning rate 的收斂線請以不同顏色呈現在一張圖裡做比較）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use learning rate: 0.1\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:19.160932572549996 validation 100:10.354122022619407\n",
      "training 200:18.320721973030576 validation 200:10.115329330447938\n",
      "training 300:17.72373186683206 validation 300:9.883045617898041\n",
      "training 400:17.24428875630067 validation 400:9.67187610878531\n",
      "training 500:16.83553896675529 validation 500:9.476990970286032\n",
      "training 600:16.47477708331891 validation 600:9.29515377592024\n",
      "training 700:16.14932634207709 validation 700:9.12451502306083\n",
      "training 800:15.851332233783229 validation 800:8.963827798456059\n",
      "training 900:15.575527821936078 validation 900:8.81209085492047\n",
      "training 1000:15.318165884922138 validation 1000:8.66842528491573\n",
      "training 1100:15.07646004419237 validation 1100:8.53204031949323\n",
      "training 1200:14.8482679760517 validation 1200:8.402225864349752\n",
      "training 1300:14.631899165324779 validation 1300:8.278349806748114\n",
      "training 1400:14.42599148868811 validation 1400:8.159854062336034\n",
      "training 1500:14.229428319798206 validation 1500:8.046248707989035\n",
      "training 1600:14.04128080511581 validation 1600:7.937104947960604\n",
      "training 1700:13.860766481827469 validation 1700:7.832047750013673\n",
      "training 1800:13.687218894934581 validation 1800:7.730748744482779\n",
      "training 1900:13.520064838354397 validation 1900:7.632919722006438\n",
      "training 2000:13.358807010825755 validation 2000:7.538306875630742\n",
      "training 2100:13.203010597461857 validation 2100:7.446685812870558\n",
      "training 2200:13.052292748213365 validation 2200:7.3578572960629955\n",
      "training 2300:12.906314227608519 validation 2300:7.271643637095732\n",
      "training 2400:12.764772714629503 validation 2400:7.187885661597701\n",
      "training 2500:12.627397372465895 validation 2500:7.106440158591617\n",
      "training 2600:12.493944406725312 validation 2600:7.0271777385136565\n",
      "training 2700:12.364193401145627 validation 2700:6.9499810318736275\n",
      "training 2800:12.237944270814864 validation 2800:6.8747431706730655\n",
      "training 2900:12.115014710245111 validation 2900:6.801366503988718\n",
      "training 3000:11.99523804133282 validation 3000:6.729761507400718\n",
      "training 3100:11.878461386992619 validation 3100:6.659845853055224\n",
      "training 3200:11.76454411196858 validation 3200:6.591543613126266\n",
      "training 3300:11.65335648434139 validation 3300:6.524784574387777\n",
      "training 3400:11.544778520515827 validation 3400:6.459503645661567\n",
      "training 3500:11.438698983677709 validation 3500:6.395640343210009\n",
      "training 3600:11.335014511356064 validation 3600:6.333138341823239\n",
      "training 3700:11.233628852183148 validation 3700:6.2719450815228575\n",
      "training 3800:11.13445219548804 validation 3800:6.212011421563965\n",
      "training 3900:11.037400580194404 validation 3900:6.15329133484425\n",
      "training 4000:10.942395371775474 validation 4000:6.095741636988065\n",
      "training 4100:10.849362797867613 validation 4100:6.039321745317313\n",
      "training 4200:10.75823353464996 validation 4200:5.983993463692319\n",
      "training 4300:10.668942337330604 validation 4300:5.92972078983795\n",
      "training 4400:10.581427709094829 validation 4400:5.876469742290409\n",
      "training 4500:10.495631603711043 validation 4500:5.824208204529951\n",
      "training 4600:10.411499157687983 validation 4600:5.772905784221162\n",
      "training 4700:10.328978448459827 validation 4700:5.722533685779435\n",
      "training 4800:10.248020275564928 validation 4800:5.673064594730642\n",
      "training 4900:10.168577962195473 validation 4900:5.624472572539624\n",
      "training 5000:10.0906071748439 validation 5000:5.576732960759147\n",
      "training 5100:10.014065759067414 validation 5100:5.52982229349999\n",
      "training 5200:9.938913589643805 validation 5200:5.483718217349527\n",
      "training 5300:9.865112433607305 validation 5300:5.438399417974388\n",
      "training 5400:9.792625824837843 validation 5400:5.393845552735343\n",
      "training 5500:9.721418949036307 validation 5500:5.350037188722333\n",
      "training 5600:9.651458538055559 validation 5600:5.306955745686196\n",
      "training 5700:9.58271277267618 validation 5700:5.26458344340328\n",
      "training 5800:9.515151193018916 validation 5800:5.222903253060655\n",
      "training 5900:9.448744615876057 validation 5900:5.181898852294797\n",
      "training 6000:9.383465058322253 validation 6000:5.141554583555844\n",
      "training 6100:9.319285667034396 validation 6100:5.101855415504089\n",
      "training 6200:9.256180652810238 validation 6200:5.06278690717561\n",
      "training 6300:9.194125229828632 validation 6300:5.0243351746805756\n",
      "training 6400:9.133095559241127 validation 6400:4.9864868602214285\n",
      "training 6500:9.07306869672597 validation 6500:4.9492291032389515\n",
      "training 6600:9.014022543672077 validation 6600:4.912549513512705\n",
      "training 6700:8.95593580169308 validation 6700:4.876436146058804\n",
      "training 6800:8.898787930200514 validation 6800:4.840877477682727\n",
      "training 6900:8.842559106790734 validation 6900:4.805862385057849\n",
      "training 7000:8.787230190223124 validation 7000:4.771380124212171\n",
      "training 7100:8.732782685787761 validation 7100:4.737420311316208\n",
      "training 7200:8.679198712878883 validation 7200:4.703972904674423\n",
      "training 7300:8.626460974607046 validation 7300:4.671028187831087\n",
      "training 7400:8.574552729297546 validation 7400:4.638576753709071\n",
      "training 7500:8.523457763736058 validation 7500:4.60660948970698\n",
      "training 7600:8.473160368034288 validation 7600:4.575117563686297\n",
      "training 7700:8.423645311999296 validation 7700:4.544092410785792\n",
      "training 7800:8.374897822899907 validation 7800:4.513525721005618\n",
      "training 7900:8.326903564532342 validation 7900:4.48340942750813\n",
      "training 8000:8.279648617495297 validation 8000:4.453735695586671\n",
      "training 8100:8.233119460591846 validation 8100:4.424496912257406\n",
      "training 8200:8.187302953282176 validation 8200:4.395685676432821\n",
      "training 8300:8.142186319117112 validation 8300:4.3672947896386205\n",
      "training 8400:8.097757130087865 validation 8400:4.339317247238745\n",
      "training 8500:8.054003291832233 validation 8500:4.31174623013578\n",
      "training 8600:8.010913029642317 validation 8600:4.284575096916607\n",
      "training 8700:7.968474875222606 validation 8700:4.257797376415237\n",
      "training 8800:7.926677654151281 validation 8800:4.231406760666897\n",
      "training 8900:7.885510474000943 validation 8900:4.205397098229256\n",
      "training 9000:7.844962713078213 validation 9000:4.179762387848476\n",
      "training 9100:7.8050240097444386 validation 9100:4.1544967724492405\n",
      "training 9200:7.765684252282545 validation 9200:4.1295945334294935\n",
      "training 9300:7.726933569277409 validation 9300:4.105050085241868\n",
      "training 9400:7.688762320479427 validation 9400:4.080857970245054\n",
      "training 9500:7.651161088123069 validation 9500:4.057012853809513\n",
      "training 9600:7.614120668673991 validation 9600:4.033509519662942\n",
      "training 9700:7.57763206498027 validation 9700:4.010342865461962\n",
      "training 9800:7.541686478804588 validation 9800:3.9875078985772245\n",
      "training 9900:7.506275303716187 validation 9900:3.9649997320802073\n",
      "training 10000:7.471390118322326 validation 10000:3.942813580920506\n",
      "training 10100:7.437022679820521 validation 10100:3.920944758283247\n",
      "training 10200:7.403164917854103 validation 10200:3.8993886721169755\n",
      "training 10300:7.3698089286544235 validation 10300:3.8781408218227824\n",
      "training 10400:7.3369469694544325 validation 10400:3.857196795096225\n",
      "training 10500:7.304571453159055 validation 10500:3.836552264913995\n",
      "training 10600:7.272674943258725 validation 10600:3.816202986657761\n",
      "training 10700:7.241250148973342 validation 10700:3.7961447953681726\n",
      "training 10800:7.21028992061462 validation 10800:3.776373603122347\n",
      "training 10900:7.179787245155458 validation 10900:3.756885396528576\n",
      "training 11000:7.14973524199573 validation 11000:3.7376762343323686\n",
      "training 11100:7.12012715891448 validation 11100:3.7187422451283108\n",
      "training 11200:7.090956368198981 validation 11200:3.7000796251724517\n",
      "training 11300:7.062216362941854 validation 11300:3.6816846362903637\n",
      "training 11400:7.033900753497704 validation 11400:3.663553603876133\n",
      "training 11500:7.006003264091538 validation 11500:3.6456829149780186\n",
      "training 11600:6.978517729571188 validation 11600:3.6280690164664735\n",
      "training 11700:6.9514380922968835 validation 11700:3.6107084132807143\n",
      "training 11800:6.924758399161149 validation 11800:3.5935976667500915\n",
      "training 11900:6.898472798732744 validation 11900:3.5767333929867497\n",
      "training 12000:6.872575538518658 validation 12000:3.560112261346284\n",
      "training 12100:6.847060962338363 validation 12100:3.543730992953145\n",
      "training 12200:6.821923507805106 validation 12200:3.527586359287924\n",
      "training 12300:6.797157703909061 validation 12300:3.5116751808336155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 12400:6.772758168697487 validation 12400:3.4959943257781716\n",
      "training 12500:6.748719607047353 validation 12500:3.4805407087708042\n",
      "training 12600:6.725036808526068 validation 12600:3.465311289729609\n",
      "training 12700:6.701704645336145 validation 12700:3.4503030726981696\n",
      "training 12800:6.678718070339984 validation 12800:3.43551310474902\n",
      "training 12900:6.656072115160954 validation 12900:3.420938474931805\n",
      "training 13000:6.633761888357204 validation 13000:3.406576313264139\n",
      "training 13100:6.611782573664992 validation 13100:3.392423789763351\n",
      "training 13200:6.590129428308223 validation 13200:3.378478113517245\n",
      "training 13300:6.568797781371105 validation 13300:3.3647365317921283\n",
      "training 13400:6.547783032231181 validation 13400:3.351196329176543\n",
      "training 13500:6.527080649049876 validation 13500:3.337854826759078\n",
      "training 13600:6.506686167317957 validation 13600:3.3247093813387703\n",
      "training 13700:6.486595188453387 validation 13700:3.3117573846666475\n",
      "training 13800:6.4668033784492875 validation 13800:3.298996262717105\n",
      "training 13900:6.447306466569657 validation 13900:3.2864234749877514\n",
      "training 14000:6.428100244090692 validation 14000:3.274036513826481\n",
      "training 14100:6.409180563085661 validation 14100:3.2618329037845704\n",
      "training 14200:6.390543335251493 validation 14200:3.249810200994739\n",
      "training 14300:6.3721845307749625 validation 14300:3.2379659925729176\n",
      "training 14400:6.354100177236956 validation 14400:3.2262978960428432\n",
      "training 14500:6.336286358552935 validation 14500:3.2148035587823593\n",
      "training 14600:6.318739213948048 validation 14600:3.203480657490496\n",
      "training 14700:6.301454936965357 validation 14700:3.1923268976744312\n",
      "training 14800:6.284429774505693 validation 14800:3.181340013155415\n",
      "training 14900:6.267660025897695 validation 14900:3.170517765592821\n",
      "training 15000:6.251142041996769 validation 15000:3.1598579440255157\n",
      "training 15100:6.234872224311607 validation 15100:3.149358364429758\n",
      "training 15200:6.218847024157124 validation 15200:3.1390168692929077\n",
      "training 15300:6.203062941832543 validation 15300:3.1288313272021635\n",
      "training 15400:6.187516525823638 validation 15400:3.1187996324477245\n",
      "training 15500:6.172204372028032 validation 15500:3.1089197046396815\n",
      "training 15600:6.1571231230024095 validation 15600:3.099189488337944\n",
      "training 15700:6.142269467230942 validation 15700:3.0896069526947274\n",
      "training 15800:6.12764013841374 validation 15800:3.080170091108896\n",
      "training 15900:6.113231914774591 validation 15900:3.070876920891646\n",
      "training 16000:6.099041618387183 validation 16000:3.061725482943057\n",
      "training 16100:6.085066114518828 validation 16100:3.052713841438856\n",
      "training 16200:6.0713023109911815 validation 16200:3.043840083527072\n",
      "training 16300:6.057747157556992 validation 16300:3.0351023190339683\n",
      "training 16400:6.044397645292303 validation 16400:3.026498680178862\n",
      "training 16500:6.0312508060034835 validation 16500:3.0180273212974287\n",
      "training 16600:6.0183037116483264 validation 16600:3.0096864185729935\n",
      "training 16700:6.0055534737707434 validation 16700:3.0014741697754923\n",
      "training 16800:5.992997242948382 validation 16800:2.9933887940076627\n",
      "training 16900:5.980632208252712 validation 16900:2.9854285314581563\n",
      "training 17000:5.96845559672095 validation 17000:2.977591643161154\n",
      "training 17100:5.956464672839436 validation 17100:2.969876410762223\n",
      "training 17200:5.944656738037909 validation 17200:2.962281136290048\n",
      "training 17300:5.9330291301942735 validation 17300:2.954804141933748\n",
      "training 17400:5.921579223149396 validation 17400:2.9474437698254814\n",
      "training 17500:5.910304426231565 validation 17500:2.9401983818280786\n",
      "training 17600:5.899202183790176 validation 17600:2.9330663593274062\n",
      "training 17700:5.888269974738328 validation 17700:2.9260461030292486\n",
      "training 17800:5.877505312103906 validation 17800:2.919136032760417\n",
      "training 17900:5.866905742588914 validation 17900:2.9123345872739197\n",
      "training 18000:5.856468846136591 validation 18000:2.905640224057873\n",
      "training 18100:5.846192235506214 validation 18100:2.8990514191480923\n",
      "training 18200:5.836073555855078 validation 18200:2.892566666944\n",
      "training 18300:5.826110484327556 validation 18300:2.8861844800278\n",
      "training 18400:5.816300729650883 validation 18400:2.8799033889866505\n",
      "training 18500:5.806642031737421 validation 18500:2.8737219422377\n",
      "training 18600:5.797132161293242 validation 18600:2.8676387058558386\n",
      "training 18700:5.787768919432706 validation 18700:2.861652263403961\n",
      "training 18800:5.7785501372989 validation 18800:2.8557612157656695\n",
      "training 18900:5.7694736756896905 validation 18900:2.849964180980199\n",
      "training 19000:5.760537424689262 validation 19000:2.844259794079521\n",
      "training 19100:5.7517393033048565 validation 19100:2.8386467069274204\n",
      "training 19200:5.743077259108649 validation 19200:2.8331235880604906\n",
      "training 19300:5.7345492678845424 validation 19300:2.8276891225309186\n",
      "training 19400:5.7261533332796954 validation 19400:2.8223420117509304\n",
      "training 19500:5.717887486460799 validation 19500:2.8170809733388893\n",
      "training 19600:5.709749785774676 validation 19600:2.8119047409668174\n",
      "training 19700:5.701738316413371 validation 19700:2.8068120642093892\n",
      "training 19800:5.693851190083452 validation 19800:2.801801708394261\n",
      "training 19900:5.686086544679393 validation 19900:2.7968724544536405\n",
      "training 20000:5.678442543961025 validation 20000:2.792023098777091\n",
      "training 20100:5.670917377234823 validation 20100:2.7872524530654252\n",
      "training 20200:5.663509259039076 validation 20200:2.7825593441857235\n",
      "training 20300:5.656216428832697 validation 20300:2.777942614027331\n",
      "training 20400:5.649037150687717 validation 20400:2.773401119358853\n",
      "training 20500:5.641969712985318 validation 20500:2.7689337316860843\n",
      "training 20600:5.635012428115263 validation 20600:2.764539337110777\n",
      "training 20700:5.62816363217879 validation 20700:2.760216836190301\n",
      "training 20800:5.621421684694781 validation 20800:2.755965143798077\n",
      "training 20900:5.614784968309182 validation 20900:2.751783188984803\n",
      "training 21000:5.608251888507654 validation 21000:2.7476699148404333\n",
      "training 21100:5.601820873331284 validation 21100:2.7436242783568576\n",
      "training 21200:5.595490373095404 validation 21200:2.7396452502912867\n",
      "training 21300:5.589258860111441 validation 21300:2.7357318150303214\n",
      "training 21400:5.5831248284117 validation 21400:2.731882970454666\n",
      "training 21500:5.577086793477087 validation 21500:2.7280977278044922\n",
      "training 21600:5.5711432919677195 validation 21600:2.7243751115454318\n",
      "training 21700:5.565292881456328 validation 21700:2.720714159235175\n",
      "training 21800:5.55953414016449 validation 21800:2.7171139213906854\n",
      "training 21900:5.553865666701606 validation 21900:2.7135734613560145\n",
      "training 22000:5.548286079806563 validation 22000:2.710091855170685\n",
      "training 22100:5.542794018092127 validation 22100:2.706668191438694\n",
      "training 22200:5.53738813979197 validation 22200:2.7033015711980894\n",
      "training 22300:5.532067122510281 validation 22300:2.6999911077911034\n",
      "training 22400:5.526829662974011 validation 22400:2.696735926734899\n",
      "training 22500:5.521674476787678 validation 22500:2.6935351655928916\n",
      "training 22600:5.516600298190656 validation 22600:2.6903879738466316\n",
      "training 22700:5.511605879817046 validation 22700:2.6872935127683095\n",
      "training 22800:5.50668999245796 validation 22800:2.684250955293812\n",
      "training 22900:5.501851424826331 validation 22900:2.6812594858964096\n",
      "training 23000:5.497088983324102 validation 23000:2.678318300461005\n",
      "training 23100:5.4924014918118935 validation 23100:2.6754266061590295\n",
      "training 23200:5.487787791381004 validation 23200:2.6725836213239034\n",
      "training 23300:5.483246740127839 validation 23300:2.6697885753271544\n",
      "training 23400:5.478777212930668 validation 23400:2.6670407084551373\n",
      "training 23500:5.47437810122875 validation 23500:2.664339271786408\n",
      "training 23600:5.470048312803722 validation 23600:2.661683527069707\n",
      "training 23700:5.465786771563384 validation 23700:2.659072746602637\n",
      "training 23800:5.4615924173276795 validation 23800:2.6565062131109634\n",
      "training 23900:5.457464205616991 validation 23900:2.653983219628591\n",
      "training 24000:5.453401107442701 validation 24000:2.651503069378238\n",
      "training 24100:5.449402109099962 validation 24100:2.6490650756527705\n",
      "training 24200:5.445466211962722 validation 24200:2.6466685616972536\n",
      "training 24300:5.4415924322809115 validation 24300:2.644312860591688\n",
      "training 24400:5.437779800979883 validation 24400:2.6419973151344824\n",
      "training 24500:5.4340273634619845 validation 24500:2.6397212777266317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 24600:5.430334179410326 validation 24600:2.637484110256648\n",
      "training 24700:5.426699322594682 validation 24700:2.6352851839862192\n",
      "training 24800:5.423121880679522 validation 24800:2.6331238794366194\n",
      "training 24900:5.419600955034202 validation 24900:2.630999586275896\n",
      "training 25000:5.416135660545218 validation 25000:2.6289117032068128\n",
      "training 25100:5.412725125430595 validation 25100:2.6268596378555786\n",
      "training 25200:5.409368491056315 validation 25200:2.6248428066613525\n",
      "training 25300:5.406064911754858 validation 25300:2.622860634766559\n",
      "training 25400:5.40281355464577 validation 25400:2.620912555908003\n",
      "training 25500:5.399613599458269 validation 25500:2.618998012308782\n",
      "training 25600:5.3964642383559065 validation 25600:2.61711645457104\n",
      "training 25700:5.393364675763206 validation 25700:2.6152673415695307\n",
      "training 25800:5.390314128194329 validation 25800:2.613450140346016\n",
      "training 25900:5.38731182408372 validation 25900:2.6116643260045143\n",
      "training 26000:5.384357003618706 validation 26000:2.6099093816073737\n",
      "training 26100:5.381448918574099 validation 26100:2.6081847980722266\n",
      "training 26200:5.37858683214869 validation 26200:2.6064900740697725\n",
      "training 26300:5.375770018803724 validation 26300:2.6048247159224487\n",
      "training 26400:5.372997764103252 validation 26400:2.6031882375039435\n",
      "training 26500:5.370269364556419 validation 26500:2.601580160139611\n",
      "training 26600:5.367584127461619 validation 26600:2.600000012507738\n",
      "training 26700:5.364941370752543 validation 26700:2.5984473305417057\n",
      "training 26800:5.362340422846091 validation 26800:2.5969216573330387\n",
      "training 26900:5.359780622492104 validation 26900:2.5954225430353284\n",
      "training 27000:5.357261318624967 validation 27000:2.5939495447690613\n",
      "training 27100:5.354781870217011 validation 27100:2.592502226527342\n",
      "training 27200:5.352341646133722 validation 27200:2.5910801590824963\n",
      "training 27300:5.349940024990752 validation 27300:2.589682919893599\n",
      "training 27400:5.347576395012689 validation 27400:2.588310093014874\n",
      "training 27500:5.34525015389362 validation 27500:2.586961269005024\n",
      "training 27600:5.3429607086593895 validation 27600:2.5856360448374365\n",
      "training 27700:5.3407074755316595 validation 27700:2.584334023811318\n",
      "training 27800:5.338489879793634 validation 27800:2.583054815463714\n",
      "training 27900:5.336307355657521 validation 27900:2.5817980354824512\n",
      "training 28000:5.334159346133651 validation 28000:2.58056330561996\n",
      "training 28100:5.3320453029013155 validation 28100:2.5793502536080295\n",
      "training 28200:5.3299646861812295 validation 28200:2.578158513073445\n",
      "training 28300:5.32791696460965 validation 28300:2.5769877234545335\n",
      "training 28400:5.325901615114122 validation 28400:2.575837529918615\n",
      "training 28500:5.323918122790861 validation 28500:2.574707583280349\n",
      "training 28600:5.321965980783708 validation 28600:2.5735975399209825\n",
      "training 28700:5.320044690164693 validation 28700:2.572507061708491\n",
      "training 28800:5.3181537598161475 validation 28800:2.5714358159186115\n",
      "training 28900:5.316292706314393 validation 28900:2.5703834751567696\n",
      "training 29000:5.314461053814976 validation 29000:2.569349717280898\n",
      "training 29100:5.312658333939394 validation 29100:2.5683342253251342\n",
      "training 29200:5.310884085663384 validation 29200:2.567336687424406\n",
      "training 29300:5.309137855206649 validation 29300:2.5663567967398846\n",
      "training 29400:5.307419195924125 validation 29400:2.5653942513853307\n",
      "training 29500:5.305727668198675 validation 29500:2.5644487543542978\n",
      "training 29600:5.30406283933525 validation 29600:2.563520013448206\n",
      "training 29700:5.302424283456486 validation 29700:2.562607741205287\n",
      "training 29800:5.300811581399723 validation 29800:2.561711654830373\n",
      "training 29900:5.299224320615423 validation 29900:2.5608314761255504\n",
      "training 30000:5.297662095066987 validation 30000:2.5599669314216587\n",
      "training 30100:5.296124505131966 validation 30100:2.5591177515106414\n",
      "training 30200:5.2946111575045975 validation 30200:2.5582836715787205\n",
      "training 30300:5.29312166509973 validation 30300:2.5574644311404247\n",
      "training 30400:5.291655646958053 validation 30400:2.5566597739734296\n",
      "training 30500:5.290212728152656 validation 30500:2.5558694480542297\n",
      "training 30600:5.288792539696905 validation 30600:2.5550932054946336\n",
      "training 30700:5.2873947184535695 validation 30700:2.554330802479054\n",
      "training 30800:5.28601890704528 validation 30800:2.5535819992026227\n",
      "training 30900:5.284664753766202 validation 30900:2.5528465598100953\n",
      "training 31000:5.283331912494964 validation 31000:2.5521242523355507\n",
      "training 31100:5.282020042608851 validation 31100:2.551414848642886\n",
      "training 31200:5.280728808899172 validation 31200:2.5507181243670853\n",
      "training 31300:5.279457881487859 validation 31300:2.5500338588562657\n",
      "training 31400:5.278206935745242 validation 31400:2.5493618351144973\n",
      "training 31500:5.276975652209003 validation 31500:2.5487018397453785\n",
      "training 31600:5.275763716504307 validation 31600:2.548053662896374\n",
      "training 31700:5.274570819265062 validation 31700:2.547417098203903\n",
      "training 31800:5.2733966560563115 validation 31800:2.5467919427391594\n",
      "training 31900:5.272240927297772 validation 31900:2.5461779969546843\n",
      "training 32000:5.271103338188449 validation 32000:2.545575064631655\n",
      "training 32100:5.269983598632363 validation 32100:2.5449829528279024\n",
      "training 32200:5.26888142316535 validation 32200:2.5444014718266414\n",
      "training 32300:5.267796530882935 validation 32300:2.5438304350859164\n",
      "training 32400:5.266728645369254 validation 32400:2.5432696591887414\n",
      "training 32500:5.265677494627006 validation 32500:2.542718963793937\n",
      "training 32600:5.264642811008464 validation 32600:2.54217817158766\n",
      "training 32700:5.263624331147458 validation 32700:2.5416471082356056\n",
      "training 32800:5.262621795892388 validation 32800:2.5411256023358915\n",
      "training 32900:5.261634950240222 validation 32900:2.540613485372605\n",
      "training 33000:5.260663543271471 validation 33000:2.540110591670012\n",
      "training 33100:5.259707328086113 validation 33100:2.5396167583474183\n",
      "training 33200:5.2587660617404755 validation 33200:2.5391318252746715\n",
      "training 33300:5.257839505185082 validation 33300:2.5386556350283156\n",
      "training 33400:5.256927423203376 validation 33400:2.538188032848359\n",
      "training 33500:5.256029584351395 validation 33500:2.537728866595681\n",
      "training 33600:5.255145760898345 validation 33600:2.5372779867100532\n",
      "training 33700:5.2542757287680475 validation 33700:2.5368352461687653\n",
      "training 33800:5.253419267481283 validation 33800:2.536400500445865\n",
      "training 33900:5.2525761600989895 validation 33900:2.5359736074719827\n",
      "training 34000:5.251746193166322 validation 34000:2.535554427594759\n",
      "training 34100:5.2509291566575556 validation 34100:2.5351428235398425\n",
      "training 34200:5.250124843921812 validation 34200:2.5347386603724718\n",
      "training 34300:5.249333051629629 validation 34300:2.5343418054596243\n",
      "training 34400:5.248553579720314 validation 34400:2.533952128432727\n",
      "training 34500:5.247786231350112 validation 34500:2.533569501150921\n",
      "training 34600:5.247030812841159 validation 34600:2.5331937976648784\n",
      "training 34700:5.246287133631209 validation 34700:2.5328248941811546\n",
      "training 34800:5.2455550062241345 validation 34800:2.532462669027087\n",
      "training 34900:5.244834246141174 validation 34900:2.5321070026162067\n",
      "training 35000:5.244124671872928 validation 35000:2.5317577774141875\n",
      "training 35100:5.243426104832095 validation 35100:2.531414877905294\n",
      "training 35200:5.242738369306926 validation 35200:2.5310781905593567\n",
      "training 35300:5.242061292415397 validation 35300:2.5307476037992314\n",
      "training 35400:5.241394704060091 validation 35400:2.5304230079687677\n",
      "training 35500:5.24073843688375 validation 35500:2.5301042953012556\n",
      "training 35600:5.240092326225551 validation 35600:2.529791359888364\n",
      "training 35700:5.239456210078031 validation 35700:2.5294840976495516\n",
      "training 35800:5.238829929044675 validation 35800:2.529182406301947\n",
      "training 35900:5.238213326298175 validation 35900:2.5288861853306925\n",
      "training 36000:5.237606247539334 validation 36000:2.528595335959751\n",
      "training 36100:5.2370085409565865 validation 36100:2.5283097611231553\n",
      "training 36200:5.236420057186178 validation 36200:2.528029365436712\n",
      "training 36300:5.235840649272941 validation 36300:2.527754055170137\n",
      "training 36400:5.23527017263169 validation 36400:2.527483738219631\n",
      "training 36500:5.234708485009218 validation 36500:2.5272183240808768\n",
      "training 36600:5.2341554464468825 validation 36600:2.5269577238224645\n",
      "training 36700:5.23361091924378 validation 36700:2.526701850059725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 36800:5.233074767920486 validation 36800:2.5264506169289804\n",
      "training 36900:5.232546859183382 validation 36900:2.526203940062195\n",
      "training 37000:5.232027061889515 validation 37000:2.5259617365620244\n",
      "training 37100:5.231515247012033 validation 37100:2.5257239249772607\n",
      "training 37200:5.23101128760614 validation 37200:2.5254904252786643\n",
      "training 37300:5.230515058775604 validation 37300:2.525261158835174\n",
      "training 37400:5.230026437639786 validation 37400:2.5250360483905\n",
      "training 37500:5.229545303301178 validation 37500:2.5248150180400843\n",
      "training 37600:5.229071536813461 validation 37600:2.5245979932084244\n",
      "training 37700:5.228605021150075 validation 37700:2.524384900626767\n",
      "training 37800:5.228145641173268 validation 37800:2.524175668311145\n",
      "training 37900:5.227693283603639 validation 37900:2.5239702255407774\n",
      "training 38000:5.227247836990166 validation 38000:2.5237685028368024\n",
      "training 38100:5.2268091916807125 validation 38100:2.5235704319413594\n",
      "training 38200:5.226377239792983 validation 38200:2.523375945797003\n",
      "training 38300:5.225951875185958 validation 38300:2.523184978526447\n",
      "training 38400:5.225532993431773 validation 38400:2.5229974654126366\n",
      "training 38500:5.225120491788038 validation 38500:2.522813342879138\n",
      "training 38600:5.224714269170616 validation 38600:2.522632548470845\n",
      "training 38700:5.224314226126804 validation 38700:2.522455020834997\n",
      "training 38800:5.223920264808972 validation 38800:2.522280699702507\n",
      "training 38900:5.2235322889485944 validation 38900:2.5221095258695834\n",
      "training 39000:5.223150203830718 validation 39000:2.521941441179661\n",
      "training 39100:5.222773916268821 validation 39100:2.5217763885056126\n",
      "training 39200:5.222403334580074 validation 39200:2.5216143117322596\n",
      "training 39300:5.2220383685610035 validation 39300:2.521455155739162\n",
      "training 39400:5.221678929463541 validation 39400:2.5212988663836864\n",
      "training 39500:5.221324929971448 validation 39500:2.521145390484358\n",
      "training 39600:5.220976284177133 validation 39600:2.5209946758044746\n",
      "training 39700:5.220632907558817 validation 39700:2.520846671035994\n",
      "training 39800:5.220294716958083 validation 39800:2.5207013257836848\n",
      "training 39900:5.219961630557785 validation 39900:2.5205585905495327\n",
      "training 40000:5.219633567860293 validation 40000:2.5204184167174044\n",
      "training 40100:5.219310449666108 validation 40100:2.5202807565379683\n",
      "training 40200:5.218992198052805 validation 40200:2.5201455631138514\n",
      "training 40300:5.218678736354325 validation 40300:2.5200127903850507\n",
      "training 40400:5.218369989140586 validation 40400:2.519882393114579\n",
      "training 40500:5.218065882197439 validation 40500:2.519754326874348\n",
      "training 40600:5.217766342506927 validation 40600:2.519628548031285\n",
      "training 40700:5.217471298227885 validation 40700:2.519505013733682\n",
      "training 40800:5.2171806786768204 validation 40800:2.519383681897758\n",
      "training 40900:5.216894414309138 validation 40900:2.519264511194462\n",
      "training 41000:5.216612436700629 validation 41000:2.519147461036476\n",
      "training 41100:5.216334678529298 validation 41100:2.519032491565447\n",
      "training 41200:5.216061073557447 validation 41200:2.5189195636394235\n",
      "training 41300:5.215791556614072 validation 41300:2.5188086388205004\n",
      "training 41400:5.215526063577535 validation 41400:2.518699679362673\n",
      "training 41500:5.215264531358517 validation 41500:2.5185926481998906\n",
      "training 41600:5.215006897883234 validation 41600:2.5184875089343075\n",
      "training 41700:5.2147531020769575 validation 41700:2.518384225824733\n",
      "training 41800:5.214503083847763 validation 41800:2.5182827637752676\n",
      "training 41900:5.214256784070564 validation 41900:2.5181830883241396\n",
      "training 42000:5.214014144571395 validation 42000:2.5180851656327143\n",
      "training 42100:5.213775108111953 validation 42100:2.517988962474699\n",
      "training 42200:5.213539618374393 validation 42200:2.517894446225519\n",
      "training 42300:5.21330761994636 validation 42300:2.517801584851879\n",
      "training 42400:5.213079058306267 validation 42400:2.517710346901494\n",
      "training 42500:5.212853879808816 validation 42500:2.5176207014929934\n",
      "training 42600:5.212632031670747 validation 42600:2.517532618305997\n",
      "training 42700:5.212413461956829 validation 42700:2.5174460675713535\n",
      "training 42800:5.212198119566061 validation 42800:2.517361020061544\n",
      "training 42900:5.211985954218113 validation 42900:2.5172774470812516\n",
      "training 43000:5.211776916439978 validation 43000:2.5171953204580793\n",
      "training 43100:5.211570957552844 validation 43100:2.5171146125334376\n",
      "training 43200:5.211368029659179 validation 43200:2.517035296153573\n",
      "training 43300:5.211168085630026 validation 43300:2.5169573446607525\n",
      "training 43400:5.2109710790925 validation 43400:2.5168807318846027\n",
      "training 43500:5.210776964417497 validation 43500:2.516805432133582\n",
      "training 43600:5.210585696707594 validation 43600:2.5167314201866096\n",
      "training 43700:5.210397231785151 validation 43700:2.51665867128483\n",
      "training 43800:5.210211526180602 validation 43800:2.5165871611235144\n",
      "training 43900:5.210028537120948 validation 43900:2.5165168658441037\n",
      "training 44000:5.209848222518418 validation 44000:2.516447762026386\n",
      "training 44100:5.209670540959334 validation 44100:2.5163798266808017\n",
      "training 44200:5.20949545169314 validation 44200:2.5163130372408857\n",
      "training 44300:5.209322914621626 validation 44300:2.5162473715558327\n",
      "training 44400:5.209152890288317 validation 44400:2.516182807883189\n",
      "training 44500:5.208985339868039 validation 44500:2.516119324881673\n",
      "training 44600:5.208820225156647 validation 44600:2.5160569016041126\n",
      "training 44700:5.208657508560935 validation 44700:2.5159955174905044\n",
      "training 44800:5.208497153088704 validation 44800:2.5159351523611924\n",
      "training 44900:5.208339122338978 validation 44900:2.5158757864101604\n",
      "training 45000:5.208183380492403 validation 45000:2.515817400198438\n",
      "training 45100:5.2080298923017905 validation 45100:2.515759974647627\n",
      "training 45200:5.207878623082815 validation 45200:2.515703491033524\n",
      "training 45300:5.207729538704857 validation 45300:2.5156479309798625\n",
      "training 45400:5.207582605582021 validation 45400:2.515593276452163\n",
      "training 45500:5.2074377906642635 validation 45500:2.515539509751679\n",
      "training 45600:5.2072950614286935 validation 45600:2.5154866135094536\n",
      "training 45700:5.207154385871007 validation 45700:2.515434570680475\n",
      "training 45800:5.207015732497054 validation 45800:2.5153833645379344\n",
      "training 45900:5.206879070314557 validation 45900:2.515332978667578\n",
      "training 46000:5.2067443688249515 validation 46000:2.5152833969621606\n",
      "training 46100:5.206611598015369 validation 46100:2.51523460361599\n",
      "training 46200:5.206480728350748 validation 46200:2.5151865831195668\n",
      "training 46300:5.206351730766069 validation 46300:2.5151393202543177\n",
      "training 46400:5.206224576658723 validation 46400:2.515092800087417\n",
      "training 46500:5.2060992378810065 validation 46500:2.5150470079666953\n",
      "training 46600:5.205975686732729 validation 46600:2.515001929515642\n",
      "training 46700:5.205853895953951 validation 46700:2.514957550628486\n",
      "training 46800:5.205733838717836 validation 46800:2.514913857465368\n",
      "training 46900:5.205615488623625 validation 46900:2.51487083644759\n",
      "training 47000:5.20549881968971 validation 47000:2.5148284742529503\n",
      "training 47100:5.2053838063468465 validation 47100:2.514786757811157\n",
      "training 47200:5.205270423431451 validation 47200:2.514745674299322\n",
      "training 47300:5.205158646179029 validation 47300:2.514705211137531\n",
      "training 47400:5.205048450217697 validation 47400:2.5146653559844943\n",
      "training 47500:5.204939811561807 validation 47500:2.514626096733263\n",
      "training 47600:5.2048327066057 validation 47600:2.514587421507031\n",
      "training 47700:5.2047271121175305 validation 47700:2.514549318655001\n",
      "training 47800:5.204623005233214 validation 47800:2.5145117767483267\n",
      "training 47900:5.204520363450458 validation 47900:2.514474784576121\n",
      "training 48000:5.204419164622907 validation 48000:2.514438331141539\n",
      "training 48100:5.204319386954372 validation 48100:2.5144024056579224\n",
      "training 48200:5.204221008993152 validation 48200:2.5143669975450162\n",
      "training 48300:5.204124009626461 validation 48300:2.514332096425246\n",
      "training 48400:5.204028368074931 validation 48400:2.514297692120064\n",
      "training 48500:5.203934063887219 validation 48500:2.514263774646357\n",
      "training 48600:5.203841076934697 validation 48600:2.514230334212916\n",
      "training 48700:5.203749387406219 validation 48700:2.5141973612169655\n",
      "training 48800:5.20365897580299 validation 48800:2.514164846240761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 48900:5.20356982293351 validation 48900:2.514132780048232\n",
      "training 49000:5.203481909908598 validation 49000:2.5141011535816973\n",
      "training 49100:5.20339521813651 validation 49100:2.5140699579586294\n",
      "training 49200:5.20330972931812 validation 49200:2.5140391844684764\n",
      "training 49300:5.20322542544219 validation 49300:2.51400882456954\n",
      "training 49400:5.203142288780721 validation 49400:2.5139788698859085\n",
      "training 49500:5.203060301884368 validation 49500:2.513949312204445\n",
      "training 49600:5.202979447577942 validation 49600:2.51392014347182\n",
      "training 49700:5.202899708955977 validation 49700:2.513891355791607\n",
      "training 49800:5.2028210693783725 validation 49800:2.5138629414214217\n",
      "training 49900:5.202743512466114 validation 49900:2.5138348927701104\n",
      "training 50000:5.202667022097048 validation 50000:2.513807202394994\n",
      "training 50100:5.202591582401743 validation 50100:2.5137798629991526\n",
      "training 50200:5.202517177759407 validation 50200:2.513752867428764\n",
      "training 50300:5.202443792793874 validation 50300:2.5137262086704846\n",
      "training 50400:5.202371412369662 validation 50400:2.513699879848877\n",
      "training 50500:5.20230002158809 validation 50500:2.513673874223884\n",
      "training 50600:5.202229605783455 validation 50600:2.513648185188345\n",
      "training 50700:5.202160150519283 validation 50700:2.513622806265558\n",
      "training 50800:5.2020916415846274 validation 50800:2.5135977311068847\n",
      "training 50900:5.202024064990443 validation 50900:2.5135729534893914\n",
      "training 51000:5.201957406966003 validation 51000:2.5135484673135426\n",
      "training 51100:5.20189165395539 validation 51100:2.513524266600922\n",
      "training 51200:5.201826792614031 validation 51200:2.513500345492008\n",
      "training 51300:5.2017628098053015 validation 51300:2.5134766982439736\n",
      "training 51400:5.201699692597176 validation 51400:2.5134533192285353\n",
      "training 51500:5.201637428258933 validation 51500:2.5134302029298374\n",
      "training 51600:5.201576004257925 validation 51600:2.51340734394237\n",
      "training 51700:5.2015154082563875 validation 51700:2.5133847369689306\n",
      "training 51800:5.201455628108305 validation 51800:2.5133623768186117\n",
      "training 51900:5.201396651856337 validation 51900:2.5133402584048357\n",
      "training 52000:5.201338467728777 validation 52000:2.513318376743414\n",
      "training 52100:5.201281064136577 validation 52100:2.5132967269506494\n",
      "training 52200:5.201224429670415 validation 52200:2.513275304241462\n",
      "training 52300:5.201168553097805 validation 52300:2.513254103927559\n",
      "training 52400:5.201113423360261 validation 52400:2.513233121415627\n",
      "training 52500:5.201059029570509 validation 52500:2.513212352205566\n",
      "training 52600:5.201005361009736 validation 52600:2.513191791888744\n",
      "training 52700:5.200952407124892 validation 52700:2.513171436146294\n",
      "training 52800:5.20090015752603 validation 52800:2.5131512807474303\n",
      "training 52900:5.200848601983699 validation 52900:2.5131313215478026\n",
      "training 53000:5.20079773042636 validation 53000:2.513111554487875\n",
      "training 53100:5.200747532937873 validation 53100:2.5130919755913363\n",
      "training 53200:5.200697999754998 validation 53200:2.5130725809635366\n",
      "training 53300:5.20064912126495 validation 53300:2.5130533667899524\n",
      "training 53400:5.200600888002997 validation 53400:2.5130343293346806\n",
      "training 53500:5.200553290650085 validation 53500:2.5130154649389564\n",
      "training 53600:5.200506320030514 validation 53600:2.5129967700196976\n",
      "training 53700:5.200459967109644 validation 53700:2.51297824106808\n",
      "training 53800:5.2004142229916415 validation 53800:2.5129598746481325\n",
      "training 53900:5.200369078917264 validation 53900:2.5129416673953577\n",
      "training 54000:5.200324526261674 validation 54000:2.5129236160153825\n",
      "training 54100:5.200280556532305 validation 54100:2.512905717282623\n",
      "training 54200:5.2002371613667355 validation 54200:2.512887968038986\n",
      "training 54300:5.200194332530626 validation 54300:2.5128703651925823\n",
      "training 54400:5.200152061915669 validation 54400:2.5128529057164677\n",
      "training 54500:5.200110341537584 validation 54500:2.512835586647411\n",
      "training 54600:5.20006916353414 validation 54600:2.5128184050846722\n",
      "training 54700:5.2000285201632135 validation 54700:2.5128013581888164\n",
      "training 54800:5.1999884038008695 validation 54800:2.5127844431805406\n",
      "training 54900:5.19994880693949 validation 54900:2.512767657339521\n",
      "training 55000:5.199909722185918 validation 55000:2.5127509980032854\n",
      "training 55100:5.199871142259636 validation 55100:2.512734462566105\n",
      "training 55200:5.199833059990977 validation 55200:2.512718048477904\n",
      "training 55300:5.199795468319364 validation 55300:2.512701753243188\n",
      "training 55400:5.199758360291574 validation 55400:2.512685574419998\n",
      "training 55500:5.1997217290600375 validation 55500:2.512669509618872\n",
      "training 55600:5.1996855678811595 validation 55600:2.512653556501839\n",
      "training 55700:5.199649870113665 validation 55700:2.5126377127814212\n",
      "training 55800:5.199614629216983 validation 55800:2.5126219762196564\n",
      "training 55900:5.199579838749648 validation 55900:2.5126063446271405\n",
      "training 56000:5.199545492367728 validation 56000:2.5125908158620858\n",
      "training 56100:5.199511583823278 validation 56100:2.512575387829393\n",
      "training 56200:5.199478106962824 validation 56200:2.5125600584797487\n",
      "training 56300:5.199445055725866 validation 56300:2.5125448258087286\n",
      "training 56400:5.1994124241434045 validation 56400:2.5125296878559262\n",
      "training 56500:5.199380206336498 validation 56500:2.51251464270409\n",
      "training 56600:5.199348396514835 validation 56600:2.512499688478281\n",
      "training 56700:5.199316988975337 validation 56700:2.512484823345046\n",
      "training 56800:5.199285978100779 validation 56800:2.512470045511599\n",
      "training 56900:5.19925535835843 validation 56900:2.5124553532250284\n",
      "training 57000:5.199225124298731 validation 57000:2.5124407447715074\n",
      "training 57100:5.199195270553969 validation 57100:2.512426218475531\n",
      "training 57200:5.1991657918369985 validation 57200:2.5124117726991493\n",
      "training 57300:5.199136682939962 validation 57300:2.512397405841237\n",
      "training 57400:5.199107938733049 validation 57400:2.5123831163367587\n",
      "training 57500:5.199079554163262 validation 57500:2.512368902656051\n",
      "training 57600:5.199051524253212 validation 57600:2.512354763304129\n",
      "training 57700:5.199023844099921 validation 57700:2.5123406968199884\n",
      "training 57800:5.198996508873662 validation 57800:2.512326701775933\n",
      "training 57900:5.198969513816801 validation 57900:2.512312776776913\n",
      "training 58000:5.198942854242668 validation 58000:2.5122989204598642\n",
      "training 58100:5.1989165255344405 validation 58100:2.512285131493076\n",
      "training 58200:5.198890523144049 validation 58200:2.5122714085755593\n",
      "training 58300:5.1988648425910995 validation 58300:2.5122577504364307\n",
      "training 58400:5.198839479461805 validation 58400:2.512244155834308\n",
      "training 58500:5.198814429407954 validation 58500:2.512230623556715\n",
      "training 58600:5.198789688145873 validation 58600:2.512217152419494\n",
      "training 58700:5.198765251455422 validation 58700:2.512203741266243\n",
      "training 58800:5.198741115178997 validation 58800:2.512190388967743\n",
      "training 58900:5.198717275220554 validation 58900:2.5121770944214123\n",
      "training 59000:5.198693727544648 validation 59000:2.512163856550764\n",
      "training 59100:5.198670468175481 validation 59100:2.5121506743048743\n",
      "training 59200:5.198647493195978 validation 59200:2.5121375466578613\n",
      "training 59300:5.198624798746862 validation 59300:2.5121244726083716\n",
      "training 59400:5.198602381025761 validation 59400:2.512111451179079\n",
      "training 59500:5.198580236286316 validation 59500:2.512098481416192\n",
      "training 59600:5.198558360837305 validation 59600:2.512085562388971\n",
      "training 59700:5.198536751041794 validation 59700:2.5120726931892468\n",
      "training 59800:5.198515403316283 validation 59800:2.512059872930965\n",
      "training 59900:5.198494314129879 validation 59900:2.512047100749719\n",
      "training 60000:5.198473480003477 validation 60000:2.512034375802307\n",
      "training 60100:5.198452897508955 validation 60100:2.51202169726629\n",
      "training 60200:5.198432563268381 validation 60200:2.5120090643395576\n",
      "training 60300:5.198412473953239 validation 60300:2.5119964762399065\n",
      "training 60400:5.198392626283654 validation 60400:2.5119839322046245\n",
      "training 60500:5.198373017027648 validation 60500:2.511971431490079\n",
      "training 60600:5.198353643000388 validation 60600:2.5119589733713212\n",
      "training 60700:5.19833450106346 validation 60700:2.511946557141687\n",
      "training 60800:5.198315588124155 validation 60800:2.5119341821124155\n",
      "training 60900:5.198296901134754 validation 60900:2.5119218476122707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 61000:5.198278437091838 validation 61000:2.5119095529871673\n",
      "training 61100:5.198260193035598 validation 61100:2.5118972975998073\n",
      "training 61200:5.198242166049169 validation 61200:2.511885080829325\n",
      "training 61300:5.198224353257962 validation 61300:2.511872902070932\n",
      "training 61400:5.198206751829016 validation 61400:2.5118607607355785\n",
      "training 61500:5.198189358970348 validation 61500:2.51184865624961\n",
      "training 61600:5.198172171930333 validation 61600:2.51183658805444\n",
      "training 61700:5.198155187997074 validation 61700:2.511824555606224\n",
      "training 61800:5.1981384044977945 validation 61800:2.511812558375541\n",
      "training 61900:5.198121818798238 validation 61900:2.5118005958470793\n",
      "training 62000:5.198105428302073 validation 62000:2.511788667519335\n",
      "training 62100:5.198089230450312 validation 62100:2.511776772904305\n",
      "training 62200:5.198073222720738 validation 62200:2.5117649115271945\n",
      "training 62300:5.198057402627342 validation 62300:2.5117530829261274\n",
      "training 62400:5.198041767719767 validation 62400:2.5117412866518656\n",
      "training 62500:5.19802631558276 validation 62500:2.5117295222675233\n",
      "training 62600:5.198011043835639 validation 62600:2.511717789348303\n",
      "training 62700:5.197995950131761 validation 62700:2.5117060874812216\n",
      "training 62800:5.197981032158002 validation 62800:2.511694416264848\n",
      "training 62900:5.197966287634242 validation 62900:2.5116827753090476\n",
      "training 63000:5.19795171431287 validation 63000:2.5116711642347296\n",
      "training 63100:5.197937309978279 validation 63100:2.511659582673598\n",
      "training 63200:5.197923072446381 validation 63200:2.51164803026791\n",
      "training 63300:5.197908999564127 validation 63300:2.511636506670237\n",
      "training 63400:5.1978950892090365 validation 63400:2.5116250115432304\n",
      "training 63500:5.197881339288728 validation 63500:2.511613544559395\n",
      "training 63600:5.1978677477404664 validation 63600:2.5116021054008604\n",
      "training 63700:5.197854312530706 validation 63700:2.511590693759166\n",
      "training 63800:5.197841031654654 validation 63800:2.5115793093350396\n",
      "training 63900:5.19782790313583 validation 63900:2.5115679518381913\n",
      "training 64000:5.197814925025638 validation 64000:2.5115566209871005\n",
      "training 64100:5.1978020954029445 validation 64100:2.5115453165088164\n",
      "training 64200:5.197789412373663 validation 64200:2.511534038138756\n",
      "training 64300:5.197776874070342 validation 64300:2.5115227856205107\n",
      "training 64400:5.197764478651763 validation 64400:2.5115115587056547\n",
      "training 64500:5.1977522243025485 validation 64500:2.5115003571535546\n",
      "training 64600:5.197740109232766 validation 64600:2.511489180731186\n",
      "training 64700:5.197728131677551 validation 64700:2.511478029212954\n",
      "training 64800:5.197716289896719 validation 64800:2.5114669023805165\n",
      "training 64900:5.197704582174404 validation 64900:2.5114558000226106\n",
      "training 65000:5.197693006818685 validation 65000:2.511444721934881\n",
      "training 65100:5.197681562161232 validation 65100:2.5114336679197145\n",
      "training 65200:5.197670246556946 validation 65200:2.5114226377860764\n",
      "training 65300:5.197659058383613 validation 65300:2.5114116313493526\n",
      "training 65400:5.197647996041562 validation 65400:2.511400648431189\n",
      "training 65500:5.197637057953322 validation 65500:2.5113896888593383\n",
      "training 65600:5.1976262425632935 validation 65600:2.5113787524675155\n",
      "training 65700:5.197615548337421 validation 65700:2.511367839095242\n",
      "training 65800:5.197604973762867 validation 65800:2.5113569485877054\n",
      "training 65900:5.197594517347699 validation 65900:2.5113460807956183\n",
      "training 66000:5.197584177620579 validation 66000:2.511335235575078\n",
      "training 66100:5.197573953130445 validation 66100:2.511324412787429\n",
      "training 66200:5.197563842446223 validation 66200:2.5113136122991335\n",
      "training 66300:5.197553844156521 validation 66300:2.511302833981641\n",
      "training 66400:5.197543956869335 validation 66400:2.511292077711254\n",
      "training 66500:5.197534179211766 validation 66500:2.5112813433690104\n",
      "training 66600:5.1975245098297345 validation 66600:2.5112706308405586\n",
      "training 66700:5.197514947387693 validation 66700:2.511259940016033\n",
      "training 66800:5.197505490568364 validation 66800:2.5112492707899428\n",
      "training 66900:5.197496138072461 validation 66900:2.5112386230610526\n",
      "training 67000:5.197486888618422 validation 67000:2.5112279967322726\n",
      "training 67100:5.19747774094215 validation 67100:2.5112173917105447\n",
      "training 67200:5.197468693796756 validation 67200:2.511206807906735\n",
      "training 67300:5.197459745952296 validation 67300:2.511196245235529\n",
      "training 67400:5.197450896195532 validation 67400:2.51118570361533\n",
      "training 67500:5.197442143329679 validation 67500:2.5111751829681515\n",
      "training 67600:5.197433486174164 validation 67600:2.5111646832195222\n",
      "training 67700:5.1974249235643875 validation 67700:2.5111542042983905\n",
      "training 67800:5.197416454351488 validation 67800:2.5111437461370207\n",
      "training 67900:5.197408077402109 validation 67900:2.5111333086709102\n",
      "training 68000:5.197399791598177 validation 68000:2.5111228918386908\n",
      "training 68100:5.19739159583667 validation 68100:2.511112495582041\n",
      "training 68200:5.197383489029402 validation 68200:2.511102119845603\n",
      "training 68300:5.197375470102802 validation 68300:2.5110917645768875\n",
      "training 68400:5.197367537997703 validation 68400:2.5110814297261963\n",
      "training 68500:5.1973596916691305 validation 68500:2.5110711152465446\n",
      "training 68600:5.197351930086094 validation 68600:2.51106082109357\n",
      "training 68700:5.197344252231384 validation 68700:2.511050547225462\n",
      "training 68800:5.19733665710137 validation 68800:2.5110402936028815\n",
      "training 68900:5.197329143705807 validation 68900:2.511030060188888\n",
      "training 69000:5.197321711067631 validation 69000:2.5110198469488627\n",
      "training 69100:5.197314358222779 validation 69100:2.5110096538504396\n",
      "training 69200:5.19730708421999 validation 69200:2.5109994808634344\n",
      "training 69300:5.197299888120629 validation 69300:2.5109893279597753\n",
      "training 69400:5.1972927689984925 validation 69400:2.5109791951134346\n",
      "training 69500:5.197285725939638 validation 69500:2.510969082300364\n",
      "training 69600:5.197278758042203 validation 69600:2.5109589894984317\n",
      "training 69700:5.19727186441623 validation 69700:2.510948916687355\n",
      "training 69800:5.197265044183492 validation 69800:2.5109388638486436\n",
      "training 69900:5.19725829647733 validation 69900:2.5109288309655358\n",
      "training 70000:5.19725162044248 validation 70000:2.510918818022941\n",
      "training 70100:5.197245015234912 validation 70100:2.510908825007384\n",
      "training 70200:5.197238480021664 validation 70200:2.5108988519069433\n",
      "training 70300:5.197232013980692 validation 70300:2.510888898711203\n",
      "training 70400:5.197225616300704 validation 70400:2.5108789654111954\n",
      "training 70500:5.197219286181007 validation 70500:2.5108690519993466\n",
      "training 70600:5.197213022831361 validation 70600:2.510859158469429\n",
      "training 70700:5.1972068254718256 validation 70700:2.5108492848165094\n",
      "training 70800:5.197200693332608 validation 70800:2.5108394310369007\n",
      "training 70900:5.197194625653927 validation 70900:2.510829597128111\n",
      "training 71000:5.197188621685865 validation 71000:2.510819783088802\n",
      "training 71100:5.197182680688225 validation 71100:2.510809988918738\n",
      "training 71200:5.197176801930397 validation 71200:2.510800214618745\n",
      "training 71300:5.197170984691222 validation 71300:2.5107904601906674\n",
      "training 71400:5.197165228258849 validation 71400:2.5107807256373214\n",
      "training 71500:5.197159531930613 validation 71500:2.5107710109624586\n",
      "training 71600:5.197153895012903 validation 71600:2.510761316170721\n",
      "training 71700:5.197148316821026 validation 71700:2.5107516412676048\n",
      "training 71800:5.1971427966790875 validation 71800:2.510741986259417\n",
      "training 71900:5.1971373339198665 validation 71900:2.510732351153245\n",
      "training 72000:5.1971319278846915 validation 72000:2.5107227359569158\n",
      "training 72100:5.197126577923322 validation 72100:2.510713140678954\n",
      "training 72200:5.197121283393824 validation 72200:2.51070356532856\n",
      "training 72300:5.19711604366246 validation 72300:2.510694009915564\n",
      "training 72400:5.197110858103568 validation 72400:2.5106844744503984\n",
      "training 72500:5.197105726099455 validation 72500:2.5106749589440653\n",
      "training 72600:5.197100647040274 validation 72600:2.5106654634081003\n",
      "training 72700:5.197095620323926 validation 72700:2.510655987854549\n",
      "training 72800:5.197090645355944 validation 72800:2.510646532295928\n",
      "training 72900:5.197085721549388 validation 72900:2.5106370967452043\n",
      "training 73000:5.19708084832474 validation 73000:2.51062768121576\n",
      "training 73100:5.197076025109802 validation 73100:2.5106182857213666\n",
      "training 73200:5.197071251339589 validation 73200:2.5106089102761593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 73300:5.197066526456236 validation 73300:2.510599554894608\n",
      "training 73400:5.19706184990889 validation 73400:2.5105902195914944\n",
      "training 73500:5.197057221153621 validation 73500:2.5105809043818823\n",
      "training 73600:5.197052639653318 validation 73600:2.5105716092810986\n",
      "training 73700:5.197048104877602 validation 73700:2.510562334304704\n",
      "training 73800:5.19704361630273 validation 73800:2.5105530794684756\n",
      "training 73900:5.197039173411496 validation 73900:2.5105438447883777\n",
      "training 74000:5.1970347756931545 validation 74000:2.5105346302805462\n",
      "training 74100:5.197030422643319 validation 74100:2.5105254359612617\n",
      "training 74200:5.197026113763879 validation 74200:2.510516261846934\n",
      "training 74300:5.197021848562917 validation 74300:2.5105071079540764\n",
      "training 74400:5.197017626554615 validation 74400:2.5104979742992906\n",
      "training 74500:5.19701344725918 validation 74500:2.510488860899244\n",
      "training 74600:5.197009310202754 validation 74600:2.5104797677706556\n",
      "training 74700:5.197005214917335 validation 74700:2.5104706949302695\n",
      "training 74800:5.197001160940698 validation 74800:2.5104616423948483\n",
      "training 74900:5.196997147816317 validation 74900:2.5104526101811486\n",
      "training 75000:5.196993175093281 validation 75000:2.5104435983059052\n",
      "training 75100:5.196989242326225 validation 75100:2.5104346067858176\n",
      "training 75200:5.196985349075249 validation 75200:2.5104256356375325\n",
      "training 75300:5.196981494905847 validation 75300:2.510416684877629\n",
      "training 75400:5.196977679388829 validation 75400:2.5104077545226042\n",
      "training 75500:5.196973902100257 validation 75500:2.510398844588858\n",
      "training 75600:5.196970162621363 validation 75600:2.51038995509268\n",
      "training 75700:5.196966460538488 validation 75700:2.5103810860502365\n",
      "training 75800:5.19696279544301 validation 75800:2.510372237477556\n",
      "training 75900:5.196959166931272 validation 75900:2.51036340939052\n",
      "training 76000:5.196955574604522 validation 76000:2.510354601804846\n",
      "training 76100:5.196952018068839 validation 76100:2.5103458147360778\n",
      "training 76200:5.196948496935075 validation 76200:2.5103370481995775\n",
      "training 76300:5.196945010818787 validation 76300:2.5103283022105067\n",
      "training 76400:5.196941559340175 validation 76400:2.5103195767838233\n",
      "training 76500:5.196938142124016 validation 76500:2.510310871934269\n",
      "training 76600:5.19693475879961 validation 76600:2.510302187676356\n",
      "training 76700:5.196931409000714 validation 76700:2.510293524024363\n",
      "training 76800:5.196928092365483 validation 76800:2.51028488099232\n",
      "training 76900:5.196924808536411 validation 76900:2.5102762585940033\n",
      "training 77000:5.196921557160275 validation 77000:2.510267656842926\n",
      "training 77100:5.1969183378880786 validation 77100:2.5102590757523298\n",
      "training 77200:5.196915150374992 validation 77200:2.5102505153351733\n",
      "training 77300:5.196911994280298 validation 77300:2.5102419756041305\n",
      "training 77400:5.19690886926734 validation 77400:2.510233456571581\n",
      "training 77500:5.196905775003467 validation 77500:2.5102249582495992\n",
      "training 77600:5.196902711159977 validation 77600:2.5102164806499516\n",
      "training 77700:5.196899677412068 validation 77700:2.5102080237840894\n",
      "training 77800:5.196896673438791 validation 77800:2.5101995876631396\n",
      "training 77900:5.1968936989229855 validation 77900:2.5101911722979047\n",
      "training 78000:5.196890753551248 validation 78000:2.5101827776988492\n",
      "training 78100:5.196887837013865 validation 78100:2.5101744038760994\n",
      "training 78200:5.196884949004777 validation 78200:2.5101660508394357\n",
      "training 78300:5.196882089221524 validation 78300:2.5101577185982906\n",
      "training 78400:5.196879257365203 validation 78400:2.510149407161739\n",
      "training 78500:5.196876453140414 validation 78500:2.5101411165384975\n",
      "training 78600:5.1968736762552235 validation 78600:2.510132846736918\n",
      "training 78700:5.196870926421113 validation 78700:2.5101245977649844\n",
      "training 78800:5.196868203352935 validation 78800:2.510116369630307\n",
      "training 78900:5.196865506768871 validation 78900:2.5101081623401225\n",
      "training 79000:5.196862836390388 validation 79000:2.5100999759012836\n",
      "training 79100:5.1968601919421955 validation 79100:2.510091810320265\n",
      "training 79200:5.196857573152202 validation 79200:2.5100836656031515\n",
      "training 79300:5.196854979751476 validation 79300:2.51007554175564\n",
      "training 79400:5.196852411474202 validation 79400:2.5100674387830333\n",
      "training 79500:5.196849868057644 validation 79500:2.5100593566902414\n",
      "training 79600:5.196847349242102 validation 79600:2.5100512954817757\n",
      "training 79700:5.196844854770878 validation 79700:2.510043255161748\n",
      "training 79800:5.196842384390229 validation 79800:2.510035235733869\n",
      "training 79900:5.19683993784934 validation 79900:2.510027237201444\n",
      "training 80000:5.196837514900276 validation 80000:2.510019259567376\n",
      "training 80100:5.196835115297949 validation 80100:2.510011302834156\n",
      "training 80200:5.196832738800087 validation 80200:2.5100033670038693\n",
      "training 80300:5.196830385167189 validation 80300:2.509995452078188\n",
      "training 80400:5.196828054162496 validation 80400:2.509987558058376\n",
      "training 80500:5.19682574555195 validation 80500:2.5099796849452853\n",
      "training 80600:5.196823459104168 validation 80600:2.50997183273935\n",
      "training 80700:5.196821194590401 validation 80700:2.509964001440592\n",
      "training 80800:5.196818951784503 validation 80800:2.5099561910486194\n",
      "training 80900:5.196816730462897 validation 80900:2.509948401562621\n",
      "training 81000:5.196814530404546 validation 81000:2.509940632981373\n",
      "training 81100:5.196812351390914 validation 81100:2.509932885303234\n",
      "training 81200:5.196810193205943 validation 81200:2.5099251585261446\n",
      "training 81300:5.196808055636014 validation 81300:2.509917452647631\n",
      "training 81400:5.19680593846992 validation 81400:2.5099097676648006\n",
      "training 81500:5.196803841498837 validation 81500:2.509902103574343\n",
      "training 81600:5.19680176451629 validation 81600:2.5098944603725353\n",
      "training 81700:5.196799707318128 validation 81700:2.5098868380552335\n",
      "training 81800:5.196797669702492 validation 81800:2.5098792366178806\n",
      "training 81900:5.196795651469784 validation 81900:2.509871656055504\n",
      "training 82000:5.196793652422644 validation 82000:2.5098640963627163\n",
      "training 82100:5.196791672365923 validation 82100:2.509856557533715\n",
      "training 82200:5.1967897111066454 validation 82200:2.509849039562285\n",
      "training 82300:5.196787768453995 validation 82300:2.5098415424417992\n",
      "training 82400:5.196785844219276 validation 82400:2.509834066165219\n",
      "training 82500:5.196783938215898 validation 82500:2.509826610725094\n",
      "training 82600:5.19678205025934 validation 82600:2.509819176113566\n",
      "training 82700:5.196780180167132 validation 82700:2.5098117623223666\n",
      "training 82800:5.196778327758828 validation 82800:2.509804369342822\n",
      "training 82900:5.1967764928559745 validation 82900:2.5097969971658523\n",
      "training 83000:5.1967746752820965 validation 83000:2.509789645781973\n",
      "training 83100:5.196772874862665 validation 83100:2.5097823151812966\n",
      "training 83200:5.196771091425077 validation 83200:2.5097750053535366\n",
      "training 83300:5.196769324798632 validation 83300:2.5097677162880028\n",
      "training 83400:5.196767574814506 validation 83400:2.5097604479736084\n",
      "training 83500:5.196765841305728 validation 83500:2.509753200398872\n",
      "training 83600:5.196764124107164 validation 83600:2.5097459735519148\n",
      "training 83700:5.196762423055486 validation 83700:2.509738767420466\n",
      "training 83800:5.196760737989155 validation 83800:2.509731581991865\n",
      "training 83900:5.196759068748401 validation 83900:2.5097244172530604\n",
      "training 84000:5.196757415175192 validation 84000:2.509717273190614\n",
      "training 84100:5.196755777113227 validation 84100:2.5097101497907026\n",
      "training 84200:5.1967541544079 validation 84200:2.50970304703912\n",
      "training 84300:5.196752546906294 validation 84300:2.50969596492128\n",
      "training 84400:5.196750954457146 validation 84400:2.5096889034222154\n",
      "training 84500:5.196749376910843 validation 84500:2.509681862526583\n",
      "training 84600:5.1967478141193855 validation 84600:2.5096748422186668\n",
      "training 84700:5.1967462659363814 validation 84700:2.5096678424823757\n",
      "training 84800:5.196744732217018 validation 84800:2.5096608633012507\n",
      "training 84900:5.196743212818051 validation 84900:2.5096539046584643\n",
      "training 85000:5.196741707597775 validation 85000:2.5096469665368244\n",
      "training 85100:5.196740216416016 validation 85100:2.509640048918774\n",
      "training 85200:5.196738739134105 validation 85200:2.5096331517863995\n",
      "training 85300:5.196737275614865 validation 85300:2.5096262751214273\n",
      "training 85400:5.196735825722592 validation 85400:2.5096194189052268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 85500:5.196734389323034 validation 85500:2.509612583118817\n",
      "training 85600:5.196732966283381 validation 85600:2.5096057677428645\n",
      "training 85700:5.196731556472238 validation 85700:2.5095989727576886\n",
      "training 85800:5.196730159759618 validation 85800:2.5095921981432654\n",
      "training 85900:5.1967287760169185 validation 85900:2.5095854438792244\n",
      "training 86000:5.196727405116908 validation 86000:2.5095787099448583\n",
      "training 86100:5.196726046933711 validation 86100:2.509571996319122\n",
      "training 86200:5.196724701342789 validation 86200:2.5095653029806337\n",
      "training 86300:5.196723368220925 validation 86300:2.5095586299076813\n",
      "training 86400:5.196722047446211 validation 86400:2.509551977078225\n",
      "training 86500:5.19672073889803 validation 86500:2.509545344469894\n",
      "training 86600:5.196719442457039 validation 86600:2.5095387320599962\n",
      "training 86700:5.196718158005161 validation 86700:2.5095321398255197\n",
      "training 86800:5.196716885425562 validation 86800:2.5095255677431325\n",
      "training 86900:5.196715624602643 validation 86900:2.5095190157891882\n",
      "training 87000:5.196714375422019 validation 87000:2.5095124839397256\n",
      "training 87100:5.196713137770512 validation 87100:2.5095059721704764\n",
      "training 87200:5.196711911536133 validation 87200:2.509499480456864\n",
      "training 87300:5.196710696608067 validation 87300:2.5094930087740064\n",
      "training 87400:5.196709492876662 validation 87400:2.509486557096723\n",
      "training 87500:5.196708300233415 validation 87500:2.509480125399531\n",
      "training 87600:5.196707118570958 validation 87600:2.5094737136566536\n",
      "training 87700:5.1967059477830455 validation 87700:2.5094673218420223\n",
      "training 87800:5.196704787764539 validation 87800:2.509460949929277\n",
      "training 87900:5.196703638411399 validation 87900:2.5094545978917706\n",
      "training 88000:5.196702499620668 validation 88000:2.5094482657025714\n",
      "training 88100:5.196701371290461 validation 88100:2.509441953334467\n",
      "training 88200:5.19670025331995 validation 88200:2.509435660759968\n",
      "training 88300:5.196699145609357 validation 88300:2.509429387951304\n",
      "training 88400:5.196698048059936 validation 88400:2.5094231348804388\n",
      "training 88500:5.1966969605739655 validation 88500:2.5094169015190615\n",
      "training 88600:5.196695883054733 validation 88600:2.5094106878385953\n",
      "training 88700:5.196694815406531 validation 88700:2.5094044938102003\n",
      "training 88800:5.196693757534632 validation 88800:2.5093983194047733\n",
      "training 88900:5.196692709345292 validation 88900:2.5093921645929553\n",
      "training 89000:5.196691670745732 validation 89000:2.5093860293451287\n",
      "training 89100:5.196690641644128 validation 89100:2.5093799136314265\n",
      "training 89200:5.196689621949597 validation 89200:2.509373817421731\n",
      "training 89300:5.196688611572193 validation 89300:2.509367740685675\n",
      "training 89400:5.19668761042289 validation 89400:2.5093616833926498\n",
      "training 89500:5.196686618413579 validation 89500:2.509355645511806\n",
      "training 89600:5.196685635457048 validation 89600:2.509349627012054\n",
      "training 89700:5.196684661466982 validation 89700:2.50934362786207\n",
      "training 89800:5.196683696357941 validation 89800:2.5093376480302982\n",
      "training 89900:5.196682740045365 validation 89900:2.5093316874849507\n",
      "training 90000:5.196681792445553 validation 90000:2.5093257461940133\n",
      "training 90100:5.1966808534756534 validation 90100:2.50931982412525\n",
      "training 90200:5.196679923053662 validation 90200:2.5093139212462012\n",
      "training 90300:5.196679001098407 validation 90300:2.5093080375241894\n",
      "training 90400:5.196678087529542 validation 90400:2.5093021729263216\n",
      "training 90500:5.196677182267534 validation 90500:2.509296327419492\n",
      "training 90600:5.196676285233655 validation 90600:2.509290500970384\n",
      "training 90700:5.1966753963499785 validation 90700:2.5092846935454753\n",
      "training 90800:5.19667451553936 validation 90800:2.509278905111036\n",
      "training 90900:5.196673642725441 validation 90900:2.509273135633139\n",
      "training 91000:5.196672777832631 validation 91000:2.509267385077653\n",
      "training 91100:5.196671920786101 validation 91100:2.5092616534102543\n",
      "training 91200:5.196671071511775 validation 91200:2.5092559405964256\n",
      "training 91300:5.196670229936329 validation 91300:2.5092502466014546\n",
      "training 91400:5.1966693959871675 validation 91400:2.5092445713904468\n",
      "training 91500:5.196668569592431 validation 91500:2.5092389149283165\n",
      "training 91600:5.1966677506809775 validation 91600:2.5092332771797987\n",
      "training 91700:5.196666939182381 validation 91700:2.5092276581094484\n",
      "training 91800:5.196666135026918 validation 91800:2.509222057681642\n",
      "training 91900:5.196665338145567 validation 91900:2.5092164758605824\n",
      "training 92000:5.196664548469992 validation 92000:2.5092109126102984\n",
      "training 92100:5.196663765932543 validation 92100:2.5092053678946504\n",
      "training 92200:5.196662990466244 validation 92200:2.509199841677334\n",
      "training 92300:5.196662222004786 validation 92300:2.509194333921876\n",
      "training 92400:5.196661460482521 validation 92400:2.5091888445916455\n",
      "training 92500:5.196660705834457 validation 92500:2.5091833736498517\n",
      "training 92600:5.1966599579962445 validation 92600:2.5091779210595457\n",
      "training 92700:5.196659216904175 validation 92700:2.509172486783625\n",
      "training 92800:5.196658482495172 validation 92800:2.5091670707848373\n",
      "training 92900:5.196657754706784 validation 92900:2.5091616730257815\n",
      "training 93000:5.196657033477181 validation 93000:2.509156293468906\n",
      "training 93100:5.196656318745141 validation 93100:2.50915093207652\n",
      "training 93200:5.19665561045005 validation 93200:2.50914558881079\n",
      "training 93300:5.196654908531893 validation 93300:2.509140263633742\n",
      "training 93400:5.196654212931246 validation 93400:2.509134956507267\n",
      "training 93500:5.196653523589272 validation 93500:2.5091296673931214\n",
      "training 93600:5.196652840447715 validation 93600:2.5091243962529304\n",
      "training 93700:5.196652163448891 validation 93700:2.5091191430481903\n",
      "training 93800:5.1966514925356835 validation 93800:2.509113907740269\n",
      "training 93900:5.196650827651539 validation 93900:2.50910869029041\n",
      "training 94000:5.196650168740458 validation 94000:2.509103490659737\n",
      "training 94100:5.196649515746991 validation 94100:2.509098308809252\n",
      "training 94200:5.196648868616232 validation 94200:2.5090931446998415\n",
      "training 94300:5.196648227293814 validation 94300:2.509087998292273\n",
      "training 94400:5.1966475917259 validation 94400:2.509082869547206\n",
      "training 94500:5.196646961859182 validation 94500:2.5090777584251858\n",
      "training 94600:5.196646337640871 validation 94600:2.509072664886652\n",
      "training 94700:5.1966457190186945 validation 94700:2.509067588891936\n",
      "training 94800:5.19664510594089 validation 94800:2.5090625304012684\n",
      "training 94900:5.196644498356199 validation 94900:2.5090574893747744\n",
      "training 95000:5.196643896213862 validation 95000:2.509052465772484\n",
      "training 95100:5.196643299463615 validation 95100:2.509047459554328\n",
      "training 95200:5.196642708055681 validation 95200:2.5090424706801433\n",
      "training 95300:5.196642121940768 validation 95300:2.509037499109672\n",
      "training 95400:5.196641541070064 validation 95400:2.509032544802567\n",
      "training 95500:5.196640965395225 validation 95500:2.509027607718394\n",
      "training 95600:5.196640394868381 validation 95600:2.5090226878166315\n",
      "training 95700:5.196639829442126 validation 95700:2.5090177850566717\n",
      "training 95800:5.196639269069508 validation 95800:2.509012899397827\n",
      "training 95900:5.196638713704034 validation 95900:2.5090080307993303\n",
      "training 96000:5.196638163299656 validation 96000:2.5090031792203353\n",
      "training 96100:5.196637617810776 validation 96100:2.508998344619919\n",
      "training 96200:5.1966370771922294 validation 96200:2.5089935269570858\n",
      "training 96300:5.196636541399293 validation 96300:2.508988726190768\n",
      "training 96400:5.196636010387673 validation 96400:2.508983942279827\n",
      "training 96500:5.196635484113498 validation 96500:2.5089791751830566\n",
      "training 96600:5.196634962533324 validation 96600:2.5089744248591863\n",
      "training 96700:5.196634445604121 validation 96700:2.5089696912668793\n",
      "training 96800:5.196633933283277 validation 96800:2.508964974364738\n",
      "training 96900:5.19663342552858 validation 96900:2.5089602741113053\n",
      "training 97000:5.1966329222982335 validation 97000:2.508955590465063\n",
      "training 97100:5.196632423550836 validation 97100:2.5089509233844387\n",
      "training 97200:5.196631929245382 validation 97200:2.5089462728278056\n",
      "training 97300:5.196631439341261 validation 97300:2.5089416387534844\n",
      "training 97400:5.196630953798249 validation 97400:2.5089370211197424\n",
      "training 97500:5.196630472576508 validation 97500:2.5089324198848013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 97600:5.196629995636578 validation 97600:2.508927835006834\n",
      "training 97700:5.1966295229393795 validation 97700:2.5089232664439676\n",
      "training 97800:5.196629054446203 validation 97800:2.5089187141542846\n",
      "training 97900:5.196628590118707 validation 97900:2.5089141780958286\n",
      "training 98000:5.19662812991892 validation 98000:2.508909658226601\n",
      "training 98100:5.196627673809225 validation 98100:2.508905154504565\n",
      "training 98200:5.196627221752368 validation 98200:2.5089006668876475\n",
      "training 98300:5.196626773711448 validation 98300:2.5088961953337403\n",
      "training 98400:5.196626329649913 validation 98400:2.5088917398007013\n",
      "training 98500:5.1966258895315605 validation 98500:2.508887300246357\n",
      "training 98600:5.196625453320528 validation 98600:2.508882876628505\n",
      "training 98700:5.196625020981295 validation 98700:2.5088784689049133\n",
      "training 98800:5.19662459247868 validation 98800:2.5088740770333224\n",
      "training 98900:5.19662416777783 validation 98900:2.5088697009714505\n",
      "training 99000:5.1966237468442245 validation 99000:2.50886534067699\n",
      "training 99100:5.196623329643668 validation 99100:2.5088609961076127\n",
      "training 99200:5.19662291614229 validation 99200:2.508856667220967\n",
      "training 99300:5.196622506306536 validation 99300:2.508852353974687\n",
      "training 99400:5.196622100103174 validation 99400:2.5088480563263866\n",
      "training 99500:5.19662169749928 validation 99500:2.508843774233664\n",
      "training 99600:5.196621298462243 validation 99600:2.5088395076541037\n",
      "training 99700:5.19662090295976 validation 99700:2.5088352565452783\n",
      "training 99800:5.19662051095983 validation 99800:2.5088310208647466\n",
      "training 99900:5.196620122430754 validation 99900:2.508826800570059\n",
      "Use learning rate: 1\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:8.49033379798537 validation 100:4.593146029108891\n",
      "training 200:6.305334500302518 validation 200:3.2161068525741707\n",
      "training 300:5.677816694928922 validation 300:2.820356889395374\n",
      "training 400:5.476030619871812 validation 400:2.697981765284061\n",
      "training 500:5.39173359122238 validation 500:2.6462566163190546\n",
      "training 600:5.344811832322643 validation 600:2.6152692378738496\n",
      "training 700:5.313310657711986 validation 700:2.592891892948612\n",
      "training 800:5.290124706727844 validation 800:2.5756599979205808\n",
      "training 900:5.2723050428731115 validation 900:2.562135412075649\n",
      "training 1000:5.258294372409034 validation 1000:2.551446873010692\n",
      "training 1100:5.247123824326192 validation 1100:2.542962226689324\n",
      "training 1200:5.23813184577943 validation 1200:2.536199202654448\n",
      "training 1300:5.230842182702717 validation 1300:2.5307864189564677\n",
      "training 1400:5.224900382537836 validation 1400:2.5264373807398663\n",
      "training 1500:5.220036454594813 validation 1500:2.5229305511120543\n",
      "training 1600:5.21604115566264 validation 1600:2.520093918346672\n",
      "training 1700:5.21275013648186 validation 1700:2.5177932081341434\n",
      "training 1800:5.210032937357224 validation 1800:2.515922979335149\n",
      "training 1900:5.20778512010263 validation 1900:2.5143999306341116\n",
      "training 2000:5.205922504407096 validation 2000:2.5131578680867466\n",
      "training 2100:5.204376859710305 validation 2100:2.5121439074628595\n",
      "training 2200:5.203092630112777 validation 2200:2.5113155916025445\n",
      "training 2300:5.202024408989936 validation 2300:2.5106386868326007\n",
      "training 2400:5.201134968289796 validation 2400:2.510085485646382\n",
      "training 2500:5.2003937051465385 validation 2500:2.5096334892793797\n",
      "training 2600:5.199775407040969 validation 2600:2.5092643775271966\n",
      "training 2700:5.199259263176039 validation 2700:2.5089631975179127\n",
      "training 2800:5.198828068221897 validation 2800:2.5087177207616644\n",
      "training 2900:5.198467577758169 validation 2900:2.508517930572186\n",
      "training 3000:5.198165984292273 validation 3000:2.5083556112690437\n",
      "training 3100:5.197913489768054 validation 3100:2.508224017409453\n",
      "training 3200:5.197701955736838 validation 3200:2.508117606361813\n",
      "training 3300:5.197524616344257 validation 3300:2.5080318213129753\n",
      "training 3400:5.197375842336856 validation 3400:2.5079629146476057\n",
      "training 3500:5.197250946654712 validation 3500:2.507907803800063\n",
      "training 3600:5.197146024023206 validation 3600:2.507863953335541\n",
      "training 3700:5.197057818412727 validation 3700:2.507829278296375\n",
      "training 3800:5.196983613391108 validation 3800:2.507802064845033\n",
      "training 3900:5.196921141317275 validation 3900:2.507780905015767\n",
      "training 4000:5.196868508066895 validation 4000:2.5077646430027376\n",
      "training 4100:5.196824130580034 validation 4100:2.507752330901452\n",
      "training 4200:5.196786685006733 validation 4200:2.5077431922106084\n",
      "training 4300:5.196755063621576 validation 4300:2.5077365917146457\n",
      "training 4400:5.1967283390009635 validation 4400:2.5077320106195558\n",
      "training 4500:5.196705734220597 validation 4500:2.507729026018681\n",
      "training 4600:5.196686598047116 validation 4600:2.507727293930856\n",
      "training 4700:5.196670384275618 validation 4700:2.507726535288196\n",
      "training 4800:5.196656634511028 validation 4800:2.507726524360928\n",
      "training 4900:5.196644963811923 validation 4900:2.507727079196826\n",
      "training 5000:5.196635048714881 validation 5000:2.5077280537266926\n",
      "early stopping at iter 4752\n",
      "Use learning rate: 10\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.046884573339843 validation 100:3.129793925373246\n",
      "training 200:5.689574928231222 validation 200:2.8859204213933403\n",
      "training 300:5.5371341249282695 validation 300:2.770214661857582\n",
      "training 400:5.451157773294572 validation 400:2.7006923914468683\n",
      "training 500:5.395387188875675 validation 500:2.6543411023581887\n",
      "training 600:5.356070822585828 validation 600:2.621597695388315\n",
      "training 700:5.326807072949664 validation 700:2.5975645501404356\n",
      "training 800:5.304201959621865 validation 800:2.5794262005399013\n",
      "training 900:5.286280913464895 validation 900:2.5654435192700005\n",
      "training 1000:5.271808315140357 validation 1000:2.554485268006435\n",
      "training 1100:5.259963342102254 validation 1100:2.545785021086534\n",
      "training 1200:5.250173121065974 validation 1200:2.53880582385719\n",
      "training 1300:5.242021349464694 validation 1300:2.5331608034075384\n",
      "training 1400:5.235195498137026 validation 1400:2.528564576311392\n",
      "training 1500:5.229454795657604 validation 1500:2.5248023959550343\n",
      "training 1600:5.224609931328926 validation 1600:2.521709920057703\n",
      "training 1700:5.220509650285372 validation 1700:2.519159557707007\n",
      "training 1800:5.217031566128346 validation 1800:2.5170510206608108\n",
      "training 1900:5.2140756533751595 validation 1900:2.515304637316566\n",
      "training 2000:5.211559503947813 validation 2000:2.51385652818182\n",
      "training 2100:5.209414783455647 validation 2100:2.512655063971784\n",
      "training 2200:5.207584528060976 validation 2200:2.511658225149967\n",
      "training 2300:5.206021046040556 validation 2300:2.5108316062399973\n",
      "training 2400:5.204684264590327 validation 2400:2.5101468886050897\n",
      "training 2500:5.203540411184718 validation 2500:2.509580658438016\n",
      "training 2600:5.202560950779248 validation 2600:2.50911348242587\n",
      "training 2700:5.201721721661092 validation 2700:2.5087291780408183\n",
      "training 2800:5.201002227577331 validation 2800:2.5084142324605705\n",
      "training 2900:5.200385054210067 validation 2900:2.508157336165352\n",
      "training 3000:5.199855385565935 validation 3000:2.5079490058710863\n",
      "training 3100:5.199400601331067 validation 3100:2.5077812776895088\n",
      "training 3200:5.199009940318695 validation 3200:2.507647455961796\n",
      "training 3300:5.198674218211728 validation 3300:2.5075419065769347\n",
      "training 3400:5.19838559015407 validation 3400:2.5074598860947326\n",
      "training 3500:5.198137350564445 validation 3500:2.5073973998811945\n",
      "training 3600:5.1979237639709925 validation 3600:2.507351083897324\n",
      "training 3700:5.197739921790753 validation 3700:2.5073181058802425\n",
      "training 3800:5.197581620876379 validation 3800:2.5072960825035335\n",
      "training 3900:5.197445260374453 validation 3900:2.507283009764205\n",
      "training 4000:5.197327754024789 validation 4000:2.507277204362168\n",
      "training 4100:5.197226455506877 validation 4100:2.5072772542483817\n",
      "training 4200:5.197139094830592 validation 4200:2.50728197684472\n",
      "early stopping at iter 4049\n",
      "Use learning rate: 100\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.07570206300596 validation 100:3.149122033252096\n",
      "training 200:5.708482938814458 validation 200:2.899883838477167\n",
      "training 300:5.551127836211838 validation 300:2.781495275829524\n",
      "training 400:5.462258766761482 validation 400:2.710137537273305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 500:5.404587095994841 validation 500:2.662362619745554\n",
      "training 600:5.363921812625104 validation 600:2.628466236352276\n",
      "training 700:5.333642246541786 validation 700:2.603482966158103\n",
      "training 800:5.3102332059780695 validation 800:2.5845524863986014\n",
      "training 900:5.291649423815118 validation 900:2.5699025498353856\n",
      "training 1000:5.276612873868813 validation 1000:2.558376954145005\n",
      "training 1100:5.2642767547203455 validation 1100:2.5491903608107087\n",
      "training 1200:5.254051901698825 validation 1200:2.54179137014789\n",
      "training 1300:5.245511474684761 validation 1300:2.535781935778285\n",
      "training 1400:5.238335841471147 validation 1400:2.5308678984701234\n",
      "training 1500:5.232279191066629 validation 1500:2.526827509507473\n",
      "training 1600:5.227148427009191 validation 1600:2.5234907559917596\n",
      "training 1700:5.222789285418574 validation 1700:2.5207253928613222\n",
      "training 1800:5.2190768672616095 validation 1800:2.518427266769924\n",
      "training 1900:5.215908966332377 validation 1900:2.516513462965121\n",
      "training 2000:5.2132012292945324 validation 2000:2.514917355725414\n",
      "training 2100:5.210883555247528 validation 2100:2.51358497130401\n",
      "training 2200:5.208897358933415 validation 2200:2.5124722740034313\n",
      "training 2300:5.20719345189724 validation 2300:2.5115431130870034\n",
      "training 2400:5.20573037640612 validation 2400:2.5107676502648846\n",
      "training 2500:5.204473078090285 validation 2500:2.5101211416357208\n",
      "training 2600:5.203391836660422 validation 2600:2.50958298443123\n",
      "training 2700:5.202461396406584 validation 2700:2.5091359639207442\n",
      "training 2800:5.201660253501605 validation 2800:2.508765653264045\n",
      "training 2900:5.200970067860907 validation 2900:2.508459931425599\n",
      "training 3000:5.200375174977022 validation 3000:2.5082085930897797\n",
      "training 3100:5.199862178726283 validation 3100:2.5080030309113015\n",
      "training 3200:5.199419610273226 validation 3200:2.5078359751172195\n",
      "training 3300:5.199037641299532 validation 3300:2.5077012789390016\n",
      "training 3400:5.198707842146388 validation 3400:2.5075937409371396\n",
      "training 3500:5.198422977281176 validation 3500:2.507508957226517\n",
      "training 3600:5.198176831920888 validation 3600:2.507443198088606\n",
      "training 3700:5.197964064765346 validation 3700:2.507393304588424\n",
      "training 3800:5.197780082685224 validation 3800:2.507356601688161\n",
      "training 3900:5.1976209339257515 validation 3900:2.5073308250296726\n",
      "training 4000:5.197483216966135 validation 4000:2.507314059091574\n",
      "training 4100:5.197364002646336 validation 4100:2.5073046848483895\n",
      "training 4200:5.197260767559514 validation 4200:2.5073013353948497\n",
      "training 4300:5.19717133702699 validation 4300:2.507302858267477\n",
      "training 4400:5.197093836236543 validation 4400:2.507308283412684\n",
      "early stopping at iter 4216\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = [10**power for power in range(-1,3)]\n",
    "loss_history = []\n",
    "validation_loss_history = []\n",
    "best_validation = []\n",
    "\n",
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "iter_time = 100000\n",
    "\n",
    "early_stopping_iter = 250\n",
    "eps = 0.0000000001\n",
    "\n",
    "for lr in learning_rate_list:\n",
    "    print(\"Use learning rate:\", lr)\n",
    "    history = []\n",
    "    validation_history = []\n",
    "    w = np.zeros([dim, 1])\n",
    "    learning_rate = lr\n",
    "    adagrad = np.zeros([dim, 1])\n",
    "    temp_loss = np.inf\n",
    "    temp_iter = 0\n",
    "    for t in range(iter_time):\n",
    "        loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "        history.append(loss)\n",
    "        validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "        validation_history.append(validation_loss)\n",
    "        if validation_loss < temp_loss:\n",
    "            temp_w = np.copy(w)\n",
    "            temp_loss = np.copy(validation_loss)\n",
    "            temp_iter = 0\n",
    "        else:\n",
    "            if temp_iter < early_stopping_iter:\n",
    "                temp_iter += 1\n",
    "            else:\n",
    "                print(\"early stopping at iter\", t-temp_iter)\n",
    "                best_validation.append((lr,temp_loss))\n",
    "                w = temp_w\n",
    "                break\n",
    "        if(t%100==0):\n",
    "            print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "        gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "        adagrad += gradient ** 2\n",
    "        w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "    loss_history.append(np.array(history))\n",
    "    validation_loss_history.append(np.array(validation_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, array(2.50772645)), (10, array(2.50727658)), (100, array(2.50730128))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will find that the best learning rate $\\eta$ should be between 10 and 100\n",
    "### Let's try to tune the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.],\n",
       "       [38.],\n",
       "       [45.],\n",
       "       ...,\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [ 5.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.dot(train_x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use learning rate: 1\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:8.495138279083523 validation 100:4.583482915914626\n",
      "training 200:6.305840427421834 validation 200:3.2204616074907664\n",
      "training 300:5.691683258548431 validation 300:2.825704729564583\n",
      "training 400:5.486230592551655 validation 400:2.7106902205979373\n",
      "training 500:5.394471102094957 validation 500:2.65389723973897\n",
      "training 600:5.350820587768866 validation 600:2.621127885891252\n",
      "training 700:5.314740015238981 validation 700:2.6003225342979897\n",
      "training 800:5.292221454776598 validation 800:2.5841807390356486\n",
      "training 900:5.268196316336295 validation 900:2.5679374800854755\n",
      "training 1000:5.260432629624095 validation 1000:2.556924461822797\n",
      "training 1100:5.253381615737255 validation 1100:2.547600622419404\n",
      "training 1200:5.245680324469374 validation 1200:2.5411684625363553\n",
      "training 1300:5.240989976876617 validation 1300:2.5344058518593933\n",
      "training 1400:5.232222284592438 validation 1400:2.5271701205335346\n",
      "training 1500:5.225234784798026 validation 1500:2.52124729015009\n",
      "training 1600:5.219814309875635 validation 1600:2.5228607987856844\n",
      "training 1700:5.221898470637895 validation 1700:2.5210016656962027\n",
      "training 1800:5.21927195276452 validation 1800:2.5220541735001167\n",
      "training 1900:5.211639078305352 validation 1900:2.516646630310702\n",
      "training 2000:5.213217455176896 validation 2000:2.51566219180351\n",
      "training 2100:5.208905488743117 validation 2100:2.5145014659134084\n",
      "training 2200:5.206425340539327 validation 2200:2.5146070085886807\n",
      "early stopping at iter 2027\n",
      "Use learning rate: 2\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.042240436917667 validation 100:3.0984095182555684\n",
      "training 200:5.658027009396988 validation 200:2.8596257283847826\n",
      "training 300:5.503955342854435 validation 300:2.7505709373229363\n",
      "training 400:5.428164344681012 validation 400:2.6797053846154193\n",
      "training 500:5.377981241727839 validation 500:2.640965585068179\n",
      "training 600:5.342680238440196 validation 600:2.613996839380564\n",
      "training 700:5.311776365102944 validation 700:2.5895155804242096\n",
      "training 800:5.288743404066339 validation 800:2.5769474527234086\n",
      "training 900:5.270043127409475 validation 900:2.5582734247499865\n",
      "training 1000:5.262416651717861 validation 1000:2.550585184648261\n",
      "training 1100:5.252926929865147 validation 1100:2.5447168534276847\n",
      "training 1200:5.247046147759347 validation 1200:2.539880075853369\n",
      "training 1300:5.240466692344676 validation 1300:2.5315070520751664\n",
      "training 1400:5.236126480873855 validation 1400:2.522194474689227\n",
      "training 1500:5.231156998080866 validation 1500:2.519422079565776\n",
      "training 1600:5.224963894605324 validation 1600:2.5202646487035048\n",
      "training 1700:5.2216612910412055 validation 1700:2.518368471724742\n",
      "training 1800:5.219695674072992 validation 1800:2.518368471724742\n",
      "training 1900:5.2147614260252535 validation 1900:2.5159786604734027\n",
      "training 2000:5.209924385431215 validation 2000:2.512600939654622\n",
      "training 2100:5.208684701523675 validation 2100:2.5145014659134084\n",
      "training 2200:5.206306399643024 validation 2200:2.512988200466318\n",
      "early stopping at iter 2011\n",
      "Use learning rate: 3\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.001076218648123 validation 100:3.0968102211223014\n",
      "training 200:5.667806179919632 validation 200:2.867534308039723\n",
      "training 300:5.512611802847076 validation 300:2.751824975095582\n",
      "training 400:5.4330024777803 validation 400:2.6847186227815873\n",
      "training 500:5.385263370637063 validation 500:2.643242395417829\n",
      "training 600:5.34628867044894 validation 600:2.612845940077934\n",
      "training 700:5.319481747128432 validation 700:2.594771268045582\n",
      "training 800:5.295379817996401 validation 800:2.576535470713946\n",
      "training 900:5.273080563298334 validation 900:2.562557716885776\n",
      "training 1000:5.2605671632420865 validation 1000:2.554016583288778\n",
      "training 1100:5.257084997901331 validation 1100:2.5490933403368086\n",
      "training 1200:5.248782422082916 validation 1200:2.5428736996776666\n",
      "training 1300:5.240871820420246 validation 1300:2.534580372476581\n",
      "training 1400:5.235349254037218 validation 1400:2.52443823324894\n",
      "training 1500:5.233676135201831 validation 1500:2.520756017308475\n",
      "training 1600:5.228907347170506 validation 1600:2.5184035990877414\n",
      "training 1700:5.223964865616676 validation 1700:2.5179117714101507\n",
      "training 1800:5.222271159668913 validation 1800:2.517946905144483\n",
      "training 1900:5.218458311408863 validation 1900:2.517068414665112\n",
      "training 2000:5.21379437690737 validation 2000:2.5143959188079443\n",
      "training 2100:5.209109284024352 validation 2100:2.5135865770921564\n",
      "training 2200:5.20756363739406 validation 2200:2.5129177938486547\n",
      "training 2300:5.207070972284073 validation 2300:2.512776974695064\n",
      "training 2400:5.205932567706251 validation 2400:2.5115444703064616\n",
      "training 2500:5.205456742776759 validation 2500:2.5104523182446146\n",
      "training 2600:5.204164998665331 validation 2600:2.5099941775646317\n",
      "training 2700:5.205150832352914 validation 2700:2.5104875563723947\n",
      "training 2800:5.204181997380012 validation 2800:2.511967111364616\n",
      "early stopping at iter 2595\n",
      "Use learning rate: 4\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.019915638009169 validation 100:3.104798468740488\n",
      "training 200:5.674186343271967 validation 200:2.8702478495109234\n",
      "training 300:5.520277232337916 validation 300:2.7555515603925103\n",
      "training 400:5.438926172064327 validation 400:2.691530844581187\n",
      "training 500:5.389778930008781 validation 500:2.6431085196886395\n",
      "training 600:5.350936316397093 validation 600:2.6190345217087794\n",
      "training 700:5.32418602505969 validation 700:2.594941728840797\n",
      "training 800:5.302408351318583 validation 800:2.578114377565026\n",
      "training 900:5.276049182411969 validation 900:2.569934776572869\n",
      "training 1000:5.262567944736231 validation 1000:2.555817088682153\n",
      "training 1100:5.2592048513810346 validation 1100:2.5494056590580807\n",
      "training 1200:5.251798465250135 validation 1200:2.5439867069582864\n",
      "training 1300:5.245629731592051 validation 1300:2.5353830125887704\n",
      "training 1400:5.23876143501165 validation 1400:2.52738014339805\n",
      "training 1500:5.2365150510323755 validation 1500:2.5204401484877574\n",
      "training 1600:5.228738161362159 validation 1600:2.5184035990877414\n",
      "training 1700:5.223473747430153 validation 1700:2.5195625273223623\n",
      "training 1800:5.22152575500601 validation 1800:2.51636540146442\n",
      "training 1900:5.219865152965648 validation 1900:2.518614352977318\n",
      "training 2000:5.213981014388196 validation 2000:2.516013821201786\n",
      "training 2100:5.213760442102145 validation 2100:2.514184811304209\n",
      "training 2200:5.207818445887107 validation 2200:2.5130234030354397\n",
      "training 2300:5.207206884556289 validation 2300:2.5116149154216934\n",
      "training 2400:5.206731176082154 validation 2400:2.5137977348353084\n",
      "training 2500:5.205592697194724 validation 2500:2.5104875563723947\n",
      "training 2600:5.206255424141355 validation 2600:2.511474023215293\n",
      "training 2700:5.20430098682814 validation 2700:2.5102761201858597\n",
      "training 2800:5.205932567706251 validation 2800:2.51143879892868\n",
      "training 2900:5.204198996039168 validation 2900:2.5124601027418887\n",
      "early stopping at iter 2660\n",
      "Use learning rate: 5\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.025335751055801 validation 100:3.1162598937519532\n",
      "training 200:5.680808505798014 validation 200:2.8718192965760982\n",
      "training 300:5.526203423200513 validation 300:2.7573808818644587\n",
      "training 400:5.440715030440654 validation 400:2.6923852675415407\n",
      "training 500:5.394257910089781 validation 500:2.6470215873304372\n",
      "training 600:5.3549191623841335 validation 600:2.6220727283546728\n",
      "training 700:5.329035556039869 validation 700:2.597020449640856\n",
      "training 800:5.3063776073798135 validation 800:2.580686610743993\n",
      "training 900:5.2828187644158815 validation 900:2.572721509613842\n",
      "training 1000:5.264483946631936 validation 1000:2.5560939769434805\n",
      "training 1100:5.260668061197756 validation 1100:2.5482255874877975\n",
      "training 1200:5.252623784084168 validation 1200:2.543082425647594\n",
      "training 1300:5.247181024596276 validation 1300:2.538103119487496\n",
      "training 1400:5.240449811328516 validation 1400:2.5309478655974442\n",
      "training 1500:5.235146480233149 validation 1500:2.5227556014182007\n",
      "training 1600:5.2308018543653505 validation 1600:2.519457192238919\n",
      "training 1700:5.224811512699892 validation 1700:2.5189304507491377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 1800:5.223660039074294 validation 1800:2.518544103640355\n",
      "training 1900:5.221966234258308 validation 1900:2.5185089782370684\n",
      "training 2000:5.216304939874662 validation 2000:2.5163302456486734\n",
      "training 2100:5.214218543338288 validation 2100:2.5144311016687104\n",
      "training 2200:5.208684701523675 validation 2200:2.513621771281181\n",
      "training 2300:5.207172906820808 validation 2300:2.512882589800071\n",
      "training 2400:5.206918066737319 validation 2400:2.51344579540804\n",
      "early stopping at iter 2180\n",
      "Use learning rate: 6\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.036263965484358 validation 100:3.1222156739711666\n",
      "training 200:5.685104884811649 validation 200:2.878588255860831\n",
      "training 300:5.528972138535777 validation 300:2.7619008398411697\n",
      "training 400:5.446841478768562 validation 400:2.6955048948866898\n",
      "training 500:5.395225406162188 validation 500:2.651696301750298\n",
      "training 600:5.3575287159256035 validation 600:2.621904031458232\n",
      "training 700:5.331242955523024 validation 700:2.5996080057269193\n",
      "training 800:5.307761145655929 validation 800:2.581063656054025\n",
      "training 900:5.282048408118215 validation 900:2.574680735881586\n",
      "training 1000:5.265492088646418 validation 1000:2.556889863671668\n",
      "training 1100:5.258733846678803 validation 1100:2.548329733431324\n",
      "training 1200:5.25312901733367 validation 1200:2.543882383212289\n",
      "training 1300:5.246590912835925 validation 1300:2.5393575707063687\n",
      "training 1400:5.242103892531993 validation 1400:2.532485331308134\n",
      "training 1500:5.235467535129427 validation 1500:2.5226854674027743\n",
      "training 1600:5.231038619521392 validation 1600:2.519457192238919\n",
      "training 1700:5.225895020814855 validation 1700:2.5190709259158948\n",
      "training 1800:5.224676058386233 validation 1800:2.516998122181061\n",
      "training 1900:5.2213393872129625 validation 1900:2.5180523034062086\n",
      "training 2000:5.21850916771015 validation 2000:2.5173144229020905\n",
      "training 2100:5.214082813834647 validation 2100:2.515556693399496\n",
      "training 2200:5.20953383192111 validation 2200:2.511967111364616\n",
      "training 2300:5.2080052975253075 validation 2300:2.5137977348353084\n",
      "training 2400:5.2062384321965505 validation 2400:2.5122840454983177\n",
      "training 2500:5.205762635224144 validation 2500:2.5132698072132973\n",
      "training 2600:5.204997870398828 validation 2600:2.511579693111059\n",
      "training 2700:5.204997870398828 validation 2700:2.5106989747530273\n",
      "training 2800:5.204589949876481 validation 2800:2.5102056374993698\n",
      "training 2900:5.2050488582163155 validation 2900:2.5131994084847724\n",
      "early stopping at iter 2731\n",
      "Use learning rate: 7\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.045372792557197 validation 100:3.124339986918364\n",
      "training 200:5.6863496052370435 validation 200:2.883899960278047\n",
      "training 300:5.532698912291302 validation 300:2.7651020055692546\n",
      "training 400:5.446906443929177 validation 400:2.6978340503708544\n",
      "training 500:5.396717305797832 validation 500:2.655563402582963\n",
      "training 600:5.357991035878281 validation 600:2.6229497773163963\n",
      "training 700:5.331093611457781 validation 700:2.599403818631154\n",
      "training 800:5.308444447170068 validation 800:2.584317667409783\n",
      "training 900:5.28368946672211 validation 900:2.5737528672458145\n",
      "training 1000:5.267205487232061 validation 1000:2.5570282534673785\n",
      "training 1100:5.260499896863166 validation 1100:2.550932000143473\n",
      "training 1200:5.251512099838366 validation 1200:2.543743278228824\n",
      "training 1300:5.246405435431278 validation 1300:2.5405417614386083\n",
      "training 1400:5.242272646983163 validation 1400:2.532345600270105\n",
      "training 1500:5.23362542628134 validation 1500:2.523071180362299\n",
      "training 1600:5.233270450079961 validation 1600:2.5205454424928786\n",
      "training 1700:5.227739853641643 validation 1700:2.5182279573727437\n",
      "training 1800:5.223287449141809 validation 1800:2.5179117714101507\n",
      "training 1900:5.221373272761026 validation 1900:2.51759554573762\n",
      "training 2000:5.219543138364545 validation 2000:2.5185792285537647\n",
      "training 2100:5.215389063687714 validation 2100:2.515591860025764\n",
      "training 2200:5.213047760164135 validation 2200:2.512741768673574\n",
      "training 2300:5.207954338652465 validation 2300:2.512600939654622\n",
      "training 2400:5.207495686356863 validation 2400:2.5118966761265757\n",
      "training 2500:5.205439748224846 validation 2500:2.514466284037191\n",
      "training 2600:5.205150832352914 validation 2600:2.5112978968415556\n",
      "training 2700:5.206204448140569 validation 2700:2.512178405229764\n",
      "training 2800:5.204080004259057 validation 2800:2.5102761201858597\n",
      "training 2900:5.205813615551219 validation 2900:2.510839910450169\n",
      "training 3000:5.203808012830114 validation 3000:2.5121431908197196\n",
      "early stopping at iter 2752\n",
      "Use learning rate: 8\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.046923732295936 validation 100:3.1249911535614343\n",
      "training 200:5.688729374532923 validation 200:2.885555946610481\n",
      "training 300:5.5342496619320025 validation 300:2.7656138482328214\n",
      "training 400:5.447832113291232 validation 400:2.699047038977439\n",
      "training 500:5.398831490044027 validation 500:2.656029740878345\n",
      "training 600:5.358849524257763 validation 600:2.6237591007608136\n",
      "training 700:5.332271656537041 validation 700:2.602498935199414\n",
      "training 800:5.309061009629029 validation 800:2.5851733054300787\n",
      "training 900:5.2860831585622705 validation 900:2.5743714834949225\n",
      "training 1000:5.2692541146043235 validation 1000:2.5567860664092112\n",
      "training 1100:5.260180369816297 validation 1100:2.5503770727133546\n",
      "training 1200:5.252775359161534 validation 1200:2.544056253745625\n",
      "training 1300:5.249035229656947 validation 1300:2.5411684625363553\n",
      "training 1400:5.240635499515562 validation 1400:2.533253714247781\n",
      "training 1500:5.235315458948611 validation 1500:2.524052729195525\n",
      "training 1600:5.23377755156889 validation 1600:2.5212823774044133\n",
      "training 1700:5.22836593331278 validation 1700:2.519035807858611\n",
      "training 1800:5.223812454568938 validation 1800:2.518860210227931\n",
      "training 1900:5.221678232798243 validation 1900:2.516119300438882\n",
      "training 2000:5.220153254455275 validation 2000:2.5184035990877414\n",
      "training 2100:5.214608745982102 validation 2100:2.5159083375424833\n",
      "training 2200:5.21423550927782 validation 2200:2.5125657311665326\n",
      "training 2300:5.208192142459926 validation 2300:2.5130234030354397\n",
      "training 2400:5.207223873340889 validation 2400:2.5116149154216934\n",
      "training 2500:5.206544278718486 validation 2500:2.5132698072132973\n",
      "training 2600:5.206136479363092 validation 2600:2.510734209418986\n",
      "training 2700:5.205014866393487 validation 2700:2.512037544627717\n",
      "training 2800:5.2042159946428015 validation 2800:2.5100294221242168\n",
      "training 2900:5.205439748224846 validation 2900:2.5102408790899915\n",
      "training 3000:5.204708929998047 validation 3000:2.511826238913429\n",
      "early stopping at iter 2768\n",
      "Use learning rate: 9\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.050170638571994 validation 100:3.1289518536979894\n",
      "training 200:5.692646825856571 validation 200:2.884942729171856\n",
      "training 300:5.534553366125504 validation 300:2.766413412807112\n",
      "training 400:5.448952450017138 validation 400:2.7011111445899756\n",
      "training 500:5.399208350663197 validation 500:2.657461553969883\n",
      "training 600:5.359311730271176 validation 600:2.6243996381500034\n",
      "training 700:5.331790514639789 validation 700:2.6025329270222426\n",
      "training 800:5.308577764039795 validation 800:2.585002200483375\n",
      "training 900:5.286350916801656 validation 900:2.5753334800791174\n",
      "training 1000:5.270429197013847 validation 1000:2.55668226493275\n",
      "training 1100:5.261172521951022 validation 1100:2.5497873299879386\n",
      "training 1200:5.253651040647848 validation 1200:2.54436919076421\n",
      "training 1300:5.248967815494419 validation 1300:2.541934220716321\n",
      "training 1400:5.241327552074829 validation 1400:2.533742563884986\n",
      "training 1500:5.235366151499715 validation 1500:2.5246134429043625\n",
      "training 1600:5.2337944541056745 validation 1600:2.5192816239793494\n",
      "training 1700:5.229347204655331 validation 1700:2.5190006893117376\n",
      "training 1800:5.223947931275404 validation 1800:2.519386966403268\n",
      "training 1900:5.222694637649385 validation 1900:2.5162247752543254\n",
      "training 2000:5.220170201106558 validation 2000:2.518438725960782\n",
      "training 2100:5.214218543338288 validation 2100:2.5159083375424833\n",
      "training 2200:5.214082813834647 validation 2200:2.5145366472973816\n",
      "training 2300:5.208175156833768 validation 2300:2.5130234030354397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 2400:5.207155917869925 validation 2400:2.5116149154216934\n",
      "training 2500:5.207053983000611 validation 2500:2.513234608095529\n",
      "training 2600:5.20557570308666 validation 2600:2.5112978968415556\n",
      "training 2700:5.205405758954569 validation 2700:2.5112626700845024\n",
      "training 2800:5.2043349833136325 validation 2800:2.5100294221242168\n",
      "training 2900:5.205184823287833 validation 2900:2.5106285039376393\n",
      "training 3000:5.204063005211223 validation 3000:2.511967111364616\n",
      "early stopping at iter 2781\n",
      "Use learning rate: 10\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.0509455432556285 validation 100:3.1300825617857413\n",
      "training 200:5.693423777480364 validation 200:2.8869964948888236\n",
      "training 300:5.535224653107407 validation 300:2.770024565490035\n",
      "training 400:5.449423247432082 validation 400:2.700554319130287\n",
      "training 500:5.4005681072553875 validation 500:2.6572618124119347\n",
      "training 600:5.360318541093967 validation 600:2.6243996381500034\n",
      "training 700:5.3327693438526484 validation 700:2.6025329270222426\n",
      "training 800:5.30922763584045 validation 800:2.5857207651721255\n",
      "training 900:5.28737162069617 validation 900:2.576295117450831\n",
      "training 1000:5.271184468805424 validation 1000:2.5575125587810352\n",
      "training 1100:5.260937112952425 validation 1100:2.549648547169494\n",
      "training 1200:5.2554692975180615 validation 1200:2.544299652530803\n",
      "training 1300:5.248529602331822 validation 1300:2.543082425647594\n",
      "training 1400:5.241968885059512 validation 1400:2.5348944793090347\n",
      "training 1500:5.235112683835541 validation 1500:2.5271701205335346\n",
      "training 1600:5.234673310810614 validation 1600:2.5199838236261716\n",
      "training 1700:5.229364121512041 validation 1700:2.51889533073337\n",
      "training 1800:5.223981799903052 validation 1800:2.519386966403268\n",
      "training 1900:5.222914832629673 validation 1900:2.516541173176306\n",
      "training 2000:5.220305772336319 validation 2000:2.5185089782370684\n",
      "training 2100:5.214727497513095 validation 2100:2.5159786604734027\n",
      "training 2200:5.213947080797696 validation 2200:2.5143959188079443\n",
      "training 2300:5.208378980691686 validation 2300:2.5125305221850613\n",
      "training 2400:5.208192142459926 validation 2400:2.5122840454983177\n",
      "training 2500:5.207189895716263 validation 2500:2.5130938066943656\n",
      "training 2600:5.205728648062062 validation 2600:2.511192215087923\n",
      "training 2700:5.20547373727319 validation 2700:2.5112626700845024\n",
      "training 2800:5.204029006948972 validation 2800:2.5099941775646317\n",
      "training 2900:5.2055247204295885 validation 2900:2.510875143138402\n",
      "training 3000:5.204453969264043 validation 3000:2.511967111364616\n",
      "early stopping at iter 2792\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = np.arange(1,11,1)\n",
    "loss_history = []\n",
    "validation_loss_history = []\n",
    "best_validation = []\n",
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "iter_time = 100000\n",
    "early_stopping_iter = 250\n",
    "eps = 0.0000000001\n",
    "for lr in learning_rate_list:\n",
    "    print(\"Use learning rate:\", lr)\n",
    "    history = []\n",
    "    validation_history = []\n",
    "    w = np.zeros([dim, 1])\n",
    "    learning_rate = lr\n",
    "    adagrad = np.zeros([dim, 1])\n",
    "    temp_loss = np.inf\n",
    "    temp_iter = 0\n",
    "    for t in range(iter_time):\n",
    "        loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "#         loss = np.sqrt(np.sum(np.power(np.round(np.dot(train_x, w)) - y_train_set, 2))/471/12)#rmse\n",
    "        history.append(loss)\n",
    "        validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "#         validation_loss = np.sqrt(np.sum(np.power(np.round(np.dot(validation_x, w)) - y_validation, 2))/471/12)#rmse\n",
    "        validation_history.append(validation_loss)\n",
    "        if validation_loss < temp_loss:\n",
    "            temp_w = np.copy(w)\n",
    "            temp_loss = np.copy(validation_loss)\n",
    "            temp_iter = 0\n",
    "        else:\n",
    "            if temp_iter < early_stopping_iter:\n",
    "                temp_iter += 1\n",
    "            else:\n",
    "                print(\"early stopping at iter\", t-temp_iter)\n",
    "                best_validation.append((lr,temp_loss))\n",
    "                break\n",
    "        if(t%100==0):\n",
    "            print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "        gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "        adagrad += gradient ** 2\n",
    "        w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "    loss_history.append(np.array(history))\n",
    "    validation_loss_history.append(np.array(validation_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, array(2.51400887)),\n",
       " (2, array(2.5110513)),\n",
       " (3, array(2.50999418)),\n",
       " (4, array(2.50999418)),\n",
       " (5, array(2.51076944)),\n",
       " (6, array(2.50999418)),\n",
       " (7, array(2.50999418)),\n",
       " (8, array(2.50999418)),\n",
       " (9, array(2.50999418)),\n",
       " (10, array(2.50999418))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4, array(2.50726051)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.01 to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate_list = np.arange(0.01,0.11,0.01)\n",
    "loss_history = []\n",
    "validation_loss_history = []\n",
    "best_validation = []\n",
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "iter_time = 100000\n",
    "early_stopping_iter = 250\n",
    "eps = 0.0000000001\n",
    "for lr in learning_rate_list:\n",
    "    print(\"Use learning rate:\", lr)\n",
    "    history = []\n",
    "    validation_history = []\n",
    "    w = np.zeros([dim, 1])\n",
    "    learning_rate = lr\n",
    "    adagrad = np.zeros([dim, 1])\n",
    "    temp_loss = np.inf\n",
    "    temp_iter = 0\n",
    "    for t in range(iter_time):\n",
    "        loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "        history.append(loss)\n",
    "        validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "        validation_history.append(validation_loss)\n",
    "        if validation_loss < temp_loss:\n",
    "            temp_w = np.copy(w)\n",
    "            temp_loss = np.copy(validation_loss)\n",
    "            temp_iter = 0\n",
    "        else:\n",
    "            if temp_iter < early_stopping_iter:\n",
    "                temp_iter += 1\n",
    "            else:\n",
    "                print(\"early stopping at iter\", t)\n",
    "                best_validation.append((lr,temp_loss))\n",
    "                history = history[:-temp_iter-1]\n",
    "                validation_history = validation_history[:-temp_iter-1]\n",
    "                break\n",
    "        if(t%100==0):\n",
    "            print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "        gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "        adagrad += gradient ** 2\n",
    "        w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "    loss_history.append(np.array(history))\n",
    "    validation_loss_history.append(np.array(validation_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate_list = np.arange(1,5.5,0.5)\n",
    "loss_history = []\n",
    "validation_loss_history = []\n",
    "best_validation = []\n",
    "\n",
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "iter_time = 100000\n",
    "early_stopping_iter = 250\n",
    "eps = 0.0000000001\n",
    "\n",
    "for lr in learning_rate_list:\n",
    "    print(\"Use learning rate:\", lr)\n",
    "    history = []\n",
    "    validation_history = []\n",
    "    w = np.zeros([dim, 1])\n",
    "    learning_rate = lr\n",
    "    adagrad = np.zeros([dim, 1])\n",
    "    temp_loss = np.inf\n",
    "    temp_iter = 0\n",
    "    for t in range(iter_time):\n",
    "        loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "        history.append(loss)\n",
    "        validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "        validation_history.append(validation_loss)\n",
    "        if validation_loss < temp_loss:\n",
    "            temp_w = np.copy(w)\n",
    "            temp_loss = np.copy(validation_loss)\n",
    "            temp_iter = 0\n",
    "        else:\n",
    "            if temp_iter < early_stopping_iter:\n",
    "                temp_iter += 1\n",
    "            else:\n",
    "                print(\"early stopping at iter\", t)\n",
    "                history = history[:-temp_iter-1]\n",
    "                validation_history = validation_history[:-temp_iter-1]\n",
    "                best_validation.append((lr,temp_loss))\n",
    "                break\n",
    "        if(t%100==0):\n",
    "            print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "        gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "        adagrad += gradient ** 2\n",
    "        w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "    loss_history.append(np.array(history))\n",
    "    validation_loss_history.append(np.array(validation_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 to 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use learning rate: 4.1\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.0110581880270555 validation 100:3.1053001701230247\n",
      "training 200:5.666494795011621 validation 200:2.8688986108915113\n",
      "training 300:5.519861636039562 validation 300:2.7564347149053936\n",
      "training 400:5.437230013093678 validation 400:2.6890954330036148\n",
      "training 500:5.383666536813436 validation 500:2.6444537849263434\n",
      "training 600:5.345940183768055 validation 600:2.613108023954564\n",
      "training 700:5.317899167965888 validation 700:2.590235185803498\n",
      "training 800:5.296284536571396 validation 800:2.5730705138960888\n",
      "training 900:5.279198698512755 validation 900:2.559912996258312\n",
      "training 1000:5.265451076343708 validation 1000:2.5496602867013136\n",
      "training 1100:5.254247760836925 validation 1100:2.5415678790452247\n",
      "training 1200:5.245032407373539 validation 1200:2.535115589067987\n",
      "training 1300:5.237399289772711 validation 1300:2.529929565276545\n",
      "training 1400:5.23104304849983 validation 1400:2.5257347131926284\n",
      "training 1500:5.225728129956756 validation 1500:2.522324559584189\n",
      "training 1600:5.221269313421804 validation 1600:2.519541522063741\n",
      "training 1700:5.217518766960729 validation 1700:2.517263610699479\n",
      "training 1800:5.214357113738021 validation 1800:2.5153952341036896\n",
      "training 1900:5.211687061352715 validation 1900:2.513860700349062\n",
      "training 2000:5.209428730339826 validation 2000:2.5125995324478922\n",
      "training 2100:5.2075161471726314 validation 2100:2.5115630331975165\n",
      "training 2200:5.205894559250178 validation 2200:2.510711727315426\n",
      "training 2300:5.2045183452918495 validation 2300:2.5100134304282697\n",
      "training 2400:5.203349366785273 validation 2400:2.5094417730171776\n",
      "training 2500:5.202355652511074 validation 2500:2.508975059265681\n",
      "training 2600:5.201510338801604 validation 2600:2.5085953756562187\n",
      "training 2700:5.200790808958539 validation 2700:2.5082878880638897\n",
      "training 2800:5.200177989670884 validation 2800:2.508040282719434\n",
      "training 2900:5.199655772499878 validation 2900:2.507842318133644\n",
      "training 3000:5.199210535891111 validation 3000:2.507685463442398\n",
      "training 3100:5.198830748615246 validation 3100:2.507562604673881\n",
      "training 3200:5.198506639606535 validation 3200:2.507467804850871\n",
      "training 3300:5.198229922253032 validation 3300:2.507396107095098\n",
      "training 3400:5.197993563562085 validation 3400:2.507343372325235\n",
      "training 3500:5.197791590466184 validation 3500:2.5073061449643412\n",
      "training 3600:5.197618926980224 validation 3600:2.5072815414580365\n",
      "training 3700:5.197471257067509 validation 3700:2.5072671574667313\n",
      "training 3800:5.197344908987772 validation 3800:2.5072609904165373\n",
      "training 3900:5.197236757638222 validation 3900:2.5072613747341115\n",
      "training 4000:5.197144141996378 validation 4000:2.5072669275944492\n",
      "early stopping at iter 4093\n",
      "Use learning rate: 4.109999999999999\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011179761608099 validation 100:3.1053887423107276\n",
      "training 200:5.666571533304328 validation 200:2.868954842372923\n",
      "training 300:5.519919796541478 validation 300:2.756480255150688\n",
      "training 400:5.437277703685845 validation 400:2.6891340213147807\n",
      "training 500:5.383707264944348 validation 500:2.6444868760473295\n",
      "training 600:5.345975803135534 validation 600:2.613136567569684\n",
      "training 700:5.3179307698566705 validation 700:2.590259916674181\n",
      "training 800:5.2963128090915506 validation 800:2.573092017445534\n",
      "training 900:5.2792241047642365 validation 900:2.5599317430930695\n",
      "training 1000:5.265473951039842 validation 1000:2.5496766595353137\n",
      "training 1100:5.254268364058358 validation 1100:2.5415821936010126\n",
      "training 1200:5.245050954487069 validation 1200:2.53512810958306\n",
      "training 1300:5.237415967915072 validation 1300:2.529940515756821\n",
      "training 1400:5.231058025528955 validation 1400:2.5257442855803647\n",
      "training 1500:5.225741559243537 validation 1500:2.5223329197715527\n",
      "training 1600:5.221281336397455 validation 1600:2.5195488144150353\n",
      "training 1700:5.2175295144922815 validation 1700:2.5172699615300855\n",
      "training 1800:5.214366706921282 validation 1800:2.5154007544136507\n",
      "training 1900:5.211695612038277 validation 1900:2.5138654880108904\n",
      "training 2000:5.209436341554952 validation 2000:2.512603673985304\n",
      "training 2100:5.207522913517994 validation 2100:2.5115666052514416\n",
      "training 2200:5.205900567305499 validation 2200:2.510714797867141\n",
      "training 2300:5.204523674041115 validation 2300:2.5100160598314685\n",
      "training 2400:5.203354088060773 validation 2400:2.5094440148780426\n",
      "training 2500:5.202359831450375 validation 2500:2.5089769612007613\n",
      "training 2600:5.201514034309999 validation 2600:2.508596979950969\n",
      "training 2700:5.2007940741694885 validation 2700:2.508289232249512\n",
      "training 2800:5.200180872397878 validation 2800:2.5080414000816855\n",
      "training 2900:5.199658315674277 validation 2900:2.5078432381642544\n",
      "training 3000:5.199212777981786 validation 3000:2.507686212241453\n",
      "training 3100:5.198832724026961 validation 3100:2.507563205309789\n",
      "training 3200:5.198508379055047 validation 3200:2.507468277683243\n",
      "training 3300:5.1982314531157225 validation 3300:2.507396470064859\n",
      "training 3400:5.1979949102036604 validation 3400:2.507343641215578\n",
      "training 3500:5.197792774539522 validation 3500:2.507306333635547\n",
      "training 3600:5.197619967702903 validation 3600:2.5072816620587686\n",
      "training 3700:5.1974721714748355 validation 3700:2.5072672206240765\n",
      "training 3800:5.197345712163422 validation 3800:2.507261005406804\n",
      "training 3900:5.197237462923697 validation 3900:2.5072613496361864\n",
      "training 4000:5.197144761180622 validation 4000:2.5072668694273346\n",
      "early stopping at iter 4094\n",
      "Use learning rate: 4.119999999999999\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011300972143575 validation 100:3.10547693651554\n",
      "training 200:5.6666480546478795 validation 200:2.869010919467935\n",
      "training 300:5.519977784121675 validation 300:2.7565256701981475\n",
      "training 400:5.437325243985932 validation 400:2.689172500310807\n",
      "training 500:5.383747858565297 validation 500:2.644519871028194\n",
      "training 600:5.346011300652883 validation 600:2.613165026599122\n",
      "training 700:5.317962260825738 validation 700:2.590284573109008\n",
      "training 800:5.296340980532776 validation 800:2.573113455489869\n",
      "training 900:5.279249419006144 validation 900:2.5599504323103095\n",
      "training 1000:5.265496742166781 validation 1000:2.5496929817436618\n",
      "training 1100:5.254288891583563 validation 1100:2.541596463745068\n",
      "training 1200:5.2450694332342085 validation 1200:2.5351405912166847\n",
      "training 1300:5.237432584488669 validation 1300:2.529951432279632\n",
      "training 1400:5.2310729472638995 validation 1400:2.5257538283935395\n",
      "training 1500:5.22575493900004 validation 1500:2.52234125428251\n",
      "training 1600:5.221293315113045 validation 1600:2.519556084552837\n",
      "training 1700:5.2175402225617304 validation 1700:2.517276293218888\n",
      "training 1800:5.2143762649930805 validation 1800:2.5154062583017227\n",
      "training 1900:5.211704131543101 validation 1900:2.51387026165451\n",
      "training 2000:5.209443925128953 validation 2000:2.5126078036240833\n",
      "training 2100:5.207529655400071 validation 2100:2.5115701672713233\n",
      "training 2200:5.205906553742761 validation 2200:2.5107178600208404\n",
      "training 2300:5.2045289837133755 validation 2300:2.510018682268116\n",
      "training 2400:5.203358792523456 validation 2400:2.5094462510210604\n",
      "training 2500:5.202363995590109 validation 2500:2.5089788585037303\n",
      "training 2600:5.201517716805477 validation 2600:2.508598580554328\n",
      "training 2700:5.20079732795025 validation 2700:2.508290573555711\n",
      "training 2800:5.200183745094453 validation 2800:2.508042515262435\n",
      "training 2900:5.199660850054334 validation 2900:2.507844156610512\n",
      "training 3000:5.199215012368102 validation 3000:2.507686959964488\n",
      "training 3100:5.1988346926942235 validation 3100:2.5075638052999394\n",
      "training 3200:5.198510112603433 validation 3200:2.507468750231679\n",
      "training 3300:5.198232978820118 validation 3300:2.507396833052683\n",
      "training 3400:5.197996252338044 validation 3400:2.507343910373848\n",
      "training 3500:5.197793954676591 validation 3500:2.5073065227792504\n",
      "training 3600:5.197621004989479 validation 3600:2.50728178329735\n",
      "training 3700:5.197473082883885 validation 3700:2.507267284550786\n",
      "training 3800:5.197346512723756 validation 3800:2.507261021268867\n",
      "training 3900:5.197238165928595 validation 3900:2.5072613254875953\n",
      "training 4000:5.197145378376693 validation 4000:2.507266812265877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at iter 4095\n",
      "Use learning rate: 4.129999999999999\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011421820441479 validation 100:3.1055647565976785\n",
      "training 200:5.6667243594528545 validation 200:2.8690668424110006\n",
      "training 300:5.520035599223531 validation 300:2.7565709602309134\n",
      "training 400:5.437372634485638 validation 400:2.689210870198837\n",
      "training 500:5.38378831818823 validation 500:2.644552770086135\n",
      "training 600:5.346046676830572 validation 600:2.6131934012579805\n",
      "training 700:5.317993641368006 validation 700:2.5903091553138604\n",
      "training 800:5.2963690513656045 validation 800:2.5731348282216957\n",
      "training 900:5.279274641679367 validation 900:2.5599690640871233\n",
      "training 1000:5.265519450133002 validation 1000:2.5497092534869\n",
      "training 1100:5.254309343787567 validation 1100:2.5416106896211583\n",
      "training 1200:5.24508784395673 validation 1200:2.5351530340961745\n",
      "training 1300:5.237449139803038 validation 1300:2.5299623149565336\n",
      "training 1400:5.231087813983532 validation 1400:2.5257633417288825\n",
      "training 1500:5.225768269476364 validation 1500:2.522349563200039\n",
      "training 1600:5.221305249791992 validation 1600:2.51956333254751\n",
      "training 1700:5.217550891367958 validation 1700:2.5172826058248035\n",
      "training 1800:5.214385788129919 validation 1800:2.51541174581652\n",
      "training 1900:5.211712620023391 validation 1900:2.513875021319338\n",
      "training 2000:5.209451481199722 validation 2000:2.512611921395498\n",
      "training 2100:5.207536372940318 validation 2100:2.5115737192812606\n",
      "training 2200:5.2059125186687165 validation 2200:2.5107209137943625\n",
      "training 2300:5.204534274402282 validation 2300:2.510021297750624\n",
      "training 2400:5.203363480255339 validation 2400:2.5094484814539744\n",
      "training 2500:5.202368145001983 validation 2500:2.5089807511783477\n",
      "training 2600:5.201521386350644 validation 2600:2.508600177466689\n",
      "training 2700:5.200800570355406 validation 2700:2.5082919119800597\n",
      "training 2800:5.200186607808139 validation 2800:2.5080436282569227\n",
      "training 2900:5.199663375681383 validation 2900:2.507845073465754\n",
      "training 3000:5.1992172390859706 validation 3000:2.5076877066033108\n",
      "training 3100:5.198836654648199 validation 3100:2.5075644046349352\n",
      "training 3200:5.198511840278715 validation 3200:2.5074692224858635\n",
      "training 3300:5.19823449938963 validation 3300:2.507397196047579\n",
      "training 3400:5.197997589985502 validation 3400:2.507344179788587\n",
      "training 3500:5.19779513089492 validation 3500:2.507306712383703\n",
      "training 3600:5.1976220388551075 validation 3600:2.5072819051618938\n",
      "training 3700:5.197473991307748 validation 3700:2.507267349234958\n",
      "training 3800:5.197347310680079 validation 3800:2.5072610379909124\n",
      "training 3900:5.1972388666626745 validation 3900:2.5072613022766963\n",
      "training 4000:5.197145993593011 validation 4000:2.5072667560986783\n",
      "early stopping at iter 4096\n",
      "Use learning rate: 4.139999999999999\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011542307323716 validation 100:3.105652206334813\n",
      "training 200:5.666800448137264 validation 200:2.8691226114426613\n",
      "training 300:5.520093242294091 validation 300:2.7566161254375405\n",
      "training 400:5.437419875677765 validation 400:2.689249131189806\n",
      "training 500:5.3838286443246774 validation 500:2.6445855734409727\n",
      "training 600:5.346081932177786 validation 600:2.6132216917631785\n",
      "training 700:5.318024911976619 validation 700:2.5903336634958802\n",
      "training 800:5.296397022058569 validation 800:2.5731561358344823\n",
      "training 900:5.279299773222725 validation 900:2.5599876386012035\n",
      "training 1000:5.265542075344935 validation 1000:2.5497254749259963\n",
      "training 1100:5.254329721043441 validation 1100:2.5416248713733562\n",
      "training 1200:5.245106186994569 validation 1200:2.535165438349076\n",
      "training 1300:5.237465634166019 validation 1300:2.529973163899268\n",
      "training 1400:5.231102625965168 validation 1400:2.525772825683288\n",
      "training 1500:5.225781550921208 validation 1500:2.522357846607271\n",
      "training 1600:5.221317140656448 validation 1600:2.51957055846958\n",
      "training 1700:5.21756152110873 validation 1700:2.517288899406911\n",
      "training 1800:5.214395276507303 validation 1800:2.5154172170068283\n",
      "training 1900:5.211721077634469 validation 1900:2.513879767044973\n",
      "training 2000:5.209459009904381 validation 2000:2.5126160273310063\n",
      "training 2100:5.207543066259511 validation 2100:2.5115772613055527\n",
      "training 2200:5.20591846218953 validation 2200:2.5107239592057526\n",
      "training 2300:5.204539546200971 validation 2300:2.5100239062916185\n",
      "training 2400:5.203368151337994 validation 2400:2.5094507061847473\n",
      "training 2500:5.202372279757325 validation 2500:2.508982639228597\n",
      "training 2600:5.201525043007774 validation 2600:2.508601770688667\n",
      "training 2700:5.200803801439254 validation 2700:2.5082932475203545\n",
      "training 2800:5.200189460586215 validation 2800:2.5080447390606095\n",
      "training 2900:5.199665892596549 validation 2900:2.5078459887235303\n",
      "training 3000:5.199219458171122 validation 3000:2.5076884521499383\n",
      "training 3100:5.198838609919898 validation 3100:2.507565003305588\n",
      "training 3200:5.198513562107785 validation 3200:2.507469694435682\n",
      "training 3300:5.198236014847558 validation 3300:2.507397559038752\n",
      "training 3400:5.197998923166203 validation 3400:2.507344449448525\n",
      "training 3500:5.197796303211957 validation 3500:2.5073069024373393\n",
      "training 3600:5.197623069314873 validation 3600:2.5072820276406858\n",
      "training 3700:5.19747489675946 validation 3700:2.507267414664855\n",
      "training 3800:5.197348106043646 validation 3800:2.5072610555612838\n",
      "training 3900:5.197239565135651 validation 3900:2.5072612799919978\n",
      "training 4000:5.197146606837959 validation 4000:2.5072667009144793\n",
      "early stopping at iter 4096\n",
      "Use learning rate: 4.149999999999999\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011662433625541 validation 100:3.105739289424246\n",
      "training 200:5.666876321126352 validation 200:2.8691782268094053\n",
      "training 300:5.520150713783925 validation 300:2.7566611660118534\n",
      "training 400:5.437466968056147 validation 400:2.689287283498333\n",
      "training 500:5.383868837485717 validation 500:2.6446182813150725\n",
      "training 600:5.346117067202393 validation 600:2.6132498983333914\n",
      "training 700:5.318056073142943 validation 700:2.590358097863415\n",
      "training 800:5.296424893078199 validation 800:2.5731773785225274\n",
      "training 900:5.279324814072966 validation 900:2.5600061560308136\n",
      "training 1000:5.265564618206963 validation 1000:2.549741646222313\n",
      "training 1100:5.254350023722302 validation 1100:2.541639009146015\n",
      "training 1200:5.245124462685826 validation 1200:2.535177804103143\n",
      "training 1300:5.237482067883757 validation 1300:2.529983979219747\n",
      "training 1400:5.231117383484583 validation 1400:2.525782280353798\n",
      "training 1500:5.225794783581874 validation 1500:2.522366104587481\n",
      "training 1600:5.221328987927324 validation 1600:2.51957776238971\n",
      "training 1700:5.217572111980692 validation 1700:2.517295174024443\n",
      "training 1800:5.214404730299748 validation 1800:2.5154226719215957\n",
      "training 1900:5.21172950453079 validation 1900:2.5138844988711853\n",
      "training 2000:5.209466511379284 validation 2000:2.5126201214622506\n",
      "training 2100:5.207549735477756 validation 2100:2.5115807933686916\n",
      "training 2200:5.205924384410776 validation 2200:2.51072699627326\n",
      "training 2300:5.204544799202075 validation 2300:2.5100265079039352\n",
      "training 2400:5.2033728058525455 validation 2400:2.509452925221555\n",
      "training 2500:5.2023763999270765 validation 2500:2.508984522658677\n",
      "training 2600:5.20152868683881 validation 2600:2.508603360221096\n",
      "training 2700:5.200807021255805 validation 2700:2.5082945801746077\n",
      "training 2800:5.2001923034757205 validation 2800:2.508045847669171\n",
      "training 2900:5.199668400840744 validation 2900:2.507846902377607\n",
      "training 3000:5.199221669659104 validation 3000:2.5076891965965977\n",
      "training 3100:5.198840558540178 validation 3100:2.50756560130291\n",
      "training 3200:5.198515278117402 validation 3200:2.5074701660712178\n",
      "training 3300:5.198237525217088 validation 3300:2.5073979220155946\n",
      "training 3400:5.198000251900224 validation 3400:2.507344719342575\n",
      "training 3500:5.197797471645072 validation 3500:2.507307092928768\n",
      "training 3600:5.197624096383793 validation 3600:2.5072821507221805\n",
      "training 3700:5.197475799251998 validation 3700:2.507267480828902\n",
      "training 3800:5.197348898825664 validation 3800:2.5072610739684764\n",
      "training 3900:5.197240261357199 validation 3900:2.5072612586221537\n",
      "training 4000:5.197147218119884 validation 4000:2.5072666467021585\n",
      "early stopping at iter 4097\n",
      "Use learning rate: 4.159999999999998\n",
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011782200194958 validation 100:3.105826009484999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 200:5.666951978852356 validation 200:2.8692336887634964\n",
      "training 300:5.520208014146998 validation 300:2.7567060821527924\n",
      "training 400:5.437513912115561 validation 400:2.6893253273426034\n",
      "training 500:5.383908898181912 validation 500:2.6446508939332487\n",
      "training 600:5.346152082410921 validation 600:2.613278021188975\n",
      "training 700:5.318087125356547 validation 700:2.5903824586259634\n",
      "training 800:5.2964526648890145 validation 800:2.5731985564809143\n",
      "training 900:5.279349764664768 validation 900:2.560024616554752\n",
      "training 1000:5.265587079121436 validation 1000:2.5497577675375784\n",
      "training 1100:5.254370252193311 validation 1100:2.541653103083746\n",
      "training 1200:5.245142671366779 validation 1200:2.535190131486322\n",
      "training 1300:5.237498441260716 validation 1300:2.5299947610300357\n",
      "training 1400:5.2311320868160145 validation 1400:2.525791705837589\n",
      "training 1500:5.225807967704276 validation 1500:2.5223743372240706\n",
      "training 1600:5.221340791824275 validation 1600:2.519584944378702\n",
      "training 1700:5.217582664179379 validation 1700:2.5173014297367726\n",
      "training 1800:5.214414149680785 validation 1800:2.5154281106099226\n",
      "training 1900:5.211737900865929 validation 1900:2.513889216837911\n",
      "training 2000:5.209473985760017 validation 2000:2.5126242038210487\n",
      "training 2100:5.207556380714487 validation 2100:2.511584315495357\n",
      "training 2200:5.205930285437444 validation 2200:2.5107300250153295\n",
      "training 2300:5.204550033497709 validation 2300:2.510029102600611\n",
      "training 2400:5.203377443879684 validation 2400:2.5094551385727817\n",
      "training 2500:5.202380505581796 validation 2500:2.5089864014729972\n",
      "training 2600:5.201532317905367 validation 2600:2.508604946065021\n",
      "training 2700:5.200810229858788 validation 2700:2.5082959099410456\n",
      "training 2800:5.200195136523443 validation 2800:2.508046954078494\n",
      "training 2900:5.199670900454674 validation 2900:2.507847814421957\n",
      "training 3000:5.199223873585286 validation 3000:2.5076899399357204\n",
      "training 3100:5.198842500539745 validation 3100:2.5075661986181172\n",
      "training 3200:5.198516988334197 validation 3200:2.5074706373827476\n",
      "training 3300:5.198239030521295 validation 3300:2.50739828496769\n",
      "training 3400:5.1980015762075436 validation 3400:2.507344989459834\n",
      "training 3500:5.19779863621155 validation 3500:2.5073072838467745\n",
      "training 3600:5.1976251200768155 validation 3600:2.507282274394999\n",
      "training 3700:5.197476698798278 validation 3700:2.5072675477156814\n",
      "training 3800:5.197349689037292 validation 3800:2.5072610932011403\n",
      "training 3900:5.197240955336952 validation 3900:2.507261238155961\n",
      "training 4000:5.1971478274471 validation 4000:2.5072665934507326\n",
      "early stopping at iter 4098\n"
     ]
    }
   ],
   "source": [
    "learning_rate_list = np.arange(4.1,4.16,0.01)\n",
    "\n",
    "loss_history = []\n",
    "validation_loss_history = []\n",
    "best_validation = []\n",
    "\n",
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "iter_time = 100000\n",
    "early_stopping_iter = 250\n",
    "eps = 0.0000000001\n",
    "\n",
    "for lr in learning_rate_list:\n",
    "    print(\"Use learning rate:\", lr)\n",
    "    history = []\n",
    "    validation_history = []\n",
    "    w = np.zeros([dim, 1])\n",
    "    learning_rate = lr\n",
    "    adagrad = np.zeros([dim, 1])\n",
    "    temp_loss = np.inf\n",
    "    temp_iter = 0\n",
    "    for t in range(iter_time):\n",
    "        loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "#         loss = np.sqrt(np.sum(np.power(np.round(np.dot(train_x, w)) - y_train_set, 2))/471/12)#rmse\n",
    "        history.append(loss)\n",
    "        validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "#         validation_loss = np.sqrt(np.sum(np.power(np.round(np.dot(validation_x, w)) - y_validation, 2))/471/12)#rmse\n",
    "        validation_history.append(validation_loss)\n",
    "        if validation_loss < temp_loss:\n",
    "            temp_w = np.copy(w)\n",
    "            temp_loss = np.copy(validation_loss)\n",
    "            temp_iter = 0\n",
    "        else:\n",
    "            if temp_iter < early_stopping_iter:\n",
    "                temp_iter += 1\n",
    "            else:\n",
    "                print(\"early stopping at iter\", t)\n",
    "                history = history[:-temp_iter-1]\n",
    "                validation_history = validation_history[:-temp_iter-1]\n",
    "                best_validation.append((lr,temp_loss))\n",
    "                break\n",
    "        if(t%100==0):\n",
    "            print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "        gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "        adagrad += gradient ** 2\n",
    "        w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "    loss_history.append(np.array(history))\n",
    "    validation_loss_history.append(np.array(validation_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.1, array(2.50726044)),\n",
       " (4.109999999999999, array(2.50726044)),\n",
       " (4.119999999999999, array(2.50726043)),\n",
       " (4.129999999999999, array(2.50726043)),\n",
       " (4.139999999999999, array(2.50726043)),\n",
       " (4.149999999999999, array(2.50726043)),\n",
       " (4.159999999999998, array(2.50726043))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation[-3][1] < best_validation[-2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.149999999999999"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation[-2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0:24.36221492236629 validation 0:11.803946645288281\n",
      "training 100:6.011662433625542 validation 100:3.105739289424246\n",
      "training 200:5.666876321126352 validation 200:2.8691782268094053\n",
      "training 300:5.520150713783925 validation 300:2.7566611660118534\n",
      "training 400:5.437466968056147 validation 400:2.689287283498333\n",
      "training 500:5.383868837485717 validation 500:2.6446182813150725\n",
      "training 600:5.346117067202393 validation 600:2.6132498983333914\n",
      "training 700:5.318056073142943 validation 700:2.590358097863415\n",
      "training 800:5.296424893078199 validation 800:2.573177378522527\n",
      "training 900:5.279324814072966 validation 900:2.5600061560308136\n",
      "training 1000:5.265564618206963 validation 1000:2.549741646222313\n",
      "training 1100:5.254350023722302 validation 1100:2.541639009146015\n",
      "training 1200:5.245124462685826 validation 1200:2.535177804103143\n",
      "training 1300:5.237482067883757 validation 1300:2.529983979219747\n",
      "training 1400:5.231117383484583 validation 1400:2.525782280353798\n",
      "training 1500:5.2257947835818745 validation 1500:2.5223661045874803\n",
      "training 1600:5.221328987927324 validation 1600:2.5195777623897104\n",
      "training 1700:5.217572111980691 validation 1700:2.517295174024443\n",
      "training 1800:5.214404730299748 validation 1800:2.5154226719215957\n",
      "training 1900:5.211729504530789 validation 1900:2.5138844988711853\n",
      "training 2000:5.209466511379283 validation 2000:2.5126201214622506\n",
      "training 2100:5.207549735477756 validation 2100:2.5115807933686916\n",
      "training 2200:5.205924384410776 validation 2200:2.5107269962732603\n",
      "training 2300:5.204544799202075 validation 2300:2.5100265079039352\n",
      "training 2400:5.203372805852546 validation 2400:2.509452925221555\n",
      "training 2500:5.2023763999270765 validation 2500:2.508984522658677\n",
      "training 2600:5.20152868683881 validation 2600:2.508603360221096\n",
      "training 2700:5.200807021255806 validation 2700:2.5082945801746077\n",
      "training 2800:5.2001923034757205 validation 2800:2.508045847669171\n",
      "training 2900:5.1996684008407446 validation 2900:2.5078469023776075\n",
      "training 3000:5.199221669659104 validation 3000:2.5076891965965977\n",
      "training 3100:5.198840558540178 validation 3100:2.50756560130291\n",
      "training 3200:5.198515278117402 validation 3200:2.5074701660712178\n",
      "training 3300:5.198237525217088 validation 3300:2.5073979220155946\n",
      "training 3400:5.198000251900224 validation 3400:2.507344719342575\n",
      "training 3500:5.197797471645071 validation 3500:2.507307092928768\n",
      "training 3600:5.197624096383794 validation 3600:2.5072821507221805\n",
      "training 3700:5.197475799251998 validation 3700:2.507267480828902\n",
      "training 3800:5.197348898825664 validation 3800:2.5072610739684764\n",
      "training 3900:5.197240261357199 validation 3900:2.5072612586221537\n",
      "training 4000:5.197147218119884 validation 4000:2.5072666467021585\n",
      "training 4100:5.197067495457183 validation 4100:2.5072760879694806\n",
      "training 4200:5.19699915553256 validation 4200:2.5072886317464227\n",
      "training 4300:5.196940546104127 validation 4300:2.507303494726324\n",
      "training 4400:5.196890257920086 validation 4400:2.5073200338899895\n",
      "training 4500:5.196847088556226 validation 4500:2.5073377237068364\n",
      "training 4600:5.196810011704184 validation 4600:2.5073561369364357\n",
      "training 4700:5.196778151075615 validation 4700:2.507374928459269\n",
      "training 4800:5.196750758218153 validation 4800:2.507393821658784\n",
      "early stopping at iter 3847\n"
     ]
    }
   ],
   "source": [
    "dim = int(np.sum(items_bool) * 9 + 1)\n",
    "w = np.zeros([dim, 1])\n",
    "\n",
    "train_x = np.concatenate((np.ones([x_train_set.shape[0], 1]), x_train_set), axis = 1).astype(float)\n",
    "validation_x = np.concatenate((np.ones([x_validation.shape[0], 1]), x_validation), axis = 1).astype(float)\n",
    "\n",
    "history = []\n",
    "validation_history = []\n",
    "\n",
    "learning_rate = 4.15\n",
    "iter_time = 200000\n",
    "early_stopping_iter = 1000\n",
    "adagrad = np.zeros([dim, 1])\n",
    "eps = 0.0000000001\n",
    "\n",
    "temp_loss = np.inf\n",
    "temp_iter = 0\n",
    "\n",
    "for t in range(iter_time):\n",
    "    loss = np.sqrt(np.sum(np.power(np.dot(train_x, w) - y_train_set, 2))/471/12)#rmse\n",
    "    history.append(loss)\n",
    "    validation_loss = np.sqrt(np.sum(np.power(np.dot(validation_x, w) - y_validation, 2))/471/12)#rmse\n",
    "    validation_history.append(validation_loss)\n",
    "    if validation_loss < temp_loss:\n",
    "        temp_w = np.copy(w)\n",
    "        temp_loss = np.copy(validation_loss)\n",
    "        temp_iter = 0\n",
    "    else:\n",
    "        if temp_iter < early_stopping_iter:\n",
    "            temp_iter += 1\n",
    "        else:\n",
    "            print(\"early stopping at iter\", t - temp_iter)\n",
    "            history = history[:-temp_iter-1]\n",
    "            validation_history = validation_history[:-temp_iter-1]\n",
    "            break\n",
    "    if(t%100==0):\n",
    "        print(\"training \" + str(t) + \":\" + str(loss), \"validation \" + str(t) + \":\" + str(validation_loss))\n",
    "    gradient = 2 * np.dot(train_x.transpose(), np.dot(train_x, w) - y_train_set) #dim*1\n",
    "    adagrad += gradient ** 2\n",
    "    w = w - learning_rate * gradient / np.sqrt(adagrad + eps)\n",
    "np.save('./../model/weight6.npy', w)\n",
    "# w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.89839091],\n",
       "       [ 17.80732458],\n",
       "       [ 23.35903227],\n",
       "       [  6.65259305],\n",
       "       [ 26.60200308],\n",
       "       [ 21.91880916],\n",
       "       [ 24.31587656],\n",
       "       [ 29.20919715],\n",
       "       [ 17.35989067],\n",
       "       [ 58.55763806],\n",
       "       [ 13.07625969],\n",
       "       [ 10.99661671],\n",
       "       [ 62.45782123],\n",
       "       [ 52.16025926],\n",
       "       [ 22.02575563],\n",
       "       [ 11.48025265],\n",
       "       [ 32.79680815],\n",
       "       [ 67.67635096],\n",
       "       [  1.86215716],\n",
       "       [ 17.05167628],\n",
       "       [ 41.74642672],\n",
       "       [ 71.79602678],\n",
       "       [  9.56587921],\n",
       "       [ 18.6696507 ],\n",
       "       [ 13.35989984],\n",
       "       [ 37.30228019],\n",
       "       [ 13.00475953],\n",
       "       [ 74.41071602],\n",
       "       [  7.44635216],\n",
       "       [ 56.0640257 ],\n",
       "       [ 23.84192887],\n",
       "       [  8.1996752 ],\n",
       "       [  3.10891629],\n",
       "       [ 19.01255615],\n",
       "       [ 28.61154273],\n",
       "       [ 37.64023407],\n",
       "       [ 43.05208949],\n",
       "       [ 30.83768433],\n",
       "       [ 42.18475609],\n",
       "       [ 34.92112649],\n",
       "       [  8.19725729],\n",
       "       [ 39.35014673],\n",
       "       [ 32.75518478],\n",
       "       [ 51.12441564],\n",
       "       [ 15.95608471],\n",
       "       [ 36.66516949],\n",
       "       [ 23.70653893],\n",
       "       [  9.88866335],\n",
       "       [ 25.17394854],\n",
       "       [ 32.53255204],\n",
       "       [ 19.32640732],\n",
       "       [  8.8704431 ],\n",
       "       [ 24.11028322],\n",
       "       [ 53.31257658],\n",
       "       [ 15.34690894],\n",
       "       [ 35.5911292 ],\n",
       "       [ 32.52102147],\n",
       "       [ 21.47361635],\n",
       "       [ 57.89738225],\n",
       "       [ 23.58500846],\n",
       "       [ 13.59625159],\n",
       "       [ 42.50797959],\n",
       "       [ 13.05177228],\n",
       "       [ 50.18920187],\n",
       "       [ 15.27587221],\n",
       "       [ 16.40365341],\n",
       "       [ 16.79699423],\n",
       "       [  0.69459071],\n",
       "       [ 42.66499516],\n",
       "       [ 30.43336468],\n",
       "       [ 21.76159936],\n",
       "       [ 40.54056961],\n",
       "       [ 60.89459976],\n",
       "       [  4.95320776],\n",
       "       [ 16.78352049],\n",
       "       [  3.5034682 ],\n",
       "       [ 38.71524571],\n",
       "       [ 13.84639096],\n",
       "       [ 23.28415813],\n",
       "       [ 21.04613148],\n",
       "       [ 23.64761712],\n",
       "       [ 36.75533689],\n",
       "       [ 20.93924729],\n",
       "       [ 92.39192562],\n",
       "       [ 36.73984693],\n",
       "       [ 27.50838677],\n",
       "       [ 20.98906192],\n",
       "       [ 32.69245596],\n",
       "       [ 22.94582671],\n",
       "       [ 20.14286177],\n",
       "       [ 28.90485161],\n",
       "       [ 41.22926007],\n",
       "       [  4.78276069],\n",
       "       [ 38.87881067],\n",
       "       [ 46.21687458],\n",
       "       [ 17.03577356],\n",
       "       [ 32.07351483],\n",
       "       [ 13.23840036],\n",
       "       [ 23.32063253],\n",
       "       [  4.33194807],\n",
       "       [ 18.70354425],\n",
       "       [ 27.52020165],\n",
       "       [ 16.61869348],\n",
       "       [ 16.62487282],\n",
       "       [ 23.23676882],\n",
       "       [ 38.46565714],\n",
       "       [ 31.25685196],\n",
       "       [  7.1536861 ],\n",
       "       [  6.53593143],\n",
       "       [ 78.59934795],\n",
       "       [ 46.10073949],\n",
       "       [ 16.31422348],\n",
       "       [ 27.92579388],\n",
       "       [ 15.78470649],\n",
       "       [ 13.07356893],\n",
       "       [ 25.39690638],\n",
       "       [ 26.96876877],\n",
       "       [  8.90350775],\n",
       "       [ 17.11312375],\n",
       "       [ 19.06675937],\n",
       "       [ 81.59898507],\n",
       "       [ 23.79992828],\n",
       "       [ 34.26964011],\n",
       "       [ 24.81923297],\n",
       "       [  7.07415518],\n",
       "       [ 39.83689487],\n",
       "       [  9.25716163],\n",
       "       [ 21.87586628],\n",
       "       [ 30.06732858],\n",
       "       [ 61.94634839],\n",
       "       [ 21.73460468],\n",
       "       [ 24.22299515],\n",
       "       [ 58.34167714],\n",
       "       [ 15.08496194],\n",
       "       [ 12.79336233],\n",
       "       [  2.1434045 ],\n",
       "       [ 12.85764133],\n",
       "       [ 58.22665535],\n",
       "       [ 19.6146915 ],\n",
       "       [  5.7279305 ],\n",
       "       [ 27.80976693],\n",
       "       [ 25.25476005],\n",
       "       [ 43.88625056],\n",
       "       [ 30.1401341 ],\n",
       "       [ 17.63289939],\n",
       "       [ 26.15938882],\n",
       "       [ 11.12441311],\n",
       "       [ 51.14255903],\n",
       "       [ 23.03629985],\n",
       "       [ 39.36516104],\n",
       "       [ 11.05718278],\n",
       "       [  7.63379716],\n",
       "       [ 25.16772555],\n",
       "       [  6.39242099],\n",
       "       [ 15.45784306],\n",
       "       [ 40.62272076],\n",
       "       [  8.29343489],\n",
       "       [ 37.43969472],\n",
       "       [ 11.3243093 ],\n",
       "       [ 19.00890563],\n",
       "       [ 42.09548057],\n",
       "       [ 18.65579247],\n",
       "       [ 14.19461075],\n",
       "       [  7.60828453],\n",
       "       [ 52.29621133],\n",
       "       [ 29.74837983],\n",
       "       [  2.9875652 ],\n",
       "       [ 16.15199203],\n",
       "       [ 64.41376111],\n",
       "       [ 12.69364624],\n",
       "       [ 63.64544635],\n",
       "       [ 38.86591296],\n",
       "       [ 26.59377423],\n",
       "       [ 19.21948655],\n",
       "       [ 62.71406167],\n",
       "       [ 24.80751639],\n",
       "       [ 19.90370173],\n",
       "       [ 38.78186397],\n",
       "       [ 12.07229356],\n",
       "       [ 30.71757991],\n",
       "       [ 17.5764679 ],\n",
       "       [ 12.45876334],\n",
       "       [ 54.49005482],\n",
       "       [ 46.43550855],\n",
       "       [ 16.0881717 ],\n",
       "       [ 34.70823118],\n",
       "       [ 26.59849135],\n",
       "       [ 71.81809526],\n",
       "       [  9.91692579],\n",
       "       [ 58.77662753],\n",
       "       [ 38.41833275],\n",
       "       [ 14.16652883],\n",
       "       [ 28.84794579],\n",
       "       [  1.61155983],\n",
       "       [ 18.77814245],\n",
       "       [  1.17722376],\n",
       "       [ 33.95310704],\n",
       "       [ 11.54261069],\n",
       "       [ 18.87529199],\n",
       "       [ 61.95091287],\n",
       "       [ 24.55082563],\n",
       "       [ 25.96503477],\n",
       "       [ 65.7371036 ],\n",
       "       [ 11.88692972],\n",
       "       [  8.58446987],\n",
       "       [ 11.15337315],\n",
       "       [  7.4575637 ],\n",
       "       [  1.46822177],\n",
       "       [123.75012648],\n",
       "       [ 20.56133422],\n",
       "       [ 14.96342373],\n",
       "       [ 14.27541606],\n",
       "       [ 36.6724831 ],\n",
       "       [ 35.92219691],\n",
       "       [ 20.64882815],\n",
       "       [ 34.73565503],\n",
       "       [ 78.91541787],\n",
       "       [  0.85122155],\n",
       "       [ 12.64227351],\n",
       "       [ 32.53244491],\n",
       "       [ 15.55312103],\n",
       "       [ 12.3142755 ],\n",
       "       [115.87930984],\n",
       "       [ 12.40152839],\n",
       "       [ 16.83422437],\n",
       "       [ 64.65999974],\n",
       "       [ 16.07533885],\n",
       "       [ 17.89736923],\n",
       "       [ 10.15816713],\n",
       "       [  7.09883046],\n",
       "       [ 44.37290244],\n",
       "       [ 12.71433341],\n",
       "       [ 53.8776802 ],\n",
       "       [ 43.1321546 ],\n",
       "       [ 25.56041037],\n",
       "       [ 42.67901925],\n",
       "       [ 69.05492154],\n",
       "       [ 43.528103  ],\n",
       "       [ 10.98488686],\n",
       "       [ 17.25512076]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.load('./../model/weight6.npy')\n",
    "ans_y = np.dot(test_x, w)\n",
    "ans_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'value']\n",
      "['id_0', 6.898390909664564]\n",
      "['id_1', 17.807324577918884]\n",
      "['id_2', 23.359032270639098]\n",
      "['id_3', 6.652593048071527]\n",
      "['id_4', 26.60200307533507]\n",
      "['id_5', 21.918809162044084]\n",
      "['id_6', 24.31587655807283]\n",
      "['id_7', 29.209197150869233]\n",
      "['id_8', 17.35989067012718]\n",
      "['id_9', 58.55763806261701]\n",
      "['id_10', 13.07625968525129]\n",
      "['id_11', 10.996616707930018]\n",
      "['id_12', 62.457821231931874]\n",
      "['id_13', 52.16025925885989]\n",
      "['id_14', 22.02575562983203]\n",
      "['id_15', 11.480252649635478]\n",
      "['id_16', 32.796808147018126]\n",
      "['id_17', 67.67635095613338]\n",
      "['id_18', 1.8621571629467013]\n",
      "['id_19', 17.051676280483115]\n",
      "['id_20', 41.74642672050495]\n",
      "['id_21', 71.79602678094233]\n",
      "['id_22', 9.565879214830083]\n",
      "['id_23', 18.66965069688074]\n",
      "['id_24', 13.359899836300201]\n",
      "['id_25', 37.302280187210705]\n",
      "['id_26', 13.004759530315495]\n",
      "['id_27', 74.41071601841767]\n",
      "['id_28', 7.446352163369912]\n",
      "['id_29', 56.06402570244599]\n",
      "['id_30', 23.841928873021494]\n",
      "['id_31', 8.19967520325889]\n",
      "['id_32', 3.108916287551363]\n",
      "['id_33', 19.012556149670385]\n",
      "['id_34', 28.611542726237253]\n",
      "['id_35', 37.640234068604215]\n",
      "['id_36', 43.05208949161719]\n",
      "['id_37', 30.83768433266802]\n",
      "['id_38', 42.18475608811198]\n",
      "['id_39', 34.9211264875934]\n",
      "['id_40', 8.197257288777479]\n",
      "['id_41', 39.35014673218663]\n",
      "['id_42', 32.75518477882936]\n",
      "['id_43', 51.12441563775219]\n",
      "['id_44', 15.956084710321901]\n",
      "['id_45', 36.66516949332919]\n",
      "['id_46', 23.706538931200164]\n",
      "['id_47', 9.888663353841038]\n",
      "['id_48', 25.173948543002624]\n",
      "['id_49', 32.53255204291599]\n",
      "['id_50', 19.326407319517365]\n",
      "['id_51', 8.870443099669892]\n",
      "['id_52', 24.110283217458765]\n",
      "['id_53', 53.31257658312751]\n",
      "['id_54', 15.34690893780276]\n",
      "['id_55', 35.591129201613185]\n",
      "['id_56', 32.52102146593384]\n",
      "['id_57', 21.47361635289647]\n",
      "['id_58', 57.897382248523186]\n",
      "['id_59', 23.585008455187268]\n",
      "['id_60', 13.596251586249736]\n",
      "['id_61', 42.50797959030396]\n",
      "['id_62', 13.05177227609945]\n",
      "['id_63', 50.18920186606382]\n",
      "['id_64', 15.275872210922298]\n",
      "['id_65', 16.403653414748078]\n",
      "['id_66', 16.79699423204403]\n",
      "['id_67', 0.694590708779093]\n",
      "['id_68', 42.66499515851177]\n",
      "['id_69', 30.433364679008875]\n",
      "['id_70', 21.761599358903332]\n",
      "['id_71', 40.54056961452866]\n",
      "['id_72', 60.894599758713554]\n",
      "['id_73', 4.953207760793033]\n",
      "['id_74', 16.78352048915272]\n",
      "['id_75', 3.503468196813161]\n",
      "['id_76', 38.71524571277209]\n",
      "['id_77', 13.846390963681845]\n",
      "['id_78', 23.284158129142213]\n",
      "['id_79', 21.04613147756853]\n",
      "['id_80', 23.64761711617953]\n",
      "['id_81', 36.75533688894333]\n",
      "['id_82', 20.93924728777271]\n",
      "['id_83', 92.39192561540787]\n",
      "['id_84', 36.739846928137226]\n",
      "['id_85', 27.508386774224277]\n",
      "['id_86', 20.98906191748122]\n",
      "['id_87', 32.69245595596634]\n",
      "['id_88', 22.945826713150446]\n",
      "['id_89', 20.14286177383972]\n",
      "['id_90', 28.904851606596576]\n",
      "['id_91', 41.22926006677019]\n",
      "['id_92', 4.782760694161855]\n",
      "['id_93', 38.878810671303434]\n",
      "['id_94', 46.21687458038609]\n",
      "['id_95', 17.035773563907124]\n",
      "['id_96', 32.073514827927255]\n",
      "['id_97', 13.238400358585615]\n",
      "['id_98', 23.320632534039362]\n",
      "['id_99', 4.331948072349469]\n",
      "['id_100', 18.70354425202423]\n",
      "['id_101', 27.520201650948295]\n",
      "['id_102', 16.61869348481006]\n",
      "['id_103', 16.624872824176126]\n",
      "['id_104', 23.236768824444212]\n",
      "['id_105', 38.465657138501726]\n",
      "['id_106', 31.256851959868456]\n",
      "['id_107', 7.153686100795968]\n",
      "['id_108', 6.535931434727976]\n",
      "['id_109', 78.59934795062051]\n",
      "['id_110', 46.10073949273096]\n",
      "['id_111', 16.31422348335175]\n",
      "['id_112', 27.925793880228557]\n",
      "['id_113', 15.784706487888386]\n",
      "['id_114', 13.073568932483944]\n",
      "['id_115', 25.396906384325344]\n",
      "['id_116', 26.96876876641379]\n",
      "['id_117', 8.90350774832416]\n",
      "['id_118', 17.11312374903615]\n",
      "['id_119', 19.066759367798547]\n",
      "['id_120', 81.59898507324222]\n",
      "['id_121', 23.79992827819877]\n",
      "['id_122', 34.26964010776544]\n",
      "['id_123', 24.819232965176024]\n",
      "['id_124', 7.074155180401892]\n",
      "['id_125', 39.836894869761906]\n",
      "['id_126', 9.257161630180757]\n",
      "['id_127', 21.875866280938904]\n",
      "['id_128', 30.067328575071663]\n",
      "['id_129', 61.946348392577555]\n",
      "['id_130', 21.734604682397684]\n",
      "['id_131', 24.22299514866758]\n",
      "['id_132', 58.34167713686924]\n",
      "['id_133', 15.084961943430486]\n",
      "['id_134', 12.793362331091874]\n",
      "['id_135', 2.143404500404647]\n",
      "['id_136', 12.857641327054754]\n",
      "['id_137', 58.226655350587265]\n",
      "['id_138', 19.614691504864407]\n",
      "['id_139', 5.727930496967236]\n",
      "['id_140', 27.809766931051943]\n",
      "['id_141', 25.254760045259374]\n",
      "['id_142', 43.88625055777883]\n",
      "['id_143', 30.140134097183882]\n",
      "['id_144', 17.632899392521967]\n",
      "['id_145', 26.159388821713918]\n",
      "['id_146', 11.124413108828954]\n",
      "['id_147', 51.142559029541275]\n",
      "['id_148', 23.03629985233098]\n",
      "['id_149', 39.36516104434192]\n",
      "['id_150', 11.057182781220085]\n",
      "['id_151', 7.633797164801138]\n",
      "['id_152', 25.167725554124043]\n",
      "['id_153', 6.39242099067876]\n",
      "['id_154', 15.457843059025944]\n",
      "['id_155', 40.62272076416723]\n",
      "['id_156', 8.293434888026043]\n",
      "['id_157', 37.439694723299255]\n",
      "['id_158', 11.324309301103565]\n",
      "['id_159', 19.008905631110917]\n",
      "['id_160', 42.09548057447516]\n",
      "['id_161', 18.65579247182422]\n",
      "['id_162', 14.19461074630919]\n",
      "['id_163', 7.608284525844939]\n",
      "['id_164', 52.296211328057794]\n",
      "['id_165', 29.748379826200523]\n",
      "['id_166', 2.987565196210788]\n",
      "['id_167', 16.15199202855049]\n",
      "['id_168', 64.41376111298106]\n",
      "['id_169', 12.693646237879829]\n",
      "['id_170', 63.64544634647378]\n",
      "['id_171', 38.865912958068265]\n",
      "['id_172', 26.59377422516767]\n",
      "['id_173', 19.219486545638116]\n",
      "['id_174', 62.714061665629224]\n",
      "['id_175', 24.80751639243839]\n",
      "['id_176', 19.90370173397601]\n",
      "['id_177', 38.78186396758402]\n",
      "['id_178', 12.072293559708601]\n",
      "['id_179', 30.7175799067034]\n",
      "['id_180', 17.57646790349202]\n",
      "['id_181', 12.458763339016663]\n",
      "['id_182', 54.49005482235804]\n",
      "['id_183', 46.435508552945386]\n",
      "['id_184', 16.088171702704184]\n",
      "['id_185', 34.708231182828975]\n",
      "['id_186', 26.59849134936049]\n",
      "['id_187', 71.8180952624491]\n",
      "['id_188', 9.916925788530355]\n",
      "['id_189', 58.77662753191211]\n",
      "['id_190', 38.41833275349034]\n",
      "['id_191', 14.166528826406065]\n",
      "['id_192', 28.84794578608108]\n",
      "['id_193', 1.6115598279350074]\n",
      "['id_194', 18.778142450571472]\n",
      "['id_195', 1.1772237645825872]\n",
      "['id_196', 33.953107036978764]\n",
      "['id_197', 11.542610687092381]\n",
      "['id_198', 18.875291991277685]\n",
      "['id_199', 61.95091286825114]\n",
      "['id_200', 24.550825625644535]\n",
      "['id_201', 25.96503476867838]\n",
      "['id_202', 65.73710360291872]\n",
      "['id_203', 11.886929717035216]\n",
      "['id_204', 8.584469869026314]\n",
      "['id_205', 11.153373147996474]\n",
      "['id_206', 7.457563701803539]\n",
      "['id_207', 1.4682217687060906]\n",
      "['id_208', 123.75012648197027]\n",
      "['id_209', 20.561334222008604]\n",
      "['id_210', 14.963423731997374]\n",
      "['id_211', 14.27541606252068]\n",
      "['id_212', 36.6724830957812]\n",
      "['id_213', 35.922196909356494]\n",
      "['id_214', 20.648828149282615]\n",
      "['id_215', 34.73565503238848]\n",
      "['id_216', 78.9154178674505]\n",
      "['id_217', 0.8512215457899877]\n",
      "['id_218', 12.64227350704868]\n",
      "['id_219', 32.53244491185449]\n",
      "['id_220', 15.55312102880342]\n",
      "['id_221', 12.314275496683791]\n",
      "['id_222', 115.87930984410187]\n",
      "['id_223', 12.401528394361222]\n",
      "['id_224', 16.834224367877898]\n",
      "['id_225', 64.65999974454766]\n",
      "['id_226', 16.075338853162716]\n",
      "['id_227', 17.89736923331688]\n",
      "['id_228', 10.158167126828543]\n",
      "['id_229', 7.098830459556455]\n",
      "['id_230', 44.372902442523845]\n",
      "['id_231', 12.714333410223357]\n",
      "['id_232', 53.87768020437954]\n",
      "['id_233', 43.13215460359693]\n",
      "['id_234', 25.56041036998748]\n",
      "['id_235', 42.679019251746254]\n",
      "['id_236', 69.05492153996396]\n",
      "['id_237', 43.52810299910358]\n",
      "['id_238', 10.984886860329812]\n",
      "['id_239', 17.255120758304084]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('./../results/submit6.csv', mode='w', newline='') as submit_file:\n",
    "    csv_writer = csv.writer(submit_file)\n",
    "    header = ['id', 'value']\n",
    "    print(header)\n",
    "    csv_writer.writerow(header)\n",
    "    for i in range(240):\n",
    "        row = ['id_' + str(i), ans_y[i][0]]\n",
    "        csv_writer.writerow(row)\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
