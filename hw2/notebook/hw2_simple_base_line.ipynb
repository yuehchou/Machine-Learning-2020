{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSL2CMEzmQvB"
   },
   "source": [
    "# **Homework 2 - Classification**\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ox7joE3aZkh-"
   },
   "source": [
    "Binary classification is one of the most fundamental problem in machine learning. In this tutorial, you are going to build linear binary classifiers to predict whether the income of an indivisual exceeds 50,000 or not. We presented a discriminative and a generative approaches, the logistic regression(LR) and the linear discriminant anaysis(LDA). You are encouraged to compare the differences between the two, or explore more methodologies. Although you can finish this tutorial by simpliy copying and pasting the codes, we strongly recommend you to understand the mathematical formulation first to get more insight into the two algorithms. Please find [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) and [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf) for more detailed information about the two algorithms.\n",
    "\n",
    "二元分類是機器學習中最基礎的問題之一，在這份教學中，你將學會如何實作一個線性二元分類器，來根據人們的個人資料，判斷其年收入是否高於 50,000 美元。我們將以兩種方法: logistic regression 與 generative model，來達成以上目的，你可以嘗試了解、分析兩者的設計理念及差別。針對這兩個演算法的理論基礎，可以參考李宏毅老師的教學投影片 [logistic regression](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) 與 [generative model](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf)。\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkNW5cQmohoo"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "This dataset is obtained by removing unnecessary attributes and balancing the ratio between positively and negatively labeled data in the [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)), which can be found in [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php). Only preprocessed and one-hot encoded data (i.e. *X_train*,  *Y_train* and *X_test*) will be used in this tutorial. Raw data (i.e. *train.csv* and *test.csv*) are provided to you in case you are interested in it.\n",
    "\n",
    "這個資料集是由 [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php) 的 [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)) 經過一些處理而得來。為了方便訓練，我們移除了一些不必要的資訊，並且稍微平衡了正負兩種標記的比例。事實上在訓練過程中，只有 X_train、Y_train 和 X_test 這三個經過處理的檔案會被使用到，train.csv 和 test.csv 這兩個原始資料檔則可以提供你一些額外的資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRXI0kf0W4Bd"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this section we will introduce logistic regression first. We only present how to implement it here, while mathematical formulation and analysis will be omitted. You can find more theoretical detail in [Prof. Lee's lecture](https://www.youtube.com/watch?v=hSXFuypLukA).\n",
    "\n",
    "首先我們會實作 logistic regression，針對理論細節說明請參考[李宏毅老師的教學影片](https://www.youtube.com/watch?v=hSXFuypLukA)\n",
    "\n",
    "### Preparing Data\n",
    "\n",
    "Load and normalize data, and then split training data into training set and development set.\n",
    "\n",
    "下載資料，並且對每個屬性做正規化，處理過後再將其切分為訓練集與發展集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "7NzAmkzU2MAS",
    "outputId": "61610be3-295e-4ff8-befe-8044938141c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 48830\n",
      "Size of development set: 5426\n",
      "Size of testing set: 27622\n",
      "Dimension of data: 510\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train_fpath = '../data/X_train'\n",
    "Y_train_fpath = '../data/Y_train'\n",
    "X_test_fpath = '../data/X_test'\n",
    "output_fpath = '../results/output_{}.csv'\n",
    "\n",
    "# Parse csv files to numpy array\n",
    "with open(X_train_fpath) as f:\n",
    "    next(f)\n",
    "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "with open(Y_train_fpath) as f:\n",
    "    next(f)\n",
    "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
    "with open(X_test_fpath) as f:\n",
    "    next(f)\n",
    "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "\n",
    "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
    "    # This function normalizes specific columns of X.\n",
    "    # The mean and standard variance of training data will be reused when processing testing data.\n",
    "    #\n",
    "    # Arguments:\n",
    "    #     X: data to be processed\n",
    "    #     train: 'True' when processing training data, 'False' for testing data\n",
    "    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n",
    "    #         will be normalized.\n",
    "    #     X_mean: mean value of training data, used when train = 'False'\n",
    "    #     X_std: standard deviation of training data, used when train = 'False'\n",
    "    # Outputs:\n",
    "    #     X: normalized data\n",
    "    #     X_mean: computed mean value of training data\n",
    "    #     X_std: computed standard deviation of training data\n",
    "\n",
    "    if specified_column == None:\n",
    "        specified_column = np.arange(X.shape[1])\n",
    "    if train:\n",
    "        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)\n",
    "        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n",
    "\n",
    "    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8)\n",
    "     \n",
    "    return X, X_mean, X_std\n",
    "\n",
    "def _train_dev_split(X, Y, dev_ratio = 0.25):\n",
    "    # This function spilts data into training set and development set.\n",
    "    train_size = int(len(X) * (1 - dev_ratio))\n",
    "    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n",
    "\n",
    "# Normalize training and testing data\n",
    "X_train, X_mean, X_std = _normalize(X_train, train = True)\n",
    "X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)\n",
    "    \n",
    "# Split data into training set and development set\n",
    "dev_ratio = 0.1\n",
    "X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "dev_size = X_dev.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "data_dim = X_train.shape[1]\n",
    "print('Size of training set: {}'.format(train_size))\n",
    "print('Size of development set: {}'.format(dev_size))\n",
    "print('Size of testing set: {}'.format(test_size))\n",
    "print('Dimension of data: {}'.format(data_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imgCeBDoApdb"
   },
   "source": [
    "\n",
    "### Some Useful Functions\n",
    "\n",
    "Some functions that will be repeatedly used when iteratively updating the parameters.\n",
    "\n",
    "這幾個函數可能會在訓練迴圈中被重複使用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSDAw5LTAs2o"
   },
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    # This function shuffles two equal-length list/array, X and Y, together.\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def _sigmoid(z):\n",
    "    # Sigmoid function can be used to calculate probability.\n",
    "    # To avoid overflow, minimum/maximum output value is set.\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n",
    "\n",
    "def _f(X, w, b):\n",
    "    # This is the logistic regression function, parameterized by w and b\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     X: input data, shape = [batch_size, data_dimension]\n",
    "    #     w: weight vector, shape = [data_dimension, ]\n",
    "    #     b: bias, scalar\n",
    "    # Output:\n",
    "    #     predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
    "    return _sigmoid(np.matmul(X, w) + b)\n",
    "\n",
    "def _predict(X, w, b):\n",
    "    # This function returns a truth value prediction for each row of X \n",
    "    # by rounding the result of logistic regression function.\n",
    "    return np.round(_f(X, w, b)).astype(np.int)\n",
    "    \n",
    "def _accuracy(Y_pred, Y_label):\n",
    "    # This function calculates prediction accuracy\n",
    "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxJdfhEEOYwg"
   },
   "source": [
    "### Functions about gradient and loss\n",
    "\n",
    "Please refers to [Prof. Lee's lecture slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)(p.12) for the formula of gradient and loss computation.\n",
    "\n",
    "請參考[李宏毅老師上課投影片](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)第 12 頁的梯度及損失函數計算公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqYkUgLjOWi1"
   },
   "outputs": [],
   "source": [
    "def _cross_entropy_loss(y_pred, Y_label):\n",
    "    # This function computes the cross entropy.\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     y_pred: probabilistic predictions, float vector\n",
    "    #     Y_label: ground truth labels, bool vector\n",
    "    # Output:\n",
    "    #     cross entropy, scalar\n",
    "    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))\n",
    "    return cross_entropy\n",
    "\n",
    "def _gradient(X, Y_label, w, b):\n",
    "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
    "    y_pred = _f(X, w, b)\n",
    "    pred_error = Y_label - y_pred\n",
    "    w_grad = -np.sum(pred_error * X.T, 1)\n",
    "    b_grad = -np.sum(pred_error)\n",
    "    return w_grad, b_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _progress_bar(\n",
    "    epoch,\n",
    "    max_iter,\n",
    "    percentage,\n",
    "    training_acc,\n",
    "    training_loss,\n",
    "    dev_acc,\n",
    "    dev_loss,\n",
    "    block = 10\n",
    "):\n",
    "    if percentage == 100.:\n",
    "        print('\\r',\n",
    "              '[Epoch %d/%d]:[%s]%.1f%%' % (epoch, max_iter, '█' * block, 100.0),\n",
    "              'training_acc: %.4f' % training_acc,\n",
    "              'training_loss: %.4f' % training_loss,\n",
    "              'dev_acc: %.4f' % dev_acc,\n",
    "              'dev_loss: %.4f' % dev_loss,\n",
    "              ';\\n\\n',\n",
    "              end = '')\n",
    "    else:\n",
    "        print('\\r',\n",
    "              '[Epoch %d/%d]:[%s%s]%.1f%%' % (epoch, max_iter,\n",
    "                                           '█' * int(percentage*block/100),\n",
    "                                           ' ' * (block-int(percentage*block/100)),\n",
    "                                           float(percentage)),\n",
    "              end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXEFuqydaA34"
   },
   "source": [
    "### Training\n",
    "\n",
    "Everything is prepared, let's start training! \n",
    "\n",
    "Mini-batch gradient descent is used here, in which training data are split into several mini-batches and each batch is fed into the model sequentially for losses and gradients computation. Weights and bias are updated on a mini-batch basis.\n",
    "\n",
    "Once we have gone through the whole training set,  the data have to be re-shuffled and mini-batch gradient desent has to be run on it again. We repeat such process until max number of iterations is reached.\n",
    "\n",
    "我們使用小批次梯度下降法來訓練。訓練資料被分為許多小批次，針對每一個小批次，我們分別計算其梯度以及損失，並根據該批次來更新模型的參數。當一次迴圈完成，也就是整個訓練集的所有小批次都被使用過一次以後，我們將所有訓練資料打散並且重新分成新的小批次，進行下一個迴圈，直到事先設定的迴圈數量達成為止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 1/6000]:[██████████]100.0% training_acc: 0.8709 training_loss: 0.3411 dev_acc: 0.8614 dev_loss: 0.3501 ;\n",
      "\n",
      " [Epoch 2/6000]:[██████████]100.0% training_acc: 0.8759 training_loss: 0.3177 dev_acc: 0.8645 dev_loss: 0.3284 ;\n",
      "\n",
      " [Epoch 3/6000]:[██████████]100.0% training_acc: 0.8775 training_loss: 0.3069 dev_acc: 0.8686 dev_loss: 0.3185 ;\n",
      "\n",
      " [Epoch 4/6000]:[██████████]100.0% training_acc: 0.8781 training_loss: 0.3004 dev_acc: 0.8688 dev_loss: 0.3125 ;\n",
      "\n",
      " [Epoch 5/6000]:[██████████]100.0% training_acc: 0.8789 training_loss: 0.2959 dev_acc: 0.8701 dev_loss: 0.3085 ;\n",
      "\n",
      " [Epoch 6/6000]:[██████████]100.0% training_acc: 0.8796 training_loss: 0.2926 dev_acc: 0.8712 dev_loss: 0.3056 ;\n",
      "\n",
      " [Epoch 7/6000]:[██████████]100.0% training_acc: 0.8800 training_loss: 0.2901 dev_acc: 0.8712 dev_loss: 0.3034 ;\n",
      "\n",
      " [Epoch 8/6000]:[██████████]100.0% training_acc: 0.8803 training_loss: 0.2881 dev_acc: 0.8715 dev_loss: 0.3016 ;\n",
      "\n",
      " [Epoch 9/6000]:[██████████]100.0% training_acc: 0.8806 training_loss: 0.2864 dev_acc: 0.8719 dev_loss: 0.3001 ;\n",
      "\n",
      " [Epoch 10/6000]:[██████████]100.0% training_acc: 0.8809 training_loss: 0.2849 dev_acc: 0.8721 dev_loss: 0.2990 ;\n",
      "\n",
      " [Epoch 11/6000]:[██████████]100.0% training_acc: 0.8813 training_loss: 0.2837 dev_acc: 0.8714 dev_loss: 0.2979 ;\n",
      "\n",
      " [Epoch 12/6000]:[██████████]100.0% training_acc: 0.8815 training_loss: 0.2827 dev_acc: 0.8715 dev_loss: 0.2971 ;\n",
      "\n",
      " [Epoch 13/6000]:[██████████]100.0% training_acc: 0.8818 training_loss: 0.2818 dev_acc: 0.8712 dev_loss: 0.2963 ;\n",
      "\n",
      " [Epoch 14/6000]:[██████████]100.0% training_acc: 0.8819 training_loss: 0.2809 dev_acc: 0.8714 dev_loss: 0.2957 ;\n",
      "\n",
      " [Epoch 15/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2802 dev_acc: 0.8715 dev_loss: 0.2950 ;\n",
      "\n",
      " [Epoch 16/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2795 dev_acc: 0.8721 dev_loss: 0.2945 ;\n",
      "\n",
      " [Epoch 17/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2789 dev_acc: 0.8723 dev_loss: 0.2940 ;\n",
      "\n",
      " [Epoch 18/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2784 dev_acc: 0.8723 dev_loss: 0.2936 ;\n",
      "\n",
      " [Epoch 19/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2779 dev_acc: 0.8723 dev_loss: 0.2932 ;\n",
      "\n",
      " [Epoch 20/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2774 dev_acc: 0.8727 dev_loss: 0.2928 ;\n",
      "\n",
      " [Epoch 21/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2770 dev_acc: 0.8728 dev_loss: 0.2925 ;\n",
      "\n",
      " [Epoch 22/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2766 dev_acc: 0.8725 dev_loss: 0.2922 ;\n",
      "\n",
      " [Epoch 23/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2762 dev_acc: 0.8727 dev_loss: 0.2919 ;\n",
      "\n",
      " [Epoch 24/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2759 dev_acc: 0.8727 dev_loss: 0.2916 ;\n",
      "\n",
      " [Epoch 25/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2756 dev_acc: 0.8728 dev_loss: 0.2914 ;\n",
      "\n",
      " [Epoch 26/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2753 dev_acc: 0.8732 dev_loss: 0.2911 ;\n",
      "\n",
      " [Epoch 27/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2750 dev_acc: 0.8738 dev_loss: 0.2909 ;\n",
      "\n",
      " [Epoch 28/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2747 dev_acc: 0.8739 dev_loss: 0.2907 ;\n",
      "\n",
      " [Epoch 29/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2745 dev_acc: 0.8741 dev_loss: 0.2905 ;\n",
      "\n",
      " [Epoch 30/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2742 dev_acc: 0.8745 dev_loss: 0.2903 ;\n",
      "\n",
      " [Epoch 31/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2740 dev_acc: 0.8741 dev_loss: 0.2901 ;\n",
      "\n",
      " [Epoch 32/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2738 dev_acc: 0.8743 dev_loss: 0.2900 ;\n",
      "\n",
      " [Epoch 33/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2736 dev_acc: 0.8741 dev_loss: 0.2898 ;\n",
      "\n",
      " [Epoch 34/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2734 dev_acc: 0.8741 dev_loss: 0.2897 ;\n",
      "\n",
      " [Epoch 35/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2732 dev_acc: 0.8743 dev_loss: 0.2895 ;\n",
      "\n",
      " [Epoch 36/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2730 dev_acc: 0.8747 dev_loss: 0.2894 ;\n",
      "\n",
      " [Epoch 37/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2728 dev_acc: 0.8745 dev_loss: 0.2893 ;\n",
      "\n",
      " [Epoch 38/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2727 dev_acc: 0.8745 dev_loss: 0.2891 ;\n",
      "\n",
      " [Epoch 39/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2725 dev_acc: 0.8745 dev_loss: 0.2890 ;\n",
      "\n",
      " [Epoch 40/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2724 dev_acc: 0.8745 dev_loss: 0.2889 ;\n",
      "\n",
      " [Epoch 41/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2722 dev_acc: 0.8747 dev_loss: 0.2888 ;\n",
      "\n",
      " [Epoch 42/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2721 dev_acc: 0.8747 dev_loss: 0.2887 ;\n",
      "\n",
      " [Epoch 43/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2719 dev_acc: 0.8745 dev_loss: 0.2886 ;\n",
      "\n",
      " [Epoch 44/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2718 dev_acc: 0.8743 dev_loss: 0.2885 ;\n",
      "\n",
      " [Epoch 45/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2717 dev_acc: 0.8745 dev_loss: 0.2884 ;\n",
      "\n",
      " [Epoch 46/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2716 dev_acc: 0.8747 dev_loss: 0.2883 ;\n",
      "\n",
      " [Epoch 47/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2714 dev_acc: 0.8747 dev_loss: 0.2882 ;\n",
      "\n",
      " [Epoch 48/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2713 dev_acc: 0.8747 dev_loss: 0.2882 ;\n",
      "\n",
      " [Epoch 49/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2712 dev_acc: 0.8747 dev_loss: 0.2881 ;\n",
      "\n",
      " [Epoch 50/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2711 dev_acc: 0.8749 dev_loss: 0.2880 ;\n",
      "\n",
      " [Epoch 51/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2710 dev_acc: 0.8749 dev_loss: 0.2879 ;\n",
      "\n",
      " [Epoch 52/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2709 dev_acc: 0.8750 dev_loss: 0.2879 ;\n",
      "\n",
      " [Epoch 53/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2708 dev_acc: 0.8752 dev_loss: 0.2878 ;\n",
      "\n",
      " [Epoch 54/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2707 dev_acc: 0.8754 dev_loss: 0.2877 ;\n",
      "\n",
      " [Epoch 55/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2706 dev_acc: 0.8754 dev_loss: 0.2876 ;\n",
      "\n",
      " [Epoch 56/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2706 dev_acc: 0.8758 dev_loss: 0.2876 ;\n",
      "\n",
      " [Epoch 57/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2705 dev_acc: 0.8756 dev_loss: 0.2875 ;\n",
      "\n",
      " [Epoch 58/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2704 dev_acc: 0.8758 dev_loss: 0.2875 ;\n",
      "\n",
      " [Epoch 59/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2703 dev_acc: 0.8760 dev_loss: 0.2874 ;\n",
      "\n",
      " [Epoch 60/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2702 dev_acc: 0.8762 dev_loss: 0.2873 ;\n",
      "\n",
      " [Epoch 61/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2702 dev_acc: 0.8760 dev_loss: 0.2873 ;\n",
      "\n",
      " [Epoch 62/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2701 dev_acc: 0.8760 dev_loss: 0.2872 ;\n",
      "\n",
      " [Epoch 63/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2700 dev_acc: 0.8762 dev_loss: 0.2872 ;\n",
      "\n",
      " [Epoch 64/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2699 dev_acc: 0.8763 dev_loss: 0.2871 ;\n",
      "\n",
      " [Epoch 65/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2699 dev_acc: 0.8763 dev_loss: 0.2871 ;\n",
      "\n",
      " [Epoch 66/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2698 dev_acc: 0.8763 dev_loss: 0.2870 ;\n",
      "\n",
      " [Epoch 67/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2697 dev_acc: 0.8763 dev_loss: 0.2870 ;\n",
      "\n",
      " [Epoch 68/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2697 dev_acc: 0.8763 dev_loss: 0.2869 ;\n",
      "\n",
      " [Epoch 69/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2696 dev_acc: 0.8763 dev_loss: 0.2869 ;\n",
      "\n",
      " [Epoch 70/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2695 dev_acc: 0.8765 dev_loss: 0.2869 ;\n",
      "\n",
      " [Epoch 71/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2695 dev_acc: 0.8765 dev_loss: 0.2868 ;\n",
      "\n",
      " [Epoch 72/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2694 dev_acc: 0.8765 dev_loss: 0.2868 ;\n",
      "\n",
      " [Epoch 73/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2694 dev_acc: 0.8765 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 74/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2693 dev_acc: 0.8769 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 75/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2693 dev_acc: 0.8769 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 76/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2692 dev_acc: 0.8767 dev_loss: 0.2866 ;\n",
      "\n",
      " [Epoch 77/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2692 dev_acc: 0.8769 dev_loss: 0.2866 ;\n",
      "\n",
      " [Epoch 78/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2691 dev_acc: 0.8769 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 79/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2691 dev_acc: 0.8769 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 80/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2690 dev_acc: 0.8769 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 81/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2690 dev_acc: 0.8769 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 82/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2689 dev_acc: 0.8769 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 83/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2689 dev_acc: 0.8769 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 84/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2688 dev_acc: 0.8767 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 85/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2688 dev_acc: 0.8771 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 86/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2687 dev_acc: 0.8769 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 87/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2687 dev_acc: 0.8769 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 88/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2687 dev_acc: 0.8769 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 89/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2686 dev_acc: 0.8767 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 90/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2686 dev_acc: 0.8769 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 91/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2685 dev_acc: 0.8769 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 92/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2685 dev_acc: 0.8767 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 93/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2685 dev_acc: 0.8769 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 94/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2684 dev_acc: 0.8769 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 95/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2684 dev_acc: 0.8767 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 96/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2683 dev_acc: 0.8767 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 97/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2683 dev_acc: 0.8767 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 98/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2683 dev_acc: 0.8767 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 99/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2682 dev_acc: 0.8767 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 100/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2682 dev_acc: 0.8767 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 101/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2682 dev_acc: 0.8767 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 102/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2681 dev_acc: 0.8767 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 103/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2681 dev_acc: 0.8771 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 104/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2681 dev_acc: 0.8769 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 105/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2680 dev_acc: 0.8773 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 106/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2680 dev_acc: 0.8773 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 107/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2680 dev_acc: 0.8773 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 108/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2680 dev_acc: 0.8773 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 109/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2679 dev_acc: 0.8773 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 110/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2679 dev_acc: 0.8773 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 111/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2679 dev_acc: 0.8773 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 112/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2678 dev_acc: 0.8773 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 113/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2678 dev_acc: 0.8773 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 114/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2678 dev_acc: 0.8774 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 115/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2678 dev_acc: 0.8774 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 116/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2677 dev_acc: 0.8773 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 117/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2677 dev_acc: 0.8773 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 118/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2677 dev_acc: 0.8771 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 119/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2677 dev_acc: 0.8771 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 120/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2676 dev_acc: 0.8771 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 121/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2676 dev_acc: 0.8771 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 122/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2676 dev_acc: 0.8771 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 123/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2676 dev_acc: 0.8773 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 124/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2675 dev_acc: 0.8773 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 125/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2675 dev_acc: 0.8773 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 126/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2675 dev_acc: 0.8771 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 127/6000]:[██████████]100.0% training_acc: 0.8843 training_loss: 0.2675 dev_acc: 0.8773 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 128/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2674 dev_acc: 0.8774 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 129/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2674 dev_acc: 0.8774 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 130/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2674 dev_acc: 0.8774 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 131/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2674 dev_acc: 0.8774 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 132/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2674 dev_acc: 0.8774 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 133/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2673 dev_acc: 0.8774 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 134/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2673 dev_acc: 0.8776 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 135/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2673 dev_acc: 0.8776 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 136/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2673 dev_acc: 0.8776 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 137/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2673 dev_acc: 0.8776 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 138/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2672 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 139/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2672 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 140/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2672 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 141/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2672 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 142/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2672 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 143/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2671 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 144/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2671 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 145/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2671 dev_acc: 0.8776 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 146/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2671 dev_acc: 0.8774 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 147/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2671 dev_acc: 0.8774 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 148/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2671 dev_acc: 0.8773 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 149/6000]:[██████████]100.0% training_acc: 0.8844 training_loss: 0.2670 dev_acc: 0.8774 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 150/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2670 dev_acc: 0.8773 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 151/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2670 dev_acc: 0.8773 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 152/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2670 dev_acc: 0.8773 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 153/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2670 dev_acc: 0.8773 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 154/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2670 dev_acc: 0.8773 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 155/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2669 dev_acc: 0.8774 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 156/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2669 dev_acc: 0.8774 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 157/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2669 dev_acc: 0.8774 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 158/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2669 dev_acc: 0.8774 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 159/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2669 dev_acc: 0.8774 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 160/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2669 dev_acc: 0.8776 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 161/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2668 dev_acc: 0.8776 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 162/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2668 dev_acc: 0.8776 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 163/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2668 dev_acc: 0.8776 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 164/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2668 dev_acc: 0.8778 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 165/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2668 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 166/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2668 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 167/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2668 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 168/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 169/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 170/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 171/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 172/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 173/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 174/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2667 dev_acc: 0.8780 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 175/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2666 dev_acc: 0.8780 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 176/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 177/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 178/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 179/6000]:[██████████]100.0% training_acc: 0.8846 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 180/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 181/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 182/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2666 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 183/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 184/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 185/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8782 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 186/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8782 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 187/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 188/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8782 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 189/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 190/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 191/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2665 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 192/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 193/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 194/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 195/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 196/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 197/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 198/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 199/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 200/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2664 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 201/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 202/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 203/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 204/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 205/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 206/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 207/6000]:[██████████]100.0% training_acc: 0.8847 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 208/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 209/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 210/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2663 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 211/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 212/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 213/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2846 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 214/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 215/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 216/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 217/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 218/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 219/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 220/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 221/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2662 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 222/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 223/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8784 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 224/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 225/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 226/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 227/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 228/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 229/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 230/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 231/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 232/6000]:[██████████]100.0% training_acc: 0.8848 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 233/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2661 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 234/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 235/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 236/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 237/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 238/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 239/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 240/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 241/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 242/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 243/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 244/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 245/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 246/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 247/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2660 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 248/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 249/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 250/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 251/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 252/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 253/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 254/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 255/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 256/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 257/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2659 dev_acc: 0.8785 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 258/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 259/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 260/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 261/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2659 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 262/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 263/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 264/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 265/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 266/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8784 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 267/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 268/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 269/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 270/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 271/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 272/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 273/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 274/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 275/6000]:[██████████]100.0% training_acc: 0.8849 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 276/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 277/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 278/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2658 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 279/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 280/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 281/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 282/6000]:[██████████]100.0% training_acc: 0.8850 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 283/6000]:[██████████]100.0% training_acc: 0.8851 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 284/6000]:[██████████]100.0% training_acc: 0.8851 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 285/6000]:[██████████]100.0% training_acc: 0.8851 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 286/6000]:[██████████]100.0% training_acc: 0.8851 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 287/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 288/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 289/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 290/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 291/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 292/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8780 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 293/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 294/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 295/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 296/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2657 dev_acc: 0.8782 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 297/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 298/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 299/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 300/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 301/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 302/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 303/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8785 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 304/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 305/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 306/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 307/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 308/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 309/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 310/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 311/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 312/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 313/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 314/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 315/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 316/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2656 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 317/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 318/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 319/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 320/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 321/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 322/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 323/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 324/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 325/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 326/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 327/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;6000]:[████      ]49.6%\n",
      "\n",
      " [Epoch 328/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 329/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 330/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 331/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 332/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 333/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8784 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 334/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8782 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 335/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8782 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 336/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8782 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 337/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8782 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 338/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 339/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2655 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 340/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 341/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 342/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 343/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 344/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 345/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 346/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 347/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 348/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 349/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 350/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 351/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8780 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 352/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 353/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 354/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 355/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 356/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 357/6000]:[██████████]100.0% training_acc: 0.8852 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 358/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 359/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 360/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 361/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 362/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 363/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 364/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 365/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2654 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 366/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 367/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 368/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 369/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 370/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 371/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 372/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 373/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 374/6000]:[██████████]100.0% training_acc: 0.8853 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 375/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 376/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 377/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 378/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 379/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 380/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 381/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 382/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 383/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 384/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 385/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 386/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 387/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 388/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 389/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 390/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 391/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 392/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 393/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 394/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 395/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2653 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 396/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 397/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 398/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 399/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 400/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 401/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 402/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 403/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 404/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 405/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 406/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 407/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 408/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 409/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 410/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 411/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 412/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 413/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 414/6000]:[██████████]100.0% training_acc: 0.8854 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 415/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 416/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 417/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 418/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 419/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 420/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 421/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 422/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 423/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 424/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 425/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 426/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 427/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 428/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 429/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 430/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2652 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 431/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2651 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 432/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2651 dev_acc: 0.8784 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 433/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 434/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 435/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 436/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 437/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 438/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 439/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 440/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8782 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 441/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 442/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 443/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 444/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 445/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 446/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 447/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 448/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 449/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 450/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 451/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 452/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 453/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 454/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 455/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 456/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 457/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 458/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 459/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 460/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 461/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 462/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 463/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 464/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 465/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 466/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 467/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 468/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 469/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 470/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 471/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2651 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 472/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 473/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 474/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 475/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8778 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 476/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 477/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 478/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8782 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 479/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 480/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 481/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 482/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 483/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 484/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 485/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 486/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 487/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 488/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 489/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 490/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 491/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 492/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 493/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 494/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 495/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 496/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 497/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 498/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 499/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 500/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 501/6000]:[█████████ ]99.1%[Epoch 501/6000]:[█         ]16.8%[Epoch 501/6000]:[████████  ]88.9%[Epoch 501/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 502/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 503/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 504/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 505/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 506/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 507/6000]:[██████████]100.0% training_acc: 0.8855 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 508/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 509/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 510/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 511/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 512/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 513/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 514/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 515/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 516/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 517/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 518/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 519/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 520/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2650 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 521/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 522/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 523/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 524/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 525/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 526/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 527/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 528/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 529/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8778 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 530/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 531/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 532/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 533/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 534/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 535/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 536/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 537/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 538/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 539/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 540/6000]:[██████████]100.0% training_acc: 0.8856 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 541/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 542/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 543/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 544/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 545/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 546/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 547/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 548/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 549/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 550/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 551/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 552/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 553/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 554/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 555/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 556/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 557/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 558/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 559/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 560/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 561/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 562/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 563/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 564/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 565/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 566/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 567/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 568/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 569/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 570/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 571/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 572/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 573/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 574/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 575/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 576/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 577/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 578/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 579/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2649 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 580/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 581/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 582/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 583/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 584/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 585/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 586/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 587/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 588/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 589/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 590/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 591/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 592/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 593/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 594/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 595/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 596/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 597/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 598/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 599/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 600/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 601/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 602/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 603/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 604/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 605/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 606/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 607/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 608/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 609/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 610/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 611/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 612/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 613/6000]:[██████████]100.0% training_acc: 0.8857 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 614/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 615/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 616/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 617/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;6000]:[█████████ ]92.6%\n",
      "\n",
      " [Epoch 618/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8782 dev_loss: 0.2837 ;6000]:[████████  ]82.7%\n",
      "\n",
      " [Epoch 619/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 620/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 621/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 622/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 623/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 624/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 625/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 626/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 627/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 628/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 629/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 630/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 631/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 632/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 633/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 634/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 635/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 636/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 637/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 638/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 639/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 640/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 641/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 642/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 643/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 644/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 645/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 646/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 647/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 648/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 649/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 650/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 651/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 652/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2648 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 653/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 654/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 655/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 656/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 657/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 658/6000]:[██████████]100.0% training_acc: 0.8859 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 659/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 660/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 661/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 662/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 663/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 664/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 665/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 666/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 667/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 668/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 669/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 670/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 671/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 672/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 673/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 674/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 675/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 676/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 677/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 678/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 679/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 680/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 681/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 682/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 683/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 684/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 685/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;6000]:[██████    ]68.0%\n",
      "\n",
      " [Epoch 686/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 687/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 688/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 689/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 690/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 691/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 692/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 693/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 694/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 695/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 696/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 697/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 698/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 699/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 700/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8780 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 701/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8782 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 702/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 703/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 704/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 705/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 706/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 707/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 708/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 709/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 710/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 711/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 712/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 713/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 714/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 715/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 716/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 717/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 718/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 719/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 720/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 721/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 722/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8784 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 723/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8782 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 724/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8782 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 725/6000]:[██████████]100.0% training_acc: 0.8858 training_loss: 0.2647 dev_acc: 0.8782 dev_loss: 0.2836 ;\n",
      "\n",
      "early stopping at epoch: 224\n"
     ]
    }
   ],
   "source": [
    "# Zero initialization for weights ans bias\n",
    "w = np.zeros((data_dim,)) \n",
    "b = np.zeros((1,))\n",
    "\n",
    "# Some parameters for training    \n",
    "max_iter = 6000\n",
    "batch_size = 200\n",
    "learning_rate = 0.001\n",
    "early_stopping_iter = 100\n",
    "temp_acc = 0\n",
    "temp_epoch = 0\n",
    "\n",
    "# Keep the loss and accuracy at every iteration for plotting\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "# Calcuate the number of parameter updates\n",
    "step = 1\n",
    "\n",
    "# Iterative training\n",
    "for epoch in range(max_iter):\n",
    "    # Random shuffle at the begging of each epoch\n",
    "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for idx in range(int(np.floor(train_size / batch_size))):\n",
    "        X = X_train[idx*batch_size:(idx+1)*batch_size]\n",
    "        Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "        # Compute the gradient\n",
    "        w_grad, b_grad = _gradient(X, Y, w, b)\n",
    "            \n",
    "        # gradient descent update\n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate/np.sqrt(step) * w_grad\n",
    "        b = b - learning_rate/np.sqrt(step) * b_grad\n",
    "\n",
    "        step = step + 1\n",
    "        \n",
    "        y_train_pred = _f(X_train, w, b)\n",
    "        Y_train_pred = np.round(y_train_pred)\n",
    "        T_acc = _accuracy(Y_train_pred, Y_train)\n",
    "        T_loss = _cross_entropy_loss(y_train_pred, Y_train) / train_size\n",
    "        \n",
    "        y_dev_pred = _f(X_dev, w, b)\n",
    "        Y_dev_pred = np.round(y_dev_pred)\n",
    "        D_acc = _accuracy(Y_dev_pred, Y_dev)\n",
    "        D_loss = _cross_entropy_loss(y_dev_pred, Y_dev) / dev_size\n",
    "        \n",
    "        # progress bar\n",
    "        if idx == int(np.floor(train_size / batch_size)) - 1:\n",
    "            _progress_bar(epoch+1, max_iter, 100., T_acc, T_loss, D_acc, D_loss)\n",
    "        else:\n",
    "            _progress_bar(epoch+1, max_iter, idx * batch_size * 100 / train_size, T_acc, T_loss, D_acc, D_loss)\n",
    "            \n",
    "    train_acc.append(T_acc)\n",
    "    train_loss.append(T_loss)\n",
    "\n",
    "    dev_acc.append(D_acc)\n",
    "    dev_loss.append(D_loss)\n",
    "    \n",
    "    # early stopping\n",
    "    if D_acc > temp_acc:\n",
    "        temp_acc = np.copy(D_acc)\n",
    "        temp_epoch = 0\n",
    "        \n",
    "        temp_w = np.copy(w)\n",
    "        temp_b = np.copy(b)\n",
    "        temp_step = np.copy(step)\n",
    "    else:\n",
    "        if temp_epoch < early_stopping_iter:\n",
    "            temp_epoch += 1\n",
    "        else:\n",
    "            print(\"early stopping at epoch:\", epoch - temp_epoch)\n",
    "            \n",
    "            train_acc[:-temp_epoch]\n",
    "            train_loss[:-temp_epoch]\n",
    "\n",
    "            dev_acc[:-temp_epoch]\n",
    "            dev_loss[:-temp_epoch]\n",
    "            \n",
    "            w = temp_w\n",
    "            b = temp_b\n",
    "            step = temp_step\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJuoQ_R2jUmX"
   },
   "source": [
    "### Plotting Loss and accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "DH3AJtvHjVJ7",
    "outputId": "f3fc5d1b-ddcc-4cf6-eea5-ebf23026edc5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXxU9bn48c+Tyb4RErZICAFkCbIJEfddLC51aeuCy9Wrvdi6XGttf7W9ttd621e9v1t7e9ta/WmvdamAWpdCpVWpxboLKJCQoCyyhCVAIPs2mXl+f5wTMoRAAiQ5kznP+/XKa2bOOXPmmfOC73PmOd/z/YqqYowxxn/ivA7AGGOMNywBGGOMT1kCMMYYn7IEYIwxPmUJwBhjfMoSgDHG+JQlAGOM8SlLAMYXRGSpiOwTkSSvYzEmWlgCMDFPRAqAMwEFLuvDz43vq88y5mhYAjB+8E/Ah8BTwE1tC0UkRUQeFpHNIlItIu+KSIq77gwReV9EqkRkq4jc7C5fKiJfj9jHzSLybsRrFZE7RGQdsM5d9j/uPmpEZIWInBmxfUBEfiAiG0Sk1l0/QkQeEZGHI7+EiCwSkW/1xgEy/mQJwPjBPwHPuX9fEpGh7vKfAzOA04Bs4P8AYRHJB/4C/BoYDEwDVh7B510BnAxMdF8vc/eRDcwDXhSRZHfdt4E5wMVAJnAL0AA8DcwRkTgAERkEnA/MP5IvbszhWAIwMU1EzgBGAi+o6gpgA3Cd27DeAtytqttUNaSq76tqM3A9sERV56tqUFUrVfVIEsDPVHWvqjYCqOof3H20qurDQBIw3t3268D9qvqZOla5234MVOM0+gDXAktVteIYD4kx+1kCMLHuJuANVd3jvp7nLhsEJOMkhI5GHGJ5d22NfCEi94pImVtmqgIGuJ/f1Wc9DdzgPr8BePYYYjLmIHaRysQst55/NRAQkZ3u4iQgC8gFmoAxwKoOb90KzDzEbuuB1IjXwzrZZv8Qu269/3s4Z/JrVDUsIvsAifisMUBJJ/v5A1AiIlOBQuDVQ8RkzFGxXwAmll0BhHBq8dPcv0LgHZzrAk8CvxCR49yLsae63USfAy4QkatFJF5EckRkmrvPlcBXRCRVRI4Hbu0ihgygFdgNxIvIj3Bq/W1+B/yHiIwVxxQRyQFQ1XKc6wfPAi+1lZSM6SmWAEwsuwn4vapuUdWdbX/Ab3Dq/PcBxTiN7F7gP4E4Vd2Cc1H2Xnf5SmCqu8//BlqACpwSzXNdxPA6zgXlz4HNOL86IktEvwBeAN4AaoD/BVIi1j8NTMbKP6YXiE0IY0z0EpGzcEpBBaoa9joeE1vsF4AxUUpEEoC7gd9Z4296gyUAY6KQiBQCVTgXq3/pcTgmRlkJyBhjfMp+ARhjjE/1q/sABg0apAUFBV6HYYwx/cqKFSv2qOrgjsv7VQIoKChg+fLlXodhjDH9iohs7my5lYCMMcanLAEYY4xPWQIwxhif6lfXADoTDAYpLy+nqanJ61B6VXJyMnl5eSQkJHgdijEmRnQrAYjIbOB/gADOXYkPdVifjzNmSZa7zX2quti9k/F3wHT3s55R1Z+579kE1OIM1tWqqkVH8wXKy8vJyMigoKAAEen6Df2QqlJZWUl5eTmjRo3yOhxjTIzosgQkIgHgEeAinFEV54jIxA6b3Y8z4caJOBNX/NZdfhWQpKqTcWZeus2dn7XNuao67Wgbf4CmpiZycnJitvEHEBFycnJi/leOMaZvdecawExgvapuVNUWYAFweYdtlPYhbgcA2yOWp7mTY6fgjKJYc8xRdxDLjX8bP3xHY0zf6k4JaDgHDl9bjjPfaaQHgDdE5C4gDbjAXf5HnGSxA2cSjXtUda+7Tt33KPD/VPXxzj5cROYCcwHy8/O7Ea4xxvQPTcEQu2qaqWkK0hgM0dgSOuCxoSVEk/v627PGERfXsyeC3UkAnX1ixwGE5gBPqerDInIq8KyITML59RACjgMGAu+IyBJV3QicrqrbRWQI8KaIrFXVfxz0QU5ieBygqKgo6gYuqqqqYt68edx+++1H9L6LL76YefPmkZWV1UuRGWO8EA4rtU2tVNY3s7e+hT11LeyqbaKipomKmmb30Xle3Rjs1j4DccLt544hNbFn++10Z2/lOPOWtsmjvcTT5lZgNoCqfiAiyThznl4H/FVVg8AuEXkPKAI2qup2d/tdIvIKTrI4KAFEu6qqKn77298elABCoRCBQOCQ71u8eHFvh2aMOUpNwRCry6sp21FDUzBEMBQmGFJaw2FaQ0owpARDYVrDYWqaWtlb18Le+hYq61vY19BCKHzwuWp8nDAkI4khmcmMGpTGKaNzGJqZzJCMJAakJJCSGCA1MUByQoDUxHhSEgLOX2KAhID0Shm4OwlgGTBWREYB23Au8l7XYZstOHOePuUOY5uMMwXeFuA8EfkDTgnoFOCXIpKGM/NSrfv8QuDBnvhCfe2+++5jw4YNTJs2jYSEBNLT08nNzWXlypWUlpZyxRVXsHXrVpqamrj77ruZO3cu0D6sRV1dHRdddBFnnHEG77//PsOHD+dPf/oTKSkpXXyyMaanbK9q5JMt+1ixeR+fbN7Hmu01tHbSiAfihPg4ISEQR3xAiI+LIyM5nuy0RPJzUjkxP4vstESy0xLJSU8kOy2JnLREhmYmk5OW2OMlnGPVZQJQ1VYRuRNnarsA8KSqrhGRB4HlqroQZ+q8J0TkHpzy0M2qqiLyCPB7nAmvBWd6vtUiMhp4xc1o8cA8Vf3rsX6ZHy9aQ+n2nr3GPPG4TP79yycccv1DDz1ESUkJK1euZOnSpVxyySWUlJTs76755JNPkp2dTWNjIyeddBJf/epXycnJOWAf69atY/78+TzxxBNcffXVvPTSS9xwww09+j2MMdDQ0sqmPQ1srqzni8p61myr4ZMt+9hR7fSwS06IY2peFv9y1mhm5A9kSt4A0pLiiQ8ICXFxUdeAH6tuFZRUdTGwuMOyH0U8LwVO7+R9dThdQTsu30j7HKsxZebMmQf01f/Vr37FK6+8AsDWrVtZt27dQQlg1KhRTJvmzDk+Y8YMNm3a1GfxGtOftLSG2VndRPm+BmqaDl8/D4aU8n2NbNrjNPabK+upqGk+YJvhWSkUFWQzIz+LGSOzmZCbQULAPwMk9Ps7gSMd7ky9r6Slpe1/vnTpUpYsWcIHH3xAamoq55xzTqd9+ZOSkvY/DwQCNDY29kmsxnhJVWluDdMQ2fPFfV7f3MqO6ia2VTVQvq+Rbfsa2VbVyM6aJo50DqtB6UkU5KRy5tjBjBqUxsicVApy0igYlEZ6Ukw1gUfM39++B2RkZFBbW9vpuurqagYOHEhqaipr167lww8/7OPojPHertomynbUUrajZv9f+b5GGoOhLhvzQJyQOyCZ4VkpnDZmEMMHppA3MIW8rBSyUhM53HXROBGOy0omI9mGTzkUSwDHKCcnh9NPP51JkyaRkpLC0KFD96+bPXs2jz32GFOmTGH8+PGccsopHkZqTM8Kh5Xa5lZqGoPUNAWpaWylpilIdWOQDbvqKN1RQ9mOWvbUtZddjhuQTGFuJmeOHUxaYoCUxHhSEuJI2f+8vSfMsAHJDM1IIt5HJZm+1q/mBC4qKtKOE8KUlZVRWFjoUUR9y0/f1USHltYw63fVUbajxm3Qa9hc2UBNY5C6ltZDnsEnBuIYOzSdwtxM9y+DibmZZKUm9u0XMACIyIrOhtyxXwDGdEM4rDS1OjXqtrszI2vXwVC4y320dRtMCMSREBDi3ceEQNz+roVt3QsT4txHd5uj7QMeDitVjUH21jdTWddCTVMrhzvpCyts3duwv8HfsLuOYMjZPik+jvHDMpg5Kpus1AQykxPISI4nM8V5npkS7zwmJ5Cbleyri6n9lSUAYw5hw+46nvtwC39auY3K+hZPYwnECQkBOeDmoJTEAKkJ8SQnBkh1lzUFQ87NSPXOjUn7GlropDt7l4ZmJlGYm8m5E4ZQmJvJxNwMCnLSrBwTYywBGBMhGArzxpoKnvtoM+9vqCQhIFw4cRjHD0l3Gt22xtetVbffqdl1w9gaVlpDYVpCzt2kreEwLa3O4/47TUNtr51tI+9ADYb0oLFiGltCVDcGqahuoiHYSmIgjpy0JMYMTuekUYnkuDclZaclkpPm3HHa1Y+J3AHJ5KQnHX4jExMsARgDbKtqZMHHW1iwbCu7a5sZnpXCd780nquLRjA4wxpDE5ssAZiYo6rUNLZSXtXA9qomGlpa28+ow0qwNRxxlq0Ub6virbW7UODc8UO44ZR8zh43hECM3fVpTEeWAEyf+mJPPUtKK3iztIKVW6sQ4YBxVRLdi6NtF0JTkwLuBcYEMpPjyYi42JiRHE8gTthe5dwoVO7eLLRtXyO1za3djmlQehLfOHsMc2bmMyI7tRe/vTHRxRJAD3vggQdIT0/nO9/5jtehRIVwWPl0axVvllawpKyC9bvqACjMzeTGU0cSHxCCrZ3Uvd0z9YaWEFUNLWzZ27C/v3lbr5RIGcnxDM9KIW9gKqeMznGfp3BcVgrpyfEH9aqJj+h5Y2f6xq8sAZhj0tnNQLVNQaoag6zYtI+/ra1gT10L8XHCyaOzueHkfM4vHHrUZ9qqSlMwTG2T83mtYSV3QAoDUuxuT2OOlCWAHvDTn/6UZ555hhEjRjB48GBmzJjBhg0buOOOO9i9ezepqak88cQT5ObmMnXqVDZu3EhcXBwNDQ2MHz+ejRs3kpDQPxqw+uZWnnp/Ey8s38reupbD3gyUkRTPOROGcEHhEM4ZP6RHGmkR2d8Fckhm8jHvzxg/i60E8Jf7YGdxz+5z2GS46KFDrl6xYgULFizg008/pbW1lenTpzNjxgzmzp3LY489xtixY/noo4+4/fbbeeutt5g6dSpvv/025557LosWLeJLX/pSv2j8G1tCPPvhJh57eyN761s4c+wgzpswxKnJR94M5D7PSI4nd0AKifHWb9yYaBVbCcAD77zzDldeeSWpqU5J47LLLqOpqYn333+fq65qHwm7udkZD+Waa67h+eef59xzz2XBggVHPJVkX2sKhpj30RZ+u3QDe+qaOXPsIL49axwn5g/0OjRjzDGKrQRwmDP13tTxNv1wOExWVhYrV648aNvLLruM73//++zdu5cVK1Zw3nnn9VWYR6SlNczzy7fyyFvr2VnTxCmjs3n0humcVJDtdWjGmB5iv8+P0VlnncUrr7xCY2MjtbW1LFq0iNTUVEaNGsWLL74IOBcuV61aBUB6ejozZ87k7rvv5tJLLz3svMFeebO0gnN/vpQfvlpC3sAU5v3LySyYe6o1/sbEmNj6BeCB6dOnc8011zBt2jRGjhzJmWeeCcBzzz3HN7/5TX7yk58QDAa59tprmTrVmQTtmmuu4aqrrmLp0qUeRt654vJq7njuE0YPTuPpW2Zy1thBvTIZtTHGezYcdD/S29+1uiHIJb9+h3BYee1fz2Rgmg3da0wssOGgzWGFw8q9L66koqaJ52871Rp/Y3zArgEYAB5/ZyNLynbxg4sLmW49fIzxhZhIAP2pjHW0evM7frSxkv96/TMumZzLzacV9NrnGGOiS79PAMnJyVRWVsZ0ElBVKisrSU7u+Ttfd9c2c9f8T8nPTuWhr062C77G+Ei/vwaQl5dHeXk5u3fv9jqUXpWcnExeXl6P7jMUVv51/qfUNAV55taZZCRH/x3Jxpie0+8TQEJCAqNGjfI6jH7pv9/8nA82VvJfX5vChGGZXodjjOlj/b4EZI7O39fu4jd/X881RSO4qmiE1+EYYzxgCcCHyvc1cM8LKynMzeTHl5/gdTjGGI/0+xKQ6T5VpWRbDfe/WkwopPz2+ukkJ0TfUBTGmL5hCSDGNbeG+GBDJUvKKlhSuoudNU3Exwm/uW46owaleR2eMcZDlgBiUFVDC2+t3cWSsgre/mw39S0hUhMDnDV2MLMmDuXcCUPItjt9jfE9SwAxYnNlPW+6k60v37yPUFgZkpHE5ScOZ1bhUE4dk2PlHmPMASwB9FPhsLKqvH2y9c8rnMnWxw/N4Btnj2bWxGFMGT6AOJvw3BhzCJYA+hFV5Z11e/hLyQ6WlO1id20zgThhZkE2P7w0n1mFQ8nPObrJ1o0x/tOtBCAis4H/AQLA71T1oQ7r84GngSx3m/tUdbGIJAC/A6a7n/WMqv6sO/s0B9pd28wPXinmzdIK0pPiOXucU88/Z/xgslKtnm+MOXJdJgARCQCPALOAcmCZiCxU1dKIze4HXlDVR0VkIrAYKACuApJUdbKIpAKlIjIf2NqNfRrXn1dv54evllDfEuIHF0/gptMKSIq3er4x5th05xfATGC9qm4EEJEFwOVAZGOtQNtYAgOA7RHL00QkHkgBWoCabu7T9/bWt/DDV0t4rXgHU/MG8PDVUzl+SIbXYRljYkR3EsBwnDP2NuXAyR22eQB4Q0TuAtKAC9zlf8Rp2HcAqcA9qrpXRLqzTwBEZC4wFyA/P78b4caGv5bs5P5Xi6luDPLdL43ntrNGEx+wG7eNMT2nOwmgs24kHcdengM8paoPi8ipwLMiMgnnTD8EHAcMBN4RkSXd3KezUPVx4HFwpoTsRrz9WlVDCw8sXMOrK7dzwnGZ/OHrJ9tAbcaYXtGdBFAORI4Wlkd7iafNrcBsAFX9QESSgUHAdcBfVTUI7BKR94AinLP/rvbpO29/vpvvvriKvfUt3H3+WO4873gS7KzfGNNLutO6LAPGisgoEUkErgUWdthmC3A+gIgUAsnAbnf5eeJIA04B1nZzn74RDIX52eIybnryY7JSE3j1jtO5Z9Y4a/yNMb2qy18AqtoqIncCr+N02XxSVdeIyIPAclVdCNwLPCEi9+CUcm5WVRWRR4DfAyU4ZZ/fq+pqgM722QvfL+pt3dvAXfM/ZeXWKq4/OZ8fXjrR7tg1xvQJ6U9TKRYVFeny5cu9DqPHvLZ6B/e9tBqAh746hUum5HockTEmFonIClUt6rjc7gT2QFMwxIN/LmXeR1uYOiKL38w5kRHZdgevMaZvWQLoY+sqarlz3qd8VlHLbWeP5jsXjrdavzHGE5YA+oiq8uLycn60sIS0xHie+ueTOGf8EK/DMsb4mCWAPlDbFOTfXilh4artnDYmh19eM40hmcleh2WM8TlLAL1sdXkVd83/lK17G/jOheP45jnHE7Ahmo0xUcASQC9RVZ58bxMP/aWMQelJPH/bqZxUkO11WMaY/iLYCLtKYWcJ7N0Asx7s8Y+wBNAL9ta38N0XV/G3tbuYNXEo//W1KTZkszGmc6pQuxMqSmBncftj5XrQsLNNQhqc/i1I7dmTSEsAPezDjZXcveBT9tUHeeDLE7nptAJErORjjAFaW2DP5wc29BUl0FDZvs2AfBg2CSZe4TwOnQQDR0Fcz/cWtATQQ0Jh5ddvreNXf1vHyJw0/vemk5g0fIDXYRljvFJfCRXFTgmnosR53L0WwkFnfSAJhhTC+Itg6GS3sT8BUgb2WYiWAHrAlsoG7n1xJcs27ePKE4fzH1dMIj3JDq0xvhAOQeWG9sa+7ay+dkf7NunDnAb++PNh2GTnrD7neAh4205YK3UM2vr2/3jRGuJE+MXVU/nK9DyvwzLG9JamaqhY457Vuw3+rjJobXTWx8XD4Akw6uz28s2wyZA2yNu4D8ESwFHaU9fM91925ug9ZXQ2D189jeFZKV6HZYzpCeEwVG06sHxTUQxVW9q3Scl2GvmiW9ob+8HjIT7Js7CPlCWAo7CktIL7Xl5NTWMr919SyC2njyLO+vYb0z+11Dtn8fsvzJY4Z/kttc56iYPsMTB8Bky/CYZNcRr8jFzo5x08LAEcgfrmVn7yWinzP95KYW4mz319GuOH2Ry9xvQLqlCz7cDyTUWJU79vm5AwKdO5EDv1WvesfrJzoTYxNgdrtATQTZ9s2ce3Fqxk674GvnH2GO6ZNZakeBu335io1Nrs9LjZX8Jxz+4b97VvkzXSqc9P+przOGySs6yfn9UfCUsA3bB1bwPXP/EROemJPD/3VGaOsjt6jYkadbs7dLcsdvrah1ud9fEpMHQiFF7W3gNn6AmQbHNtWwLogqryg1eKiRN4/rZT7UKvMV4JtULluoNLOHUV7dtkHOecyY+b3V7CyRkDcfZrvTOWALrw8ifbeGfdHh68/ARr/I3pK41VB/a+aetuGWp21sclON0tx5zX3tVy6CRIy/E27n7GEsBh7Klr5j9eK6Vo5EBuOHmk1+EYE3vCYdj3RYceOCVQvbV9m9RBztn8zH9pb+gHjYN4G1/rWFkCOIwHFq6hoTnEQ1+dbN08jekJrS2w7g3Y8Df3rL4UWuqcdRIHOWNhxEy3b/1k5y99qK8uzPYlSwCHsKS0gj+v3sG3Z43j+CHW1dOYo6YK2z+BlfOh5CVo3Ot2t5wE065rP6sfUggJVmbtS5YAOlHbFOT+V0sYPzSDb5w9xutwjOmfqsth9fOwaoHTKyeQBBMucRr90ed6Pg6OsQTQqf/861p21Tbx2I0zSIy3CduN6Za2vvfbP4WSl+GLfwAK+afBl++EiZdDSpbXUZoIlgA6+PiLvfzhwy3cesYopo2wf6zGdKpu18EXbiP73g8sgHPugynXQPYoT0M1h2YJIEJTMMR9L60mb2AK9144zutwjPFeKAh71nWYraoE6ne1b5M53Knhj7+ovUtmzvF24bYfsAQQ4TdvrWfjnnqeuWUmqYl2aIzPNO47cPTLnaudkk6oxVkfSHRGuzz+gvahE4ZO6vFpCk3fsVbOVbajhsfe3sBXp+dx1rjBXodjTO/Z3/d+9YENfk15+zZpg53G/eTb2merGjQOAgnexW16nCUAoDUU5nsvrSYrNYEfXlrodTjG9JzmOmdo48ihEypKIVjvrJeA07CPPNUt37jDJ2QM9TZu0ycsAQCLS3ayuryaX885kaxUu7vQ9FN1u6F82YH1+r1fsH+o4+QBTuM+/cb2xn5wISQkexq28Y4lAGDFpr2kJQa4ZHKu16EYc2SCjbD2NVg1Hza8BRoGxOl5M2wyTL2uvVY/IM8uzJoDWAIAVm+r5oThA2y4B9M/hMOw5QOn0S/9EzTXQGYenHEPjP2SM9RxUrrXUZp+wPcJoDUUpnR7DTeeYoO9mShXuaH9ztqqzZCY7txcNfVaGHkGxNlNi+bI+D4BrNtVR3NrmMl5A7wOxRjHAX3vI3rq1O8GBEafA+f+GxReColpHgdr+rNuJQARmQ38DxAAfqeqD3VYnw88DWS529ynqotF5HrguxGbTgGmq+pKEVkK5AKN7roLVTXi7pK+UVxeDcDk4ZYAjAca9h54N+3O4k763k+AsRdC7lSYcCkMGO5tzCZmdJkARCQAPALMAsqBZSKyUFVLIza7H3hBVR8VkYnAYqBAVZ8DnnP3Mxn4k6qujHjf9aq6vIe+y1FZva2KjKR4CnLsTMr0onAY9m7sMHVhJ33vh02G0d+IGPd+rPW9N72mO78AZgLrVXUjgIgsAC4HIhOAAm0TbA4AtneynznA/KMPtXcUb6thkl0ANj2pudbpa19R7JzRt417H2xw1lvfexMlupMAhgMR0/NQDpzcYZsHgDdE5C4gDbigk/1cg5M4Iv1eRELAS8BPVFU7vklE5gJzAfLz87sRbve1tIYp21HDzacV9Oh+jU+oQtWWg6cu3PdF+zbJWc7Z/PSb2rtjDp5gfe9NVOhOAujs1LhjQz0HeEpVHxaRU4FnRWSSqoYBRORkoEFVSyLec72qbhORDJwEcCPwzEEfpPo48DhAUVHRQQniWHxeUUtLa9jq/6ZrwUZnTtoDRsBcA83V7gYC2aMhdwpMu95p7IdNdgZKs773Jkp1JwGUAyMiXudxcInnVmA2gKp+ICLJwCCg7aLutXQo/6jqNvexVkTm4ZSaDkoAval4m/Ofd4r1ADJtVKF258GjX1auc2+yAhLSnAZ+8tfayzdDCq3vvel3upMAlgFjRWQUsA2nMb+uwzZbgPOBp0SkEEgGdgOISBxwFXBW28YiEg9kqeoeEUkALgWWHON3OWLF26rJTI4nPzu1rz/aRIuaHfDF2wc29g172tcPyHca+YmXt5dwBo6yPvcmJnSZAFS1VUTuBF7H6eL5pKquEZEHgeWquhC4F3hCRO7BKQ/dHFHPPwsob7uI7EoCXncb/wBO4/9Ej32rbiour2Zy3gDEfqL7S0sDrP2zcyftxqXOmX0gCYZOdMa0b+uBM/QEm8HKxLRu3QegqotxunZGLvtRxPNS4PRDvHcpcEqHZfXAjCOMtUc1t4ZYu7OGW88Y7WUYpq+Ew7D5Pecu2tJXoaXOObs/8zvO2f3gCTZHrfEd3/6L/3xnHcGQWv0/loVDsPszWPMyrHoeqrdAYgaccAVMnePMVWulHONjvk0Aq7dVAXYHcMxoqnHHvY+4eLurzOl7L3Ew+lw4/0cw4RJItGs+xoCPE0BxeTVZqQnkDUzxOhRzJFSdgdAih06oKIF9m9q3SRno1PBn3Ow8jjkPMm2ob2M68m0CWF1ezeThdgE4qgWbYNeaDo39Gmf4YwAEcsZA7jQ48QZ36sLJkHmc9b03pht8mQCagiE+r6jltgl2ATgqtTbDx4/DP34OTU6pjsR0p1fOlKvd4RPcvvc2GqYxR82XCWDtzlpaw2r1/2ijCiUvwd8edMo8Y86HGTc5jX1WgV2wNaaH+TIBFJe7F4DzrI931Nj0HrxxP2z/xCnl3PiKU7s3xvQaXyaA1eXV5KQlctwAG5DLc3vWwZv/Dp+9BhnHwRWPwpRrIC7gdWTGxDxfJoDibXYHsOf2boQPHoHlv4eEVDjvh3DK7dZF05g+5LsE0NgSYt2uOmZNtLHX+1xjFax5xbkbd+uHzrj4RbfA2d+D9MFeR2eM7/guAZTuqCFkF4D7TigIG95yxt1ZuxhCzTBoPFzwAEy+2qY3NMZDvksAJfuHgLYLwL2mudbpu1+2CIpfcCYzT8l2bsyaei0cd6L10zcmCvguAawur2ZQehJDM5O8DqX/63RGrOL2u3LjEmD8bJh6HRx/AcQnehquMeZAvksAxduqmGIXgI9e3W6nr37ZIqexP2hGrKkw7cZA1JEAAA+kSURBVAZn7PwRJ0NqtqfhGmMOzVcJoKGllfW76rhoko0Lc0SCTfD5X52Lt+vfhHCr01d/8lfdu3Kn2IxYxvRDvkoApdtrCKtNAdktqlC+zLl4W/ISNFVDRi6cegdMudaZPMUY06/5KgGsLnfKFdYDqBOhIOz53Knl71wNny12+urHp0Dhl52Lt6PPsRu0jIkhvkoAxduqGZqZxJBMn98B3LD3wDlwK4qdiVNCLc76QBKMmAln3uvMlpWU4W28xphe4bsEMHm4j7p/hkNQucHtnVPS3uDXbm/fJn1o+5j5bcMp5xxv0yMa4wO++V9e19zKht11XDb1OK9D6T3BRlj7GnzxD6exryiF1kZnnQRg8HgoOMPpodM2pHL6EG9jNsZ4xjcJYM22alRjsP4fDjvDKqyaD2tedSZLSc5yGveif3Yb+knOpOfxdu+DMaadbxJAsXsH8KRYSQCVG2D1807XzKrNkJDm1OunXgsFZ9rY+caYLvkqARw3IJnBGf34LLjjYGqI0zPn3H+DwkttdixjzBHxTwIor+6fZ/9tg6mtnAef/cUGUzPG9BhfJICapiAb99Tzlen9pLFUdfrir1oAxS86g6ml5thgasaYHuWLBNA2AmjUTwHZWAWfPONc0N1VCoFEGDcbps6xwdSMMT3OXwkgmktATTXw9JedM/+8k+CSX8AJV9pgasaYXuOLBLC6vJrhWSlkp0XpGXRrMzx/vXPWf92LMO5CryMyxviALxJA8bbq6B0ALhyCl+c6N29d+bg1/saYPuOLBPDsLSfTEgp7HcbBVOEv34PSV+HCn8DUa7yOyBjjI75IAPk5qV6H0Ll3fg7LnoDT7nL+jDGmD9ntol5Z8TS89RNnbP0LHvQ6GmOMD1kC8MLa1+DP34LjZ8Hlv7FhG4wxnuhWyyMis0XkMxFZLyL3dbI+X0T+LiKfishqEbnYXX69iKyM+AuLyDR33QwRKXb3+SvxyyS9m9+HP97i3Mx19dMQSPA6ImOMT3WZAEQkADwCXARMBOaISMf5AO8HXlDVE4Frgd8CqOpzqjpNVacBNwKbVHWl+55HgbnAWPdvdg98n+hWsQbmXwsDRjjdPW3sHmOMh7rzC2AmsF5VN6pqC7AAuLzDNgpkus8HANs52BxgPoCI5AKZqvqBqirwDHDFUcTff9RWwB++BgmpcOPLkJbjdUTGGJ/rTi+g4cDWiNflwMkdtnkAeENE7gLSgAs62c81tCeO4e5+IvfZ6UA9IjIX55cC+fn53Qg3CoVanbJP4z74+puQ1U+/hzEmpnTnF0BntXnt8HoO8JSq5gEXA8+KyP59i8jJQIOqlhzBPp2Fqo+rapGqFg0ePLgb4Uahvz0Am9+FL//SmajFGGOiQHcSQDkwIuJ1HgeXeG4FXgBQ1Q+AZGBQxPprccs/EfvM62KfsaF0Ibz/ayi61RnJ0xhjokR3EsAyYKyIjBKRRJzGfGGHbbYA5wOISCFOAtjtvo4DrsK5dgCAqu4AakXkFLf3zz8BfzrG7xJ99qyDV2+H4TNg9s+8jsYYYw7QZQJQ1VbgTuB1oAynt88aEXlQRC5zN7sX+BcRWYVzpn+ze3EX4CygXFU3dtj1N4HfAeuBDcBfjvnbRJOWenj+Rqeb51VP23y8xpioI+3tdPQrKirS5cuXex1G11Thpa9DyUtOj58x53kdkTHGx0RkhaoWdVxut6D2ho+fgJI/wnn/Zo2/MSZqWQLoaVs/htd/4Mzkdca9XkdjjDGHZAmgJ9XthhduciZqv/IxG+PHGBPVfDEcdJ8ItcIf/xka98Ktb0LKQK8jMsaYw7IEcDSCjbCrDCpKYGeJ81hRAk3VcPkjkDvF6wiNMaZLlgC6q/iP8NlfnIZ+zzrQkLM8IQ2GToQTvgKjzoJJX/E2TmOM6SZLAN3R0gCvfANSsmB4ERR+GYZOcoZ1GDjKav3GmH7JEkB3lH8M4SBc8SiMneV1NMYY0yPs1LU7Nr0HEgcjOg6Caowx/ZclgO7Y/B7kToXkzK63NcaYfsISQFeCTVC+HEae7nUkxhjToywBdKV8GYSaoeBMryMxxpgeZQmgK5vfAwTyT/E6EmOM6VGWALqy6V2nu2dKlteRGGNMj7IEcDitzU4JqOAMryMxxpgeZwngcLZ9Aq1NdgHYGBOTLAEczqZ3AYGRp3kdiTHG9DhLAIez+V0YegKkZnsdiTHG9DhLAIcSCjqTu1j5xxgToywBHMr2TyHYAAWWAIwxsckSwKFsetd5tF8AxpgYZQngUDa9C4MLIW2Q15EYY0yvsATQmVArbP3Iyj/GmJhmCaAzO1ZBS52Vf4wxMc0SQGc2u/V/uwPYGBPDLAF0ZtO7MGgcpA/xOhJjjOk1lgA6Codgy4dW/jHGxDxLAB3tXA3NNVb+McbEPEsAHW16z3m0XwDGmBhnCaCjze9B9hjIzPU6EmOM6VWWACKFQ04CsP7/xhgfsAQQqWINNFXDSKv/G2NinyWASJvd+r/9AjDG+EC3EoCIzBaRz0RkvYjc18n6fBH5u4h8KiKrReTiiHVTROQDEVkjIsUikuwuX+ruc6X7532n+03vwsACGJDndSTGGNPr4rvaQEQCwCPALKAcWCYiC1W1NGKz+4EXVPVREZkILAYKRCQe+ANwo6quEpEcIBjxvutVdXlPfZljEg7D5vdh/MVdb2uMMTGgO78AZgLrVXWjqrYAC4DLO2yjQKb7fACw3X1+IbBaVVcBqGqlqoaOPexesLsMGvda+ccY4xvdSQDDga0Rr8vdZZEeAG4QkXKcs/+73OXjABWR10XkExH5Px3e93u3/PNDEZEjD78HWf9/Y4zPdCcBdNYwa4fXc4CnVDUPuBh4VkTicEpMZwDXu49Xisj57nuuV9XJwJnu342dfrjIXBFZLiLLd+/e3Y1wj9Lmd2FAPgwc2XufYYwxUaQ7CaAcGBHxOo/2Ek+bW4EXAFT1AyAZGOS+921V3aOqDTi/Dqa7221zH2uBeTilpoOo6uOqWqSqRYMHD+7u9zoyqk7938o/xhgf6U4CWAaMFZFRIpIIXAss7LDNFuB8ABEpxEkAu4HXgSkikupeED4bKBWReBEZ5G6fAFwKlPTEFzoqez6H+t1W/jHG+EqXvYBUtVVE7sRpzAPAk6q6RkQeBJar6kLgXuAJEbkHpzx0s6oqsE9EfoGTRBRYrKqviUga8Lrb+AeAJcATvfEFu2XD351HGwDOGOMj4rTT/UNRUZEuX94LvUafuhTq98AdH/b8vo0xxmMiskJVizoutzuB6yudO4ALv+x1JMYY06csAXy2GDRsCcAY4zuWAMoWQdZIGDbZ60iMMaZP+TsBNNXAxr87Z/8e34dmjDF9zd8JYN0bEGqx8o8xxpf8nQDKFkH6UMjr9B40Y4yJaf5NAMFGWPcmTLgE4vx7GIwx/uXflm/jUgjWW/nHGONb/k0AZYsgeQAUnOl1JMYY4wl/JoBQ0On/P/5iCCR4HY0xxnjCnwlg83vQuA8mXOp1JMYY4xl/JoCyRZCQCmPO8zoSY4zxjP8SQDgMZX+G4y+AxFSvozHGGM/4LwFsWwF1O6HwMq8jMcYYT/kvAZQthLgEGHeh15EYY4yn/JUAVJ36/+hznC6gxhjjY/5KABVrYN8XUGi9f4wxxl8JoGwRIDD+Eq8jMcYYz/kvAYw8DdIHex2JMcZ4zj8JoHID7FpjY/8YY4zLPwlg7Z+dR7v71xhjAD8lgLJFcNyJkDXC60iMMSYq+CMB1GyH8mV29m+MMRH8kQDWvuY82t2/xhiznz8SQNlCGDQeBo/zOhJjjIka8V4H0OtUYdgUGDfc60iMMSaqxH4CEIEv/dTrKIwxJur4owRkjDHmIJYAjDHGpywBGGOMT1kCMMYYn7IEYIwxPmUJwBhjfMoSgDHG+JQlAGOM8SlRVa9j6DYR2Q1sPsq3DwL29GA4vcXi7Hn9JVaLs+f1l1h7O86RqnrQTFj9KgEcCxFZrqpFXsfRFYuz5/WXWC3OntdfYvUqTisBGWOMT1kCMMYYn/JTAnjc6wC6yeLsef0lVouz5/WXWD2J0zfXAIwxxhzIT78AjDHGRLAEYIwxPhXzCUBEZovIZyKyXkTu8zqewxGRTSJSLCIrRWS51/G0EZEnRWSXiJRELMsWkTdFZJ37ONDLGN2YOovzARHZ5h7TlSJysZcxujGNEJG/i0iZiKwRkbvd5dF4TA8Va1QdVxFJFpGPRWSVG+eP3eWjROQj95g+LyKJURrnUyLyRcTxnNYn8cTyNQARCQCfA7OAcmAZMEdVSz0N7BBEZBNQpKpRdeOKiJwF1AHPqOokd9n/Bfaq6kNuYh2oqt+LwjgfAOpU9edexhZJRHKBXFX9REQygBXAFcDNRN8xPVSsVxNFx1VEBEhT1ToRSQDeBe4Gvg28rKoLROQxYJWqPhqFcX4D+LOq/rEv44n1XwAzgfWqulFVW4AFwOUex9TvqOo/gL0dFl8OPO0+fxqnUfDUIeKMOqq6Q1U/cZ/XAmXAcKLzmB4q1qiijjr3ZYL7p8B5QFuj6vkxPUycnoj1BDAc2Brxupwo/McbQYE3RGSFiMz1OpguDFXVHeA0EsAQj+M5nDtFZLVbIvK8rBJJRAqAE4GPiPJj2iFWiLLjKiIBEVkJ7ALeBDYAVara6m4SFf//O8apqm3H86fu8fxvEUnqi1hiPQFIJ8uiueZ1uqpOBy4C7nBLGubYPAqMAaYBO4CHvQ2nnYikAy8B31LVGq/jOZxOYo2646qqIVWdBuTh/Pov7Gyzvo2qkwA6xCkik4DvAxOAk4BsoE9Kf7GeAMqBERGv84DtHsXSJVXd7j7uAl7B+UccrSrc+nBbnXiXx/F0SlUr3P9wYeAJouSYuvXfl4DnVPVld3FUHtPOYo3W4wqgqlXAUuAUIEtE4t1VUfX/PyLO2W6pTVW1Gfg9fXQ8Yz0BLAPGuj0BEoFrgYUex9QpEUlzL7IhImnAhUDJ4d/lqYXATe7zm4A/eRjLIbU1qK4riYJj6l4I/F+gTFV/EbEq6o7poWKNtuMqIoNFJMt9ngJcgHO94u/A19zNPD+mh4hzbUTiF5zrFH1yPGO6FxCA2z3tl0AAeFJVf+pxSJ0SkdE4Z/0A8cC8aIlVROYD5+AMWVsB/DvwKvACkA9sAa5SVU8vwB4iznNwyhQKbAJua6uze0VEzgDeAYqBsLv4Bzi19Wg7poeKdQ5RdFxFZArORd4AzontC6r6oPv/agFOWeVT4Ab3LDva4nwLGIxTtl4JfCPiYnHvxRPrCcAYY0znYr0EZIwx5hAsARhjjE9ZAjDGGJ+yBGCMMT5lCcAYY3zKEoAxxviUJQBjjPGp/w/YMjpLoj3gCQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Loss curve\n",
    "plt.plot(train_loss)\n",
    "plt.plot(dev_loss)\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.savefig('./../fig/loss.png')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy curve\n",
    "plt.plot(train_acc)\n",
    "plt.plot(dev_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.savefig('./../fig/acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzIcYAfvkUZ_"
   },
   "source": [
    "### Predicting testing labels\n",
    "\n",
    "Predictions are saved to *output_logistic.csv*.\n",
    "\n",
    "預測測試集的資料標籤並且存在 *output_logistic.csv* 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ZEAKhugPkUyH",
    "outputId": "97c3eb12-a9c5-4c43-bc62-54f7f5f4797d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unemployed full-time 0.835982593576664\n",
      "capital losses 0.6137164904353742\n",
      " 1 0.5063627375046139\n",
      "id 0.4470052253367102\n",
      " Yes 0.35539739745685195\n",
      " Some college but no degree 0.30177353658305733\n",
      " All other 0.2577995720154868\n",
      " Male -0.2577995720154867\n",
      " 5th or 6th grade 0.22515586325499992\n",
      " 11th grade 0.2247349599815066\n"
     ]
    }
   ],
   "source": [
    "# Predict testing labels\n",
    "predictions = _predict(X_test, w, b)\n",
    "with open(output_fpath.format('logistic'), 'w') as f:\n",
    "    f.write('id,label\\n')\n",
    "    for i, label in  enumerate(predictions):\n",
    "        f.write('{},{}\\n'.format(i, label))\n",
    "\n",
    "# Print out the most significant weights\n",
    "ind = np.argsort(np.abs(w))[::-1]\n",
    "with open(X_test_fpath) as f:\n",
    "    content = f.readline().strip('\\n').split(',')\n",
    "features = np.array(content)\n",
    "for i in ind[0:10]:\n",
    "    print(features[i], w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
