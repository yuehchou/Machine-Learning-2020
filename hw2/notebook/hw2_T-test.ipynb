{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OSL2CMEzmQvB"
   },
   "source": [
    "# **Homework 2 - Classification**\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ox7joE3aZkh-"
   },
   "source": [
    "Binary classification is one of the most fundamental problem in machine learning. In this tutorial, you are going to build linear binary classifiers to predict whether the income of an indivisual exceeds 50,000 or not. We presented a discriminative and a generative approaches, the logistic regression(LR) and the linear discriminant anaysis(LDA). You are encouraged to compare the differences between the two, or explore more methodologies. Although you can finish this tutorial by simpliy copying and pasting the codes, we strongly recommend you to understand the mathematical formulation first to get more insight into the two algorithms. Please find [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) and [here](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf) for more detailed information about the two algorithms.\n",
    "\n",
    "二元分類是機器學習中最基礎的問題之一，在這份教學中，你將學會如何實作一個線性二元分類器，來根據人們的個人資料，判斷其年收入是否高於 50,000 美元。我們將以兩種方法: logistic regression 與 generative model，來達成以上目的，你可以嘗試了解、分析兩者的設計理念及差別。針對這兩個演算法的理論基礎，可以參考李宏毅老師的教學投影片 [logistic regression](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf) 與 [generative model](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Classification%20(v3).pdf)。\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkNW5cQmohoo"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "This dataset is obtained by removing unnecessary attributes and balancing the ratio between positively and negatively labeled data in the [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)), which can be found in [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php). Only preprocessed and one-hot encoded data (i.e. *X_train*,  *Y_train* and *X_test*) will be used in this tutorial. Raw data (i.e. *train.csv* and *test.csv*) are provided to you in case you are interested in it.\n",
    "\n",
    "這個資料集是由 [**UCI Machine Learning Repository**](https://archive.ics.uci.edu/ml/index.php) 的 [**Census-Income (KDD) Data Set**](https://archive.ics.uci.edu/ml/datasets/Census-Income+(KDD)) 經過一些處理而得來。為了方便訓練，我們移除了一些不必要的資訊，並且稍微平衡了正負兩種標記的比例。事實上在訓練過程中，只有 X_train、Y_train 和 X_test 這三個經過處理的檔案會被使用到，train.csv 和 test.csv 這兩個原始資料檔則可以提供你一些額外的資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRXI0kf0W4Bd"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this section we will introduce logistic regression first. We only present how to implement it here, while mathematical formulation and analysis will be omitted. You can find more theoretical detail in [Prof. Lee's lecture](https://www.youtube.com/watch?v=hSXFuypLukA).\n",
    "\n",
    "首先我們會實作 logistic regression，針對理論細節說明請參考[李宏毅老師的教學影片](https://www.youtube.com/watch?v=hSXFuypLukA)\n",
    "\n",
    "### Preparing Data\n",
    "\n",
    "Load and normalize data, and then split training data into training set and development set.\n",
    "\n",
    "下載資料，並且對每個屬性做正規化，處理過後再將其切分為訓練集與發展集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "7NzAmkzU2MAS",
    "outputId": "61610be3-295e-4ff8-befe-8044938141c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 48830\n",
      "Size of development set: 5426\n",
      "Size of testing set: 27622\n",
      "Dimension of data: 510\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train_fpath = '../data/X_train'\n",
    "Y_train_fpath = '../data/Y_train'\n",
    "X_test_fpath = '../data/X_test'\n",
    "output_fpath = '../results/output_{}.csv'\n",
    "\n",
    "# Parse csv files to numpy array\n",
    "with open(X_train_fpath) as f:\n",
    "    next(f)\n",
    "    X_train = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "with open(Y_train_fpath) as f:\n",
    "    next(f)\n",
    "    Y_train = np.array([line.strip('\\n').split(',')[1] for line in f], dtype = float)\n",
    "with open(X_test_fpath) as f:\n",
    "    next(f)\n",
    "    X_test = np.array([line.strip('\\n').split(',')[1:] for line in f], dtype = float)\n",
    "\n",
    "def _normalize(X, train = True, specified_column = None, X_mean = None, X_std = None):\n",
    "    # This function normalizes specific columns of X.\n",
    "    # The mean and standard variance of training data will be reused when processing testing data.\n",
    "    #\n",
    "    # Arguments:\n",
    "    #     X: data to be processed\n",
    "    #     train: 'True' when processing training data, 'False' for testing data\n",
    "    #     specific_column: indexes of the columns that will be normalized. If 'None', all columns\n",
    "    #         will be normalized.\n",
    "    #     X_mean: mean value of training data, used when train = 'False'\n",
    "    #     X_std: standard deviation of training data, used when train = 'False'\n",
    "    # Outputs:\n",
    "    #     X: normalized data\n",
    "    #     X_mean: computed mean value of training data\n",
    "    #     X_std: computed standard deviation of training data\n",
    "\n",
    "    if specified_column == None:\n",
    "        specified_column = np.arange(X.shape[1])\n",
    "    if train:\n",
    "        X_mean = np.mean(X[:, specified_column] ,0).reshape(1, -1)\n",
    "        X_std  = np.std(X[:, specified_column], 0).reshape(1, -1)\n",
    "\n",
    "    X[:,specified_column] = (X[:, specified_column] - X_mean) / (X_std + 1e-8)\n",
    "     \n",
    "    return X, X_mean, X_std\n",
    "\n",
    "def _train_dev_split(X, Y, dev_ratio = 0.25):\n",
    "    # This function spilts data into training set and development set.\n",
    "    train_size = int(len(X) * (1 - dev_ratio))\n",
    "    return X[:train_size], Y[:train_size], X[train_size:], Y[train_size:]\n",
    "\n",
    "# Normalize training and testing data\n",
    "X_train, X_mean, X_std = _normalize(X_train, train = True)\n",
    "X_test, _, _= _normalize(X_test, train = False, specified_column = None, X_mean = X_mean, X_std = X_std)\n",
    "    \n",
    "# Split data into training set and development set\n",
    "dev_ratio = 0.1\n",
    "X_train, Y_train, X_dev, Y_dev = _train_dev_split(X_train, Y_train, dev_ratio = dev_ratio)\n",
    "\n",
    "train_size = X_train.shape[0]\n",
    "dev_size = X_dev.shape[0]\n",
    "test_size = X_test.shape[0]\n",
    "data_dim = X_train.shape[1]\n",
    "print('Size of training set: {}'.format(train_size))\n",
    "print('Size of development set: {}'.format(dev_size))\n",
    "print('Size of testing set: {}'.format(test_size))\n",
    "print('Dimension of data: {}'.format(data_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38784, 510), (10046, 510))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_0 = X_train[Y_train == 0]\n",
    "X_train_1 = X_train[Y_train == 1]\n",
    "X_train_0.shape, X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py:1892: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= _a)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in less\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "bool_value = stats.ttest_ind(X_train_0, X_train_1)[1] < 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train.T[bool_value]).T\n",
    "X_dev = (X_dev.T[bool_value]).T\n",
    "X_test = (X_test.T[bool_value]).T\n",
    "data_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48830, 380), (5426, 380), (27622, 380))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_dev.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imgCeBDoApdb"
   },
   "source": [
    "\n",
    "### Some Useful Functions\n",
    "\n",
    "Some functions that will be repeatedly used when iteratively updating the parameters.\n",
    "\n",
    "這幾個函數可能會在訓練迴圈中被重複使用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hSDAw5LTAs2o"
   },
   "outputs": [],
   "source": [
    "def _shuffle(X, Y):\n",
    "    # This function shuffles two equal-length list/array, X and Y, together.\n",
    "    randomize = np.arange(len(X))\n",
    "    np.random.shuffle(randomize)\n",
    "    return (X[randomize], Y[randomize])\n",
    "\n",
    "def _sigmoid(z):\n",
    "    # Sigmoid function can be used to calculate probability.\n",
    "    # To avoid overflow, minimum/maximum output value is set.\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-8, 1 - (1e-8))\n",
    "\n",
    "def _f(X, w, b):\n",
    "    # This is the logistic regression function, parameterized by w and b\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     X: input data, shape = [batch_size, data_dimension]\n",
    "    #     w: weight vector, shape = [data_dimension, ]\n",
    "    #     b: bias, scalar\n",
    "    # Output:\n",
    "    #     predicted probability of each row of X being positively labeled, shape = [batch_size, ]\n",
    "    return _sigmoid(np.matmul(X, w) + b)\n",
    "\n",
    "def _predict(X, w, b):\n",
    "    # This function returns a truth value prediction for each row of X \n",
    "    # by rounding the result of logistic regression function.\n",
    "    return np.round(_f(X, w, b)).astype(np.int)\n",
    "    \n",
    "def _accuracy(Y_pred, Y_label):\n",
    "    # This function calculates prediction accuracy\n",
    "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxJdfhEEOYwg"
   },
   "source": [
    "### Functions about gradient and loss\n",
    "\n",
    "Please refers to [Prof. Lee's lecture slides](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)(p.12) for the formula of gradient and loss computation.\n",
    "\n",
    "請參考[李宏毅老師上課投影片](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Logistic%20Regression%20(v3).pdf)第 12 頁的梯度及損失函數計算公式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqYkUgLjOWi1"
   },
   "outputs": [],
   "source": [
    "def _cross_entropy_loss(y_pred, Y_label):\n",
    "    # This function computes the cross entropy.\n",
    "    #\n",
    "    # Arguements:\n",
    "    #     y_pred: probabilistic predictions, float vector\n",
    "    #     Y_label: ground truth labels, bool vector\n",
    "    # Output:\n",
    "    #     cross entropy, scalar\n",
    "    cross_entropy = -np.dot(Y_label, np.log(y_pred)) - np.dot((1 - Y_label), np.log(1 - y_pred))\n",
    "    return cross_entropy\n",
    "\n",
    "def _gradient(X, Y_label, w, b):\n",
    "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
    "    y_pred = _f(X, w, b)\n",
    "    pred_error = Y_label - y_pred\n",
    "    w_grad = -np.sum(pred_error * X.T, 1)\n",
    "    b_grad = -np.sum(pred_error)\n",
    "    return w_grad, b_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _progress_bar(\n",
    "    epoch,\n",
    "    max_iter,\n",
    "    percentage,\n",
    "    training_acc,\n",
    "    training_loss,\n",
    "    dev_acc,\n",
    "    dev_loss,\n",
    "    block = 10\n",
    "):\n",
    "    if percentage == 100.:\n",
    "        print('\\r',\n",
    "              '[Epoch %d/%d]:[%s]%.1f%%' % (epoch, max_iter, '█' * block, 100.0),\n",
    "              'training_acc: %.4f' % training_acc,\n",
    "              'training_loss: %.4f' % training_loss,\n",
    "              'dev_acc: %.4f' % dev_acc,\n",
    "              'dev_loss: %.4f' % dev_loss,\n",
    "              ';\\n\\n',\n",
    "              end = '')\n",
    "    else:\n",
    "        print('\\r',\n",
    "              '[Epoch %d/%d]:[%s%s]%.1f%%' % (epoch, max_iter,\n",
    "                                           '█' * int(percentage*block/100),\n",
    "                                           ' ' * (block-int(percentage*block/100)),\n",
    "                                           float(percentage)),\n",
    "              end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXEFuqydaA34"
   },
   "source": [
    "### Training\n",
    "\n",
    "Everything is prepared, let's start training! \n",
    "\n",
    "Mini-batch gradient descent is used here, in which training data are split into several mini-batches and each batch is fed into the model sequentially for losses and gradients computation. Weights and bias are updated on a mini-batch basis.\n",
    "\n",
    "Once we have gone through the whole training set,  the data have to be re-shuffled and mini-batch gradient desent has to be run on it again. We repeat such process until max number of iterations is reached.\n",
    "\n",
    "我們使用小批次梯度下降法來訓練。訓練資料被分為許多小批次，針對每一個小批次，我們分別計算其梯度以及損失，並根據該批次來更新模型的參數。當一次迴圈完成，也就是整個訓練集的所有小批次都被使用過一次以後，我們將所有訓練資料打散並且重新分成新的小批次，進行下一個迴圈，直到事先設定的迴圈數量達成為止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 1/6000]:[██████████]100.0% training_acc: 0.8511 training_loss: 0.4012 dev_acc: 0.8437 dev_loss: 0.4069 ;\n",
      "\n",
      " [Epoch 2/6000]:[██████████]100.0% training_acc: 0.8625 training_loss: 0.3686 dev_acc: 0.8531 dev_loss: 0.3754 ;\n",
      "\n",
      " [Epoch 3/6000]:[██████████]100.0% training_acc: 0.8675 training_loss: 0.3515 dev_acc: 0.8586 dev_loss: 0.3590 ;\n",
      "\n",
      " [Epoch 4/6000]:[██████████]100.0% training_acc: 0.8702 training_loss: 0.3405 dev_acc: 0.8603 dev_loss: 0.3484 ;\n",
      "\n",
      " [Epoch 5/6000]:[██████████]100.0% training_acc: 0.8716 training_loss: 0.3326 dev_acc: 0.8614 dev_loss: 0.3409 ;\n",
      "\n",
      " [Epoch 6/6000]:[██████████]100.0% training_acc: 0.8730 training_loss: 0.3266 dev_acc: 0.8618 dev_loss: 0.3352 ;\n",
      "\n",
      " [Epoch 7/6000]:[██████████]100.0% training_acc: 0.8737 training_loss: 0.3218 dev_acc: 0.8618 dev_loss: 0.3306 ;\n",
      "\n",
      " [Epoch 8/6000]:[██████████]100.0% training_acc: 0.8748 training_loss: 0.3180 dev_acc: 0.8633 dev_loss: 0.3269 ;\n",
      "\n",
      " [Epoch 9/6000]:[██████████]100.0% training_acc: 0.8754 training_loss: 0.3147 dev_acc: 0.8647 dev_loss: 0.3239 ;\n",
      "\n",
      " [Epoch 10/6000]:[██████████]100.0% training_acc: 0.8758 training_loss: 0.3119 dev_acc: 0.8647 dev_loss: 0.3213 ;\n",
      "\n",
      " [Epoch 11/6000]:[██████████]100.0% training_acc: 0.8764 training_loss: 0.3095 dev_acc: 0.8653 dev_loss: 0.3190 ;\n",
      "\n",
      " [Epoch 12/6000]:[██████████]100.0% training_acc: 0.8770 training_loss: 0.3074 dev_acc: 0.8668 dev_loss: 0.3170 ;\n",
      "\n",
      " [Epoch 13/6000]:[██████████]100.0% training_acc: 0.8774 training_loss: 0.3056 dev_acc: 0.8671 dev_loss: 0.3153 ;\n",
      "\n",
      " [Epoch 14/6000]:[██████████]100.0% training_acc: 0.8777 training_loss: 0.3039 dev_acc: 0.8679 dev_loss: 0.3138 ;\n",
      "\n",
      " [Epoch 15/6000]:[██████████]100.0% training_acc: 0.8778 training_loss: 0.3024 dev_acc: 0.8686 dev_loss: 0.3124 ;\n",
      "\n",
      " [Epoch 16/6000]:[██████████]100.0% training_acc: 0.8780 training_loss: 0.3011 dev_acc: 0.8690 dev_loss: 0.3111 ;\n",
      "\n",
      " [Epoch 17/6000]:[██████████]100.0% training_acc: 0.8781 training_loss: 0.2998 dev_acc: 0.8690 dev_loss: 0.3100 ;\n",
      "\n",
      " [Epoch 18/6000]:[██████████]100.0% training_acc: 0.8781 training_loss: 0.2987 dev_acc: 0.8703 dev_loss: 0.3090 ;\n",
      "\n",
      " [Epoch 19/6000]:[██████████]100.0% training_acc: 0.8783 training_loss: 0.2977 dev_acc: 0.8704 dev_loss: 0.3080 ;\n",
      "\n",
      " [Epoch 20/6000]:[██████████]100.0% training_acc: 0.8786 training_loss: 0.2967 dev_acc: 0.8706 dev_loss: 0.3071 ;\n",
      "\n",
      " [Epoch 21/6000]:[██████████]100.0% training_acc: 0.8789 training_loss: 0.2958 dev_acc: 0.8706 dev_loss: 0.3063 ;\n",
      "\n",
      " [Epoch 22/6000]:[██████████]100.0% training_acc: 0.8789 training_loss: 0.2950 dev_acc: 0.8704 dev_loss: 0.3055 ;\n",
      "\n",
      " [Epoch 23/6000]:[██████████]100.0% training_acc: 0.8790 training_loss: 0.2942 dev_acc: 0.8708 dev_loss: 0.3048 ;\n",
      "\n",
      " [Epoch 24/6000]:[██████████]100.0% training_acc: 0.8791 training_loss: 0.2935 dev_acc: 0.8710 dev_loss: 0.3042 ;\n",
      "\n",
      " [Epoch 25/6000]:[██████████]100.0% training_acc: 0.8792 training_loss: 0.2928 dev_acc: 0.8708 dev_loss: 0.3036 ;\n",
      "\n",
      " [Epoch 26/6000]:[██████████]100.0% training_acc: 0.8794 training_loss: 0.2922 dev_acc: 0.8710 dev_loss: 0.3030 ;\n",
      "\n",
      " [Epoch 27/6000]:[██████████]100.0% training_acc: 0.8795 training_loss: 0.2916 dev_acc: 0.8712 dev_loss: 0.3024 ;\n",
      "\n",
      " [Epoch 28/6000]:[██████████]100.0% training_acc: 0.8796 training_loss: 0.2910 dev_acc: 0.8710 dev_loss: 0.3019 ;\n",
      "\n",
      " [Epoch 29/6000]:[██████████]100.0% training_acc: 0.8797 training_loss: 0.2905 dev_acc: 0.8712 dev_loss: 0.3014 ;\n",
      "\n",
      " [Epoch 30/6000]:[██████████]100.0% training_acc: 0.8797 training_loss: 0.2900 dev_acc: 0.8712 dev_loss: 0.3010 ;\n",
      "\n",
      " [Epoch 31/6000]:[██████████]100.0% training_acc: 0.8798 training_loss: 0.2895 dev_acc: 0.8710 dev_loss: 0.3005 ;\n",
      "\n",
      " [Epoch 32/6000]:[██████████]100.0% training_acc: 0.8798 training_loss: 0.2890 dev_acc: 0.8708 dev_loss: 0.3001 ;\n",
      "\n",
      " [Epoch 33/6000]:[██████████]100.0% training_acc: 0.8798 training_loss: 0.2886 dev_acc: 0.8708 dev_loss: 0.2997 ;\n",
      "\n",
      " [Epoch 34/6000]:[██████████]100.0% training_acc: 0.8800 training_loss: 0.2882 dev_acc: 0.8708 dev_loss: 0.2994 ;\n",
      "\n",
      " [Epoch 35/6000]:[██████████]100.0% training_acc: 0.8799 training_loss: 0.2878 dev_acc: 0.8708 dev_loss: 0.2990 ;\n",
      "\n",
      " [Epoch 36/6000]:[██████████]100.0% training_acc: 0.8799 training_loss: 0.2874 dev_acc: 0.8710 dev_loss: 0.2987 ;\n",
      "\n",
      " [Epoch 37/6000]:[██████████]100.0% training_acc: 0.8800 training_loss: 0.2870 dev_acc: 0.8714 dev_loss: 0.2983 ;\n",
      "\n",
      " [Epoch 38/6000]:[██████████]100.0% training_acc: 0.8799 training_loss: 0.2866 dev_acc: 0.8715 dev_loss: 0.2980 ;\n",
      "\n",
      " [Epoch 39/6000]:[██████████]100.0% training_acc: 0.8800 training_loss: 0.2863 dev_acc: 0.8717 dev_loss: 0.2977 ;\n",
      "\n",
      " [Epoch 40/6000]:[██████████]100.0% training_acc: 0.8802 training_loss: 0.2860 dev_acc: 0.8717 dev_loss: 0.2974 ;\n",
      "\n",
      " [Epoch 41/6000]:[██████████]100.0% training_acc: 0.8802 training_loss: 0.2857 dev_acc: 0.8717 dev_loss: 0.2971 ;\n",
      "\n",
      " [Epoch 42/6000]:[██████████]100.0% training_acc: 0.8803 training_loss: 0.2854 dev_acc: 0.8717 dev_loss: 0.2969 ;\n",
      "\n",
      " [Epoch 43/6000]:[██████████]100.0% training_acc: 0.8804 training_loss: 0.2851 dev_acc: 0.8717 dev_loss: 0.2966 ;\n",
      "\n",
      " [Epoch 44/6000]:[██████████]100.0% training_acc: 0.8804 training_loss: 0.2848 dev_acc: 0.8719 dev_loss: 0.2964 ;\n",
      "\n",
      " [Epoch 45/6000]:[██████████]100.0% training_acc: 0.8804 training_loss: 0.2845 dev_acc: 0.8719 dev_loss: 0.2961 ;\n",
      "\n",
      " [Epoch 46/6000]:[██████████]100.0% training_acc: 0.8804 training_loss: 0.2842 dev_acc: 0.8717 dev_loss: 0.2959 ;\n",
      "\n",
      " [Epoch 47/6000]:[██████████]100.0% training_acc: 0.8804 training_loss: 0.2840 dev_acc: 0.8715 dev_loss: 0.2957 ;\n",
      "\n",
      " [Epoch 48/6000]:[██████████]100.0% training_acc: 0.8805 training_loss: 0.2837 dev_acc: 0.8714 dev_loss: 0.2955 ;\n",
      "\n",
      " [Epoch 49/6000]:[██████████]100.0% training_acc: 0.8806 training_loss: 0.2835 dev_acc: 0.8712 dev_loss: 0.2953 ;\n",
      "\n",
      " [Epoch 50/6000]:[██████████]100.0% training_acc: 0.8807 training_loss: 0.2833 dev_acc: 0.8710 dev_loss: 0.2951 ;\n",
      "\n",
      " [Epoch 51/6000]:[██████████]100.0% training_acc: 0.8806 training_loss: 0.2831 dev_acc: 0.8712 dev_loss: 0.2949 ;\n",
      "\n",
      " [Epoch 52/6000]:[██████████]100.0% training_acc: 0.8807 training_loss: 0.2828 dev_acc: 0.8710 dev_loss: 0.2947 ;\n",
      "\n",
      " [Epoch 53/6000]:[██████████]100.0% training_acc: 0.8807 training_loss: 0.2826 dev_acc: 0.8710 dev_loss: 0.2945 ;\n",
      "\n",
      " [Epoch 54/6000]:[██████████]100.0% training_acc: 0.8807 training_loss: 0.2824 dev_acc: 0.8712 dev_loss: 0.2943 ;\n",
      "\n",
      " [Epoch 55/6000]:[██████████]100.0% training_acc: 0.8808 training_loss: 0.2822 dev_acc: 0.8714 dev_loss: 0.2941 ;\n",
      "\n",
      " [Epoch 56/6000]:[██████████]100.0% training_acc: 0.8808 training_loss: 0.2820 dev_acc: 0.8714 dev_loss: 0.2940 ;\n",
      "\n",
      " [Epoch 57/6000]:[██████████]100.0% training_acc: 0.8808 training_loss: 0.2818 dev_acc: 0.8715 dev_loss: 0.2938 ;\n",
      "\n",
      " [Epoch 58/6000]:[██████████]100.0% training_acc: 0.8809 training_loss: 0.2817 dev_acc: 0.8717 dev_loss: 0.2936 ;\n",
      "\n",
      " [Epoch 59/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2815 dev_acc: 0.8717 dev_loss: 0.2935 ;\n",
      "\n",
      " [Epoch 60/6000]:[██████████]100.0% training_acc: 0.8809 training_loss: 0.2813 dev_acc: 0.8717 dev_loss: 0.2933 ;\n",
      "\n",
      " [Epoch 61/6000]:[██████████]100.0% training_acc: 0.8809 training_loss: 0.2811 dev_acc: 0.8717 dev_loss: 0.2932 ;\n",
      "\n",
      " [Epoch 62/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2810 dev_acc: 0.8717 dev_loss: 0.2930 ;\n",
      "\n",
      " [Epoch 63/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2808 dev_acc: 0.8717 dev_loss: 0.2929 ;\n",
      "\n",
      " [Epoch 64/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2806 dev_acc: 0.8719 dev_loss: 0.2928 ;\n",
      "\n",
      " [Epoch 65/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2805 dev_acc: 0.8719 dev_loss: 0.2926 ;\n",
      "\n",
      " [Epoch 66/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2803 dev_acc: 0.8719 dev_loss: 0.2925 ;\n",
      "\n",
      " [Epoch 67/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2802 dev_acc: 0.8719 dev_loss: 0.2924 ;\n",
      "\n",
      " [Epoch 68/6000]:[██████████]100.0% training_acc: 0.8810 training_loss: 0.2801 dev_acc: 0.8719 dev_loss: 0.2923 ;\n",
      "\n",
      " [Epoch 69/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2799 dev_acc: 0.8721 dev_loss: 0.2921 ;\n",
      "\n",
      " [Epoch 70/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2798 dev_acc: 0.8721 dev_loss: 0.2920 ;\n",
      "\n",
      " [Epoch 71/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2796 dev_acc: 0.8721 dev_loss: 0.2919 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 72/6000]:[██████████]100.0% training_acc: 0.8812 training_loss: 0.2795 dev_acc: 0.8721 dev_loss: 0.2918 ;\n",
      "\n",
      " [Epoch 73/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2794 dev_acc: 0.8721 dev_loss: 0.2917 ;\n",
      "\n",
      " [Epoch 74/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2793 dev_acc: 0.8719 dev_loss: 0.2916 ;\n",
      "\n",
      " [Epoch 75/6000]:[██████████]100.0% training_acc: 0.8812 training_loss: 0.2791 dev_acc: 0.8721 dev_loss: 0.2915 ;\n",
      "\n",
      " [Epoch 76/6000]:[██████████]100.0% training_acc: 0.8812 training_loss: 0.2790 dev_acc: 0.8723 dev_loss: 0.2914 ;\n",
      "\n",
      " [Epoch 77/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2789 dev_acc: 0.8721 dev_loss: 0.2913 ;\n",
      "\n",
      " [Epoch 78/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2788 dev_acc: 0.8723 dev_loss: 0.2912 ;\n",
      "\n",
      " [Epoch 79/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2787 dev_acc: 0.8725 dev_loss: 0.2911 ;\n",
      "\n",
      " [Epoch 80/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2786 dev_acc: 0.8727 dev_loss: 0.2910 ;\n",
      "\n",
      " [Epoch 81/6000]:[██████████]100.0% training_acc: 0.8811 training_loss: 0.2785 dev_acc: 0.8727 dev_loss: 0.2909 ;\n",
      "\n",
      " [Epoch 82/6000]:[██████████]100.0% training_acc: 0.8812 training_loss: 0.2784 dev_acc: 0.8727 dev_loss: 0.2908 ;\n",
      "\n",
      " [Epoch 83/6000]:[██████████]100.0% training_acc: 0.8812 training_loss: 0.2782 dev_acc: 0.8727 dev_loss: 0.2907 ;\n",
      "\n",
      " [Epoch 84/6000]:[██████████]100.0% training_acc: 0.8813 training_loss: 0.2781 dev_acc: 0.8728 dev_loss: 0.2906 ;\n",
      "\n",
      " [Epoch 85/6000]:[██████████]100.0% training_acc: 0.8813 training_loss: 0.2780 dev_acc: 0.8727 dev_loss: 0.2906 ;\n",
      "\n",
      " [Epoch 86/6000]:[██████████]100.0% training_acc: 0.8813 training_loss: 0.2779 dev_acc: 0.8728 dev_loss: 0.2905 ;\n",
      "\n",
      " [Epoch 87/6000]:[██████████]100.0% training_acc: 0.8813 training_loss: 0.2779 dev_acc: 0.8730 dev_loss: 0.2904 ;\n",
      "\n",
      " [Epoch 88/6000]:[██████████]100.0% training_acc: 0.8814 training_loss: 0.2778 dev_acc: 0.8730 dev_loss: 0.2903 ;\n",
      "\n",
      " [Epoch 89/6000]:[██████████]100.0% training_acc: 0.8814 training_loss: 0.2777 dev_acc: 0.8728 dev_loss: 0.2902 ;\n",
      "\n",
      " [Epoch 90/6000]:[██████████]100.0% training_acc: 0.8814 training_loss: 0.2776 dev_acc: 0.8728 dev_loss: 0.2901 ;\n",
      "\n",
      " [Epoch 91/6000]:[██████████]100.0% training_acc: 0.8814 training_loss: 0.2775 dev_acc: 0.8732 dev_loss: 0.2901 ;\n",
      "\n",
      " [Epoch 92/6000]:[██████████]100.0% training_acc: 0.8814 training_loss: 0.2774 dev_acc: 0.8732 dev_loss: 0.2900 ;\n",
      "\n",
      " [Epoch 93/6000]:[██████████]100.0% training_acc: 0.8815 training_loss: 0.2773 dev_acc: 0.8734 dev_loss: 0.2899 ;\n",
      "\n",
      " [Epoch 94/6000]:[██████████]100.0% training_acc: 0.8816 training_loss: 0.2772 dev_acc: 0.8736 dev_loss: 0.2899 ;\n",
      "\n",
      " [Epoch 95/6000]:[██████████]100.0% training_acc: 0.8816 training_loss: 0.2771 dev_acc: 0.8736 dev_loss: 0.2898 ;\n",
      "\n",
      " [Epoch 96/6000]:[██████████]100.0% training_acc: 0.8816 training_loss: 0.2771 dev_acc: 0.8736 dev_loss: 0.2897 ;\n",
      "\n",
      " [Epoch 97/6000]:[██████████]100.0% training_acc: 0.8816 training_loss: 0.2770 dev_acc: 0.8738 dev_loss: 0.2896 ;\n",
      "\n",
      " [Epoch 98/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2769 dev_acc: 0.8736 dev_loss: 0.2896 ;\n",
      "\n",
      " [Epoch 99/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2768 dev_acc: 0.8736 dev_loss: 0.2895 ;\n",
      "\n",
      " [Epoch 100/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2767 dev_acc: 0.8736 dev_loss: 0.2894 ;\n",
      "\n",
      " [Epoch 101/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2767 dev_acc: 0.8738 dev_loss: 0.2894 ;\n",
      "\n",
      " [Epoch 102/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2766 dev_acc: 0.8738 dev_loss: 0.2893 ;\n",
      "\n",
      " [Epoch 103/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2765 dev_acc: 0.8738 dev_loss: 0.2892 ;\n",
      "\n",
      " [Epoch 104/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2764 dev_acc: 0.8738 dev_loss: 0.2892 ;\n",
      "\n",
      " [Epoch 105/6000]:[██████████]100.0% training_acc: 0.8817 training_loss: 0.2764 dev_acc: 0.8738 dev_loss: 0.2891 ;\n",
      "\n",
      " [Epoch 106/6000]:[██████████]100.0% training_acc: 0.8818 training_loss: 0.2763 dev_acc: 0.8738 dev_loss: 0.2891 ;\n",
      "\n",
      " [Epoch 107/6000]:[██████████]100.0% training_acc: 0.8818 training_loss: 0.2762 dev_acc: 0.8738 dev_loss: 0.2890 ;\n",
      "\n",
      " [Epoch 108/6000]:[██████████]100.0% training_acc: 0.8818 training_loss: 0.2762 dev_acc: 0.8738 dev_loss: 0.2890 ;\n",
      "\n",
      " [Epoch 109/6000]:[██████████]100.0% training_acc: 0.8818 training_loss: 0.2761 dev_acc: 0.8738 dev_loss: 0.2889 ;\n",
      "\n",
      " [Epoch 110/6000]:[██████████]100.0% training_acc: 0.8819 training_loss: 0.2760 dev_acc: 0.8739 dev_loss: 0.2888 ;\n",
      "\n",
      " [Epoch 111/6000]:[██████████]100.0% training_acc: 0.8819 training_loss: 0.2760 dev_acc: 0.8739 dev_loss: 0.2888 ;\n",
      "\n",
      " [Epoch 112/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2759 dev_acc: 0.8739 dev_loss: 0.2887 ;\n",
      "\n",
      " [Epoch 113/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2758 dev_acc: 0.8739 dev_loss: 0.2887 ;\n",
      "\n",
      " [Epoch 114/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2758 dev_acc: 0.8739 dev_loss: 0.2886 ;\n",
      "\n",
      " [Epoch 115/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2757 dev_acc: 0.8739 dev_loss: 0.2886 ;\n",
      "\n",
      " [Epoch 116/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2756 dev_acc: 0.8739 dev_loss: 0.2885 ;\n",
      "\n",
      " [Epoch 117/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2756 dev_acc: 0.8738 dev_loss: 0.2885 ;\n",
      "\n",
      " [Epoch 118/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2755 dev_acc: 0.8738 dev_loss: 0.2884 ;\n",
      "\n",
      " [Epoch 119/6000]:[██████████]100.0% training_acc: 0.8820 training_loss: 0.2755 dev_acc: 0.8738 dev_loss: 0.2884 ;\n",
      "\n",
      " [Epoch 120/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2754 dev_acc: 0.8739 dev_loss: 0.2883 ;\n",
      "\n",
      " [Epoch 121/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2753 dev_acc: 0.8739 dev_loss: 0.2883 ;\n",
      "\n",
      " [Epoch 122/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2753 dev_acc: 0.8739 dev_loss: 0.2882 ;\n",
      "\n",
      " [Epoch 123/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2752 dev_acc: 0.8739 dev_loss: 0.2882 ;\n",
      "\n",
      " [Epoch 124/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2752 dev_acc: 0.8739 dev_loss: 0.2881 ;\n",
      "\n",
      " [Epoch 125/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2751 dev_acc: 0.8738 dev_loss: 0.2881 ;\n",
      "\n",
      " [Epoch 126/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2751 dev_acc: 0.8738 dev_loss: 0.2880 ;\n",
      "\n",
      " [Epoch 127/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2750 dev_acc: 0.8738 dev_loss: 0.2880 ;\n",
      "\n",
      " [Epoch 128/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2750 dev_acc: 0.8738 dev_loss: 0.2880 ;\n",
      "\n",
      " [Epoch 129/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2749 dev_acc: 0.8736 dev_loss: 0.2879 ;\n",
      "\n",
      " [Epoch 130/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2749 dev_acc: 0.8736 dev_loss: 0.2879 ;\n",
      "\n",
      " [Epoch 131/6000]:[██████████]100.0% training_acc: 0.8821 training_loss: 0.2748 dev_acc: 0.8736 dev_loss: 0.2878 ;\n",
      "\n",
      " [Epoch 132/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2748 dev_acc: 0.8736 dev_loss: 0.2878 ;\n",
      "\n",
      " [Epoch 133/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2747 dev_acc: 0.8736 dev_loss: 0.2877 ;\n",
      "\n",
      " [Epoch 134/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2747 dev_acc: 0.8734 dev_loss: 0.2877 ;\n",
      "\n",
      " [Epoch 135/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2746 dev_acc: 0.8736 dev_loss: 0.2877 ;\n",
      "\n",
      " [Epoch 136/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2746 dev_acc: 0.8736 dev_loss: 0.2876 ;\n",
      "\n",
      " [Epoch 137/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2745 dev_acc: 0.8738 dev_loss: 0.2876 ;\n",
      "\n",
      " [Epoch 138/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2745 dev_acc: 0.8738 dev_loss: 0.2875 ;\n",
      "\n",
      " [Epoch 139/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2744 dev_acc: 0.8738 dev_loss: 0.2875 ;\n",
      "\n",
      " [Epoch 140/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2744 dev_acc: 0.8739 dev_loss: 0.2875 ;\n",
      "\n",
      " [Epoch 141/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2743 dev_acc: 0.8739 dev_loss: 0.2874 ;\n",
      "\n",
      " [Epoch 142/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2743 dev_acc: 0.8739 dev_loss: 0.2874 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 143/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2742 dev_acc: 0.8739 dev_loss: 0.2874 ;\n",
      "\n",
      " [Epoch 144/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2742 dev_acc: 0.8739 dev_loss: 0.2873 ;\n",
      "\n",
      " [Epoch 145/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2742 dev_acc: 0.8739 dev_loss: 0.2873 ;\n",
      "\n",
      " [Epoch 146/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2741 dev_acc: 0.8739 dev_loss: 0.2873 ;\n",
      "\n",
      " [Epoch 147/6000]:[██████████]100.0% training_acc: 0.8822 training_loss: 0.2741 dev_acc: 0.8739 dev_loss: 0.2872 ;\n",
      "\n",
      " [Epoch 148/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2740 dev_acc: 0.8739 dev_loss: 0.2872 ;\n",
      "\n",
      " [Epoch 149/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2740 dev_acc: 0.8739 dev_loss: 0.2872 ;\n",
      "\n",
      " [Epoch 150/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2739 dev_acc: 0.8741 dev_loss: 0.2871 ;\n",
      "\n",
      " [Epoch 151/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2739 dev_acc: 0.8741 dev_loss: 0.2871 ;\n",
      "\n",
      " [Epoch 152/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2739 dev_acc: 0.8743 dev_loss: 0.2871 ;\n",
      "\n",
      " [Epoch 153/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2738 dev_acc: 0.8741 dev_loss: 0.2870 ;\n",
      "\n",
      " [Epoch 154/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2738 dev_acc: 0.8741 dev_loss: 0.2870 ;\n",
      "\n",
      " [Epoch 155/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2737 dev_acc: 0.8741 dev_loss: 0.2870 ;\n",
      "\n",
      " [Epoch 156/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2737 dev_acc: 0.8743 dev_loss: 0.2869 ;\n",
      "\n",
      " [Epoch 157/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2737 dev_acc: 0.8743 dev_loss: 0.2869 ;\n",
      "\n",
      " [Epoch 158/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2736 dev_acc: 0.8743 dev_loss: 0.2869 ;\n",
      "\n",
      " [Epoch 159/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2736 dev_acc: 0.8741 dev_loss: 0.2868 ;\n",
      "\n",
      " [Epoch 160/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2736 dev_acc: 0.8741 dev_loss: 0.2868 ;\n",
      "\n",
      " [Epoch 161/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2735 dev_acc: 0.8741 dev_loss: 0.2868 ;\n",
      "\n",
      " [Epoch 162/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2735 dev_acc: 0.8741 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 163/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2734 dev_acc: 0.8741 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 164/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2734 dev_acc: 0.8741 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 165/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2734 dev_acc: 0.8741 dev_loss: 0.2867 ;\n",
      "\n",
      " [Epoch 166/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2733 dev_acc: 0.8741 dev_loss: 0.2866 ;\n",
      "\n",
      " [Epoch 167/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2733 dev_acc: 0.8741 dev_loss: 0.2866 ;\n",
      "\n",
      " [Epoch 168/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2733 dev_acc: 0.8741 dev_loss: 0.2866 ;\n",
      "\n",
      " [Epoch 169/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2732 dev_acc: 0.8745 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 170/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2732 dev_acc: 0.8745 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 171/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2732 dev_acc: 0.8745 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 172/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2731 dev_acc: 0.8745 dev_loss: 0.2865 ;\n",
      "\n",
      " [Epoch 173/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2731 dev_acc: 0.8745 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 174/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2731 dev_acc: 0.8747 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 175/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2730 dev_acc: 0.8747 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 176/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2730 dev_acc: 0.8747 dev_loss: 0.2864 ;\n",
      "\n",
      " [Epoch 177/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2730 dev_acc: 0.8747 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 178/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2729 dev_acc: 0.8747 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 179/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2729 dev_acc: 0.8749 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 180/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2729 dev_acc: 0.8749 dev_loss: 0.2863 ;\n",
      "\n",
      " [Epoch 181/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2729 dev_acc: 0.8749 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 182/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2728 dev_acc: 0.8750 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 183/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2728 dev_acc: 0.8750 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 184/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2728 dev_acc: 0.8750 dev_loss: 0.2862 ;\n",
      "\n",
      " [Epoch 185/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2727 dev_acc: 0.8752 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 186/6000]:[██████████]100.0% training_acc: 0.8823 training_loss: 0.2727 dev_acc: 0.8752 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 187/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2727 dev_acc: 0.8752 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 188/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2727 dev_acc: 0.8752 dev_loss: 0.2861 ;\n",
      "\n",
      " [Epoch 189/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2726 dev_acc: 0.8752 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 190/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2726 dev_acc: 0.8754 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 191/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2726 dev_acc: 0.8754 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 192/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2725 dev_acc: 0.8754 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 193/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2725 dev_acc: 0.8754 dev_loss: 0.2860 ;\n",
      "\n",
      " [Epoch 194/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2725 dev_acc: 0.8754 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 195/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2725 dev_acc: 0.8754 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 196/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2724 dev_acc: 0.8754 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 197/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2724 dev_acc: 0.8752 dev_loss: 0.2859 ;\n",
      "\n",
      " [Epoch 198/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2724 dev_acc: 0.8752 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 199/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2724 dev_acc: 0.8752 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 200/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2723 dev_acc: 0.8754 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 201/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2723 dev_acc: 0.8754 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 202/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2723 dev_acc: 0.8754 dev_loss: 0.2858 ;\n",
      "\n",
      " [Epoch 203/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2723 dev_acc: 0.8756 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 204/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2722 dev_acc: 0.8756 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 205/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2722 dev_acc: 0.8756 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 206/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2722 dev_acc: 0.8756 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 207/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2722 dev_acc: 0.8756 dev_loss: 0.2857 ;\n",
      "\n",
      " [Epoch 208/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2721 dev_acc: 0.8756 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 209/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2721 dev_acc: 0.8756 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 210/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2721 dev_acc: 0.8756 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 211/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2721 dev_acc: 0.8756 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 212/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2720 dev_acc: 0.8756 dev_loss: 0.2856 ;\n",
      "\n",
      " [Epoch 213/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2720 dev_acc: 0.8756 dev_loss: 0.2855 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 214/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2720 dev_acc: 0.8756 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 215/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2720 dev_acc: 0.8756 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 216/6000]:[██████████]100.0% training_acc: 0.8824 training_loss: 0.2719 dev_acc: 0.8756 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 217/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2719 dev_acc: 0.8756 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 218/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2719 dev_acc: 0.8756 dev_loss: 0.2855 ;\n",
      "\n",
      " [Epoch 219/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2719 dev_acc: 0.8756 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 220/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2719 dev_acc: 0.8758 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 221/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2718 dev_acc: 0.8758 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 222/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2718 dev_acc: 0.8758 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 223/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2718 dev_acc: 0.8758 dev_loss: 0.2854 ;\n",
      "\n",
      " [Epoch 224/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2718 dev_acc: 0.8760 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 225/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2717 dev_acc: 0.8760 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 226/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2717 dev_acc: 0.8760 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 227/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2717 dev_acc: 0.8760 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 228/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2717 dev_acc: 0.8760 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 229/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2717 dev_acc: 0.8760 dev_loss: 0.2853 ;\n",
      "\n",
      " [Epoch 230/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2716 dev_acc: 0.8762 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 231/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2716 dev_acc: 0.8762 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 232/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2716 dev_acc: 0.8760 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 233/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2716 dev_acc: 0.8760 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 234/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2716 dev_acc: 0.8760 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 235/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2715 dev_acc: 0.8760 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 236/6000]:[██████████]100.0% training_acc: 0.8825 training_loss: 0.2715 dev_acc: 0.8762 dev_loss: 0.2852 ;\n",
      "\n",
      " [Epoch 237/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2715 dev_acc: 0.8762 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 238/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2715 dev_acc: 0.8762 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 239/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2715 dev_acc: 0.8762 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 240/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2714 dev_acc: 0.8762 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 241/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2714 dev_acc: 0.8762 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 242/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2714 dev_acc: 0.8762 dev_loss: 0.2851 ;\n",
      "\n",
      " [Epoch 243/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2714 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 244/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2714 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 245/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2713 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 246/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2713 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 247/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2713 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 248/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2713 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 249/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2713 dev_acc: 0.8762 dev_loss: 0.2850 ;\n",
      "\n",
      " [Epoch 250/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2713 dev_acc: 0.8760 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 251/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2712 dev_acc: 0.8762 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 252/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2712 dev_acc: 0.8762 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 253/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2712 dev_acc: 0.8762 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 254/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2712 dev_acc: 0.8762 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 255/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2712 dev_acc: 0.8762 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 256/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2712 dev_acc: 0.8762 dev_loss: 0.2849 ;\n",
      "\n",
      " [Epoch 257/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2711 dev_acc: 0.8762 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 258/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2711 dev_acc: 0.8762 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 259/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2711 dev_acc: 0.8762 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 260/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2711 dev_acc: 0.8762 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 261/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2711 dev_acc: 0.8762 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 262/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2710 dev_acc: 0.8760 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 263/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2710 dev_acc: 0.8762 dev_loss: 0.2848 ;\n",
      "\n",
      " [Epoch 264/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2710 dev_acc: 0.8762 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 265/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2710 dev_acc: 0.8762 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 266/6000]:[██████████]100.0% training_acc: 0.8826 training_loss: 0.2710 dev_acc: 0.8762 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 267/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2710 dev_acc: 0.8762 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 268/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2710 dev_acc: 0.8765 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 269/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2709 dev_acc: 0.8765 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 270/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2709 dev_acc: 0.8763 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 271/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2709 dev_acc: 0.8765 dev_loss: 0.2847 ;\n",
      "\n",
      " [Epoch 272/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2709 dev_acc: 0.8765 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 273/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2709 dev_acc: 0.8765 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 274/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2709 dev_acc: 0.8765 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 275/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2708 dev_acc: 0.8767 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 276/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2708 dev_acc: 0.8767 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 277/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2708 dev_acc: 0.8767 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 278/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2708 dev_acc: 0.8769 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 279/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2708 dev_acc: 0.8769 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 280/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2708 dev_acc: 0.8769 dev_loss: 0.2846 ;\n",
      "\n",
      " [Epoch 281/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2708 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 282/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2707 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 283/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2707 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 284/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2707 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 285/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2707 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 286/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2707 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 287/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2707 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 288/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2845 ;\n",
      "\n",
      " [Epoch 289/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 290/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 291/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 292/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 293/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 294/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 295/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2706 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 296/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 297/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8769 dev_loss: 0.2844 ;\n",
      "\n",
      " [Epoch 298/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8769 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 299/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8771 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 300/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8771 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 301/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8771 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 302/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2705 dev_acc: 0.8771 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 303/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8771 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 304/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8771 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 305/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8769 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 306/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8769 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 307/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8769 dev_loss: 0.2843 ;\n",
      "\n",
      " [Epoch 308/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 309/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 310/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2704 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 311/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 312/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 313/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 314/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 315/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 316/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 317/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 318/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2703 dev_acc: 0.8769 dev_loss: 0.2842 ;\n",
      "\n",
      " [Epoch 319/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 320/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 321/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 322/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 323/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 324/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 325/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 326/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2702 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 327/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 328/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8769 dev_loss: 0.2841 ;\n",
      "\n",
      " [Epoch 329/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8769 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 330/6000]:[██████████]100.0% training_acc: 0.8827 training_loss: 0.2701 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 331/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 332/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 333/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 334/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 335/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2701 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 336/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 337/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 338/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 339/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 340/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2840 ;\n",
      "\n",
      " [Epoch 341/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 342/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 343/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 344/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2700 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 345/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 346/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8769 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 347/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 348/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 349/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 350/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 351/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 352/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2839 ;\n",
      "\n",
      " [Epoch 353/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8771 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 354/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2699 dev_acc: 0.8773 dev_loss: 0.2838 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 355/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8773 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 356/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 357/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 358/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 359/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 360/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 361/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 362/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 363/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 364/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2698 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 365/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2838 ;\n",
      "\n",
      " [Epoch 366/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 367/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 368/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 369/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 370/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 371/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 372/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 373/6000]:[██████████]100.0% training_acc: 0.8828 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 374/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2697 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 375/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 376/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8774 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 377/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 378/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 379/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2837 ;\n",
      "\n",
      " [Epoch 380/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 381/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 382/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 383/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 384/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 385/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 386/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2696 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 387/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 388/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8776 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 389/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 390/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 391/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 392/6000]:[██████████]100.0% training_acc: 0.8829 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 393/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2836 ;\n",
      "\n",
      " [Epoch 394/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 395/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 396/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 397/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2695 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 398/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 399/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 400/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 401/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 402/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 403/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 404/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 405/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 406/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 407/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 408/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2694 dev_acc: 0.8778 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 409/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2694 dev_acc: 0.8780 dev_loss: 0.2835 ;\n",
      "\n",
      " [Epoch 410/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 411/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 412/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 413/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 414/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 415/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 416/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 417/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 418/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 419/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 420/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 421/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 422/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2693 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 423/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 424/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 425/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 426/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2834 ;\n",
      "\n",
      " [Epoch 427/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 428/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 429/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 430/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 431/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 432/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 433/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 434/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 435/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 436/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2692 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 437/6000]:[██████████]100.0% training_acc: 0.8830 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 438/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 439/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 440/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 441/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8780 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 442/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 443/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 444/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2833 ;\n",
      "\n",
      " [Epoch 445/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 446/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 447/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 448/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 449/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 450/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2691 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 451/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 452/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 453/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 454/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 455/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 456/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 457/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 458/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 459/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 460/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 461/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 462/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 463/6000]:[██████████]100.0% training_acc: 0.8831 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2832 ;\n",
      "\n",
      " [Epoch 464/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 465/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2690 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 466/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 467/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 468/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 469/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 470/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 471/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 472/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 473/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 474/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 475/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 476/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 477/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 478/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8782 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 479/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 480/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 481/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2689 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 482/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 483/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 484/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 485/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2831 ;\n",
      "\n",
      " [Epoch 486/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 487/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 488/6000]:[██████████]100.0% training_acc: 0.8832 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 489/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 490/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 491/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 492/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 493/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 494/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 495/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 496/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 497/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2688 dev_acc: 0.8784 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 498/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 499/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 500/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 501/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 502/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 503/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 504/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 505/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 506/6000]:[██████████]100.0% training_acc: 0.8833 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 507/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2830 ;\n",
      "\n",
      " [Epoch 508/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 509/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 510/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 511/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 512/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 513/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8785 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 514/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8784 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 515/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2687 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 516/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 517/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 518/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 519/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 520/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 521/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 522/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 523/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 524/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 525/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 526/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 527/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 528/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 529/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 530/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 531/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 532/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2829 ;\n",
      "\n",
      " [Epoch 533/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 534/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2686 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 535/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;00]:[███████   ]73.3%\n",
      "\n",
      " [Epoch 536/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 537/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 538/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 539/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 540/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 541/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 542/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 543/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 544/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 545/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 546/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 547/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 548/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 549/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 550/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 551/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 552/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8784 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 553/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 554/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2685 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 555/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2684 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 556/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2684 dev_acc: 0.8782 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 557/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 558/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 559/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 560/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2828 ;\n",
      "\n",
      " [Epoch 561/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 562/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 563/6000]:[██████████]100.0% training_acc: 0.8834 training_loss: 0.2684 dev_acc: 0.8780 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 564/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 565/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 566/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 567/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 568/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 569/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 570/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 571/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 572/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 573/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 574/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 575/6000]:[██████████]100.0% training_acc: 0.8835 training_loss: 0.2684 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 576/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 577/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 578/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 579/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 580/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 581/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 582/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 583/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 584/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 585/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 586/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 587/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 588/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 589/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 590/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2827 ;\n",
      "\n",
      " [Epoch 591/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 592/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 593/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 594/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 595/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 596/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 597/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 598/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2683 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 599/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 600/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 601/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 602/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 603/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 604/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 605/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 606/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 607/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 608/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 609/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 610/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 611/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 612/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 613/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 614/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 615/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 616/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 617/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 618/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8778 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 619/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8780 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 620/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8780 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 621/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8780 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 622/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2682 dev_acc: 0.8780 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 623/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2826 ;\n",
      "\n",
      " [Epoch 624/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 625/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 626/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 627/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 628/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 629/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 630/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 631/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 632/6000]:[██████████]100.0% training_acc: 0.8836 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 633/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 634/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 635/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 636/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 637/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 638/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 639/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 640/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 641/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 642/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 643/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 644/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 645/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 646/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 647/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 648/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2681 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 649/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 650/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 651/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 652/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 653/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 654/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 655/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 656/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 657/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 658/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 659/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 660/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2825 ;\n",
      "\n",
      " [Epoch 661/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 662/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 663/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 664/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 665/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 666/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 667/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 668/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 669/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 670/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 671/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 672/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 673/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 674/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 675/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 676/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2680 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 677/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 678/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 679/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 680/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 681/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 682/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 683/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 684/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 685/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 686/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 687/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;6000]:[██        ]26.6%\n",
      "\n",
      " [Epoch 688/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 689/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 690/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 691/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 692/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 693/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 694/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 695/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 696/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 697/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 698/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8780 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 699/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 700/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2824 ;\n",
      "\n",
      " [Epoch 701/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 702/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 703/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 704/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 705/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 706/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2679 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 707/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 708/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 709/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 710/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 711/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 712/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 713/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 714/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 715/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 716/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 717/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 718/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 719/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 720/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 721/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 722/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 723/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 724/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 725/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 726/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 727/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 728/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 729/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 730/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 731/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 732/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 733/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 734/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 735/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 736/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 737/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 738/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 739/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2678 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 740/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 741/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 742/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 743/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 744/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 745/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 746/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2823 ;\n",
      "\n",
      " [Epoch 747/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 748/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 749/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 750/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 751/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 752/6000]:[██████████]100.0% training_acc: 0.8837 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 753/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 754/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 755/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 756/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 757/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 758/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 759/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 760/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 761/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 762/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 763/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 764/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 765/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 766/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 767/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 768/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 769/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 770/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 771/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 772/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 773/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 774/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2677 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 775/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 776/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8778 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 777/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 778/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 779/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 780/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 781/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 782/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 783/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 784/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 785/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 786/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 787/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 788/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 789/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 790/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 791/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 792/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 793/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 794/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 795/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 796/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 797/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2822 ;\n",
      "\n",
      " [Epoch 798/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 799/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 800/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 801/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 802/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 803/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 804/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 805/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 806/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 807/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 808/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 809/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 810/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 811/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2676 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 812/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 813/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 814/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 815/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 816/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 817/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 818/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 819/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 820/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 821/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 822/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 823/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 824/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 825/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 826/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 827/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 828/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 829/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 830/6000]:[██████████]100.0% training_acc: 0.8838 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 831/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 832/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 833/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 834/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 835/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 836/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 837/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 838/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 839/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 840/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 841/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8782 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 842/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 843/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 844/6000]:[██████████]100.0% training_acc: 0.8839 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 845/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 846/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 847/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 848/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 849/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 850/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 851/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 852/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 853/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2675 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 854/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 855/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 856/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2821 ;\n",
      "\n",
      " [Epoch 857/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 858/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 859/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 860/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 861/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 862/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 863/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 864/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 865/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 866/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 867/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 868/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 869/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 870/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 871/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 872/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 873/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 874/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 875/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 876/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 877/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 878/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 879/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 880/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 881/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 882/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 883/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 884/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 885/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 886/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 887/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 888/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 889/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 890/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 891/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 892/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 893/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 894/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 895/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 896/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 897/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2674 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 898/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 899/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 900/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 901/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 902/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 903/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 904/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 905/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 906/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 907/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 908/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 909/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 910/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 911/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 912/6000]:[██████████]100.0% training_acc: 0.8840 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 913/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 914/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 915/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 916/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 917/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 918/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 919/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 920/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 921/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 922/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 923/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 924/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 925/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2820 ;\n",
      "\n",
      " [Epoch 926/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 927/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 928/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 929/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 930/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 931/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 932/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 933/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 934/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 935/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 936/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 937/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 938/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 939/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 940/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 941/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 942/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 943/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8780 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 944/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 945/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 946/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 947/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2673 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 948/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 949/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 950/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 951/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 952/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 953/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 954/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 955/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 956/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 957/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 958/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 959/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 960/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 961/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 962/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 963/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 964/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 965/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 966/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 967/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 968/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 969/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 970/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 971/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 972/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 973/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 974/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 975/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 976/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 977/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 978/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 979/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 980/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 981/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 982/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 983/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 984/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 985/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 986/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 987/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 988/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 989/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 990/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 991/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 992/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 993/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 994/6000]:[██████████]100.0% training_acc: 0.8841 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 995/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 996/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 997/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 998/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      " [Epoch 999/6000]:[██████████]100.0% training_acc: 0.8842 training_loss: 0.2672 dev_acc: 0.8782 dev_loss: 0.2819 ;\n",
      "\n",
      "early stopping at epoch: 498\n"
     ]
    }
   ],
   "source": [
    "# Zero initialization for weights ans bias\n",
    "w = np.zeros((data_dim,)) \n",
    "b = np.zeros((1,))\n",
    "\n",
    "# Some parameters for training    \n",
    "max_iter = 6000\n",
    "batch_size = 200\n",
    "learning_rate = 0.0005\n",
    "early_stopping_iter = 100\n",
    "temp_acc = 0\n",
    "temp_epoch = 0\n",
    "\n",
    "# Keep the loss and accuracy at every iteration for plotting\n",
    "train_loss = []\n",
    "dev_loss = []\n",
    "train_acc = []\n",
    "dev_acc = []\n",
    "\n",
    "# Calcuate the number of parameter updates\n",
    "step = 1\n",
    "\n",
    "# Iterative training\n",
    "for epoch in range(max_iter):\n",
    "    # Random shuffle at the begging of each epoch\n",
    "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for idx in range(int(np.floor(train_size / batch_size))):\n",
    "        X = X_train[idx*batch_size:(idx+1)*batch_size]\n",
    "        Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "        # Compute the gradient\n",
    "        w_grad, b_grad = _gradient(X, Y, w, b)\n",
    "            \n",
    "        # gradient descent update\n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate/np.sqrt(step) * w_grad\n",
    "        b = b - learning_rate/np.sqrt(step) * b_grad\n",
    "\n",
    "        step = step + 1\n",
    "        \n",
    "        y_train_pred = _f(X_train, w, b)\n",
    "        Y_train_pred = np.round(y_train_pred)\n",
    "        T_acc = _accuracy(Y_train_pred, Y_train)\n",
    "        T_loss = _cross_entropy_loss(y_train_pred, Y_train) / train_size\n",
    "        \n",
    "        y_dev_pred = _f(X_dev, w, b)\n",
    "        Y_dev_pred = np.round(y_dev_pred)\n",
    "        D_acc = _accuracy(Y_dev_pred, Y_dev)\n",
    "        D_loss = _cross_entropy_loss(y_dev_pred, Y_dev) / dev_size\n",
    "        \n",
    "        # progress bar\n",
    "        if idx == int(np.floor(train_size / batch_size)) - 1:\n",
    "            _progress_bar(epoch+1, max_iter, 100., T_acc, T_loss, D_acc, D_loss)\n",
    "        else:\n",
    "            _progress_bar(epoch+1, max_iter, idx * batch_size * 100 / train_size, T_acc, T_loss, D_acc, D_loss)\n",
    "\n",
    "    train_acc.append(T_acc)\n",
    "    train_loss.append(T_loss)\n",
    "\n",
    "    dev_acc.append(D_acc)\n",
    "    dev_loss.append(D_loss)\n",
    "        \n",
    "    # early stopping\n",
    "    if D_acc > temp_acc:\n",
    "        temp_acc = np.copy(D_acc)\n",
    "        temp_epoch = 0\n",
    "        \n",
    "        temp_w = np.copy(w)\n",
    "        temp_b = np.copy(b)\n",
    "        temp_step = np.copy(step)\n",
    "    else:\n",
    "        if temp_epoch < early_stopping_iter:\n",
    "            temp_epoch += 1\n",
    "        else:\n",
    "            print(\"early stopping at epoch:\", epoch - temp_epoch)\n",
    "            \n",
    "            train_acc[:-temp_epoch-1]\n",
    "            train_loss[:-temp_epoch-1]\n",
    "\n",
    "            dev_acc[:-temp_epoch-1]\n",
    "            dev_loss[:-temp_epoch-1]\n",
    "            \n",
    "            w = temp_w\n",
    "            b = temp_b\n",
    "            step = temp_step\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Best model] training acc: 0.8833708785582634 \n",
      " training loss: 0.26874963151588527 \n",
      " development acc: 0.8785477331367489 \n",
      " development loss: 0.28299190499031895\n"
     ]
    }
   ],
   "source": [
    "print('[Best model]',\n",
    "      'training acc:', train_acc[-1],\n",
    "      '\\n',\n",
    "      'training loss:', train_loss[-1],\n",
    "      '\\n',\n",
    "      'development acc:', dev_acc[-1],\n",
    "      '\\n',\n",
    "      'development loss:', dev_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJuoQ_R2jUmX"
   },
   "source": [
    "### Plotting Loss and accuracy curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "DH3AJtvHjVJ7",
    "outputId": "f3fc5d1b-ddcc-4cf6-eea5-ebf23026edc5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Loss curve\n",
    "plt.plot(train_loss)\n",
    "plt.plot(dev_loss)\n",
    "plt.title('Loss')\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.savefig('./../fig/loss.png')\n",
    "plt.show()\n",
    "\n",
    "# Accuracy curve\n",
    "plt.plot(train_acc)\n",
    "plt.plot(dev_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.legend(['train', 'dev'])\n",
    "plt.savefig('./../fig/acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep training by using lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Epoch 1/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 2/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 3/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 4/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 5/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 6/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 7/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 8/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 9/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 10/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 11/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 12/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 13/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 14/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 15/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 16/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 17/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 18/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 19/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 20/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 21/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 22/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 23/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 24/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 25/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 26/6000]:[████████  ]81.9%[Epoch 26/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 27/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 28/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 29/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 30/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 31/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 32/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 33/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 34/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 35/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 36/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 37/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 38/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 39/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 40/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 41/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 42/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 43/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 44/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 45/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 46/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 47/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 48/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 49/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 50/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      " [Epoch 51/6000]:[██████████]100.0% training_acc: 0.8845 training_loss: 0.2663 dev_acc: 0.8800 dev_loss: 0.2814 ;\n",
      "\n",
      "early stopping at epoch: 0\n"
     ]
    }
   ],
   "source": [
    "# Some parameters for training    \n",
    "max_iter = 6000\n",
    "batch_size = 4000\n",
    "learning_rate = 0.0000001\n",
    "early_stopping_iter = 50\n",
    "y_dev_pred = _f(X_dev, w, b)\n",
    "Y_dev_pred = np.round(y_dev_pred)\n",
    "temp_acc = _accuracy(Y_dev_pred, Y_dev)\n",
    "temp_epoch = 0\n",
    "\n",
    "# Calcuate the number of parameter updates\n",
    "step = 1\n",
    "\n",
    "# Iterative training\n",
    "for epoch in range(max_iter):\n",
    "    # Random shuffle at the begging of each epoch\n",
    "    X_train, Y_train = _shuffle(X_train, Y_train)\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for idx in range(int(np.floor(train_size / batch_size))):\n",
    "        X = X_train[idx*batch_size:(idx+1)*batch_size]\n",
    "        Y = Y_train[idx*batch_size:(idx+1)*batch_size]\n",
    "\n",
    "        # Compute the gradient\n",
    "        w_grad, b_grad = _gradient(X, Y, w, b)\n",
    "            \n",
    "        # gradient descent update\n",
    "        # learning rate decay with time\n",
    "        w = w - learning_rate/np.sqrt(step) * w_grad\n",
    "        b = b - learning_rate/np.sqrt(step) * b_grad\n",
    "\n",
    "        step = step + 1\n",
    "        \n",
    "        y_train_pred = _f(X_train, w, b)\n",
    "        Y_train_pred = np.round(y_train_pred)\n",
    "        T_acc = _accuracy(Y_train_pred, Y_train)\n",
    "        T_loss = _cross_entropy_loss(y_train_pred, Y_train) / train_size\n",
    "        \n",
    "        y_dev_pred = _f(X_dev, w, b)\n",
    "        Y_dev_pred = np.round(y_dev_pred)\n",
    "        D_acc = _accuracy(Y_dev_pred, Y_dev)\n",
    "        D_loss = _cross_entropy_loss(y_dev_pred, Y_dev) / dev_size\n",
    "        \n",
    "        # progress bar\n",
    "        if idx == int(np.floor(train_size / batch_size)) - 1:\n",
    "            _progress_bar(epoch+1, max_iter, 100., T_acc, T_loss, D_acc, D_loss)\n",
    "        else:\n",
    "            _progress_bar(epoch+1, max_iter, idx * batch_size * 100 / train_size, T_acc, T_loss, D_acc, D_loss)\n",
    "    \n",
    "    train_acc.append(T_acc)\n",
    "    train_loss.append(T_loss)\n",
    "\n",
    "    dev_acc.append(D_acc)\n",
    "    dev_loss.append(D_loss)\n",
    "        \n",
    "    # early stopping\n",
    "    if D_acc > temp_acc:\n",
    "        temp_acc = np.copy(D_acc)\n",
    "        temp_epoch = 0\n",
    "        \n",
    "        temp_w = np.copy(w)\n",
    "        temp_b = np.copy(b)\n",
    "        temp_step = np.copy(step)\n",
    "    else:\n",
    "        if temp_epoch < early_stopping_iter:\n",
    "            temp_epoch += 1\n",
    "        else:\n",
    "            print(\"early stopping at epoch:\", epoch - temp_epoch)\n",
    "            \n",
    "            train_acc[:-temp_epoch-1]\n",
    "            train_loss[:-temp_epoch-1]\n",
    "\n",
    "            dev_acc[:-temp_epoch-1]\n",
    "            dev_loss[:-temp_epoch-1]\n",
    "            \n",
    "            w = temp_w\n",
    "            b = temp_b\n",
    "            step = temp_step\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Best model] training acc: 0.8845177145197625 \n",
      " training loss: 0.2662657394026112 \n",
      " development acc: 0.8800221157390343 \n",
      " development loss: 0.28138237802993626\n"
     ]
    }
   ],
   "source": [
    "print('[Best model]',\n",
    "      'training acc:', train_acc[-1],\n",
    "      '\\n',\n",
    "      'training loss:', train_loss[-1],\n",
    "      '\\n',\n",
    "      'development acc:', dev_acc[-1],\n",
    "      '\\n',\n",
    "      'development loss:', dev_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_pred = _f(X_dev, w, b)\n",
    "Y_dev_pred = np.round(y_dev_pred)\n",
    "D_acc = _accuracy(Y_dev_pred, Y_dev)\n",
    "D_loss = _cross_entropy_loss(y_dev_pred, Y_dev) / dev_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800221157390343"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzIcYAfvkUZ_"
   },
   "source": [
    "### Predicting testing labels\n",
    "\n",
    "Predictions are saved to *output_logistic.csv*.\n",
    "\n",
    "預測測試集的資料標籤並且存在 *output_logistic.csv* 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "ZEAKhugPkUyH",
    "outputId": "97c3eb12-a9c5-4c43-bc62-54f7f5f4797d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mexican-American 0.9305205167421388\n",
      " Do not know 0.6382392196245879\n",
      " South Korea 0.593700859873409\n",
      "id 0.49231588533069526\n",
      " Grandchild 18+ ever marr not in subfamily 0.3612821655872186\n",
      " 44 0.32178651546653336\n",
      " Professional specialty 0.26206563728990634\n",
      " Protective services -0.26206563728990623\n",
      " 41 0.2597418780784859\n",
      " 0 0.23624391989434032\n"
     ]
    }
   ],
   "source": [
    "# Predict testing labels\n",
    "predictions = _predict(X_test, w, b)\n",
    "with open(output_fpath.format('logistic'), 'w') as f:\n",
    "    f.write('id,label\\n')\n",
    "    for i, label in  enumerate(predictions):\n",
    "        f.write('{},{}\\n'.format(i, label))\n",
    "\n",
    "# Print out the most significant weights\n",
    "ind = np.argsort(np.abs(w))[::-1]\n",
    "with open(X_test_fpath) as f:\n",
    "    content = f.readline().strip('\\n').split(',')\n",
    "features = np.array(content)\n",
    "for i in ind[0:10]:\n",
    "    print(features[i], w[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
